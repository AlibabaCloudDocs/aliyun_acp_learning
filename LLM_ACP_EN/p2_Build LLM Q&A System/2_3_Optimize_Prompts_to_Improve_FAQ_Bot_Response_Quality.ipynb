{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92010cd5",
      "metadata": {},
      "source": [
        "# 2.3 Optimizing prompts to improve the quality of responses from Q&A bots\n",
        "\n",
        "## üöÑ Preface  \n",
        "\n",
        "The Q&A bot from the previous section can already answer questions about company rules and regulations, but this is only the first step in building an excellent user experience. Users often expect more personalized and precise interactions. This section introduces prompt engineering techniques, teaching you how to improve or control the output of the Q&A bot by optimizing prompts‚Äîfor example, adjusting tone, standardizing formats, or even enabling it to handle tasks like text summarization, inference, and transformation.\n",
        "\n",
        "## üçÅ Goals\n",
        "\n",
        "After completing this section , you will know about:\n",
        "\n",
        "* How prompt frameworks and templates work\n",
        "* Prompt techniques and their best practices\n",
        "* How to apply LLMs in engineering to handle various tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0e80fa",
      "metadata": {},
      "source": [
        "## 1. Previous content recap\n",
        "\n",
        "In the previous section, through the RAG method, the LLM has already obtained the company's private knowledge. For ease of invocation, this was encapsulated into several functions and saved in chatbot/rag.py. You can now quickly invoke it using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa6756d-47a4-4b11-8074-d992d6ce327b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:03.577636Z",
          "start_time": "2025-01-15T06:49:03.573248Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from config.load_key import load_key, display_config_summary\n",
        "load_key()\n",
        "display_config_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91138451-fbc1-49ff-a268-2ece341b10bd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:11.770743Z",
          "start_time": "2025-01-15T06:49:11.611835Z"
        }
      },
      "outputs": [],
      "source": [
        "from chatbot import rag, llm\n",
        "# Load index\n",
        "# The previous section has already built the index, so the index can be loaded directly here. If you need to rebuild the index, you can add a line of code: rag.indexing()\n",
        "index = rag.load_index(persist_path=\"knowledge_base/test\")\n",
        "query_engine = rag.create_query_engine(index=index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2d8d11-5a07-441c-9cd6-6ba10bf80603",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:16.961802Z",
          "start_time": "2025-01-15T06:49:16.959373Z"
        }
      },
      "outputs": [],
      "source": [
        "# Define the question-answering function\n",
        "def ask_llm(question, query_engine):\n",
        "  streaming_response = query_engine.query(question)\n",
        "  streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1715f36",
      "metadata": {},
      "source": [
        "## 2. Optimizing Prompts to Improve Response Quality \n",
        "\n",
        "In the previous section, you successfully mastered the art of harnessing internal company knowledge for your Q&A bot via RAG. Now, let's put its real-world performance to the test. \n",
        "\n",
        "Imagine a new colleague wants to learn about the company's project management tools and asks the bot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc28984",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bot performance using RAG only\n",
        "question = \"What tools should we use for project management in our company?\"\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fea9a9c",
      "metadata": {},
      "source": [
        "The response is accurate, but it lacks **utility**. A colleague might complain: *\"The answer is fine, but it would be perfect if it included direct links to the tools so I don't have to hunt for them myself.\"*\n",
        "\n",
        "This is a perfectly reasonable request. One straightforward‚Äîand perhaps \"brute-force\"‚Äîsolution might be to manually append a specific instruction to the end of the user's question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2434a47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attempting a simple \"patch\"\n",
        "question = \"What tools should we use for project management in our company?\"\n",
        "instruction = \" Please make sure to include the official website or download link for the tools in your response.\"\n",
        " \n",
        "# Simply concatenate the question and the instruction\n",
        "new_question = question + instruction\n",
        "ask_llm(new_question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb8e9e5",
      "metadata": {},
      "source": [
        "**Expected Output (Example):**\n",
        "> For project management, we recommend using **Jira** or **Trello**. Both tools are excellent for tracking project progress and ensuring efficient team collaboration.\n",
        "> \n",
        "> - **Jira**: A powerful tool specifically designed for software development projects. It helps you track issues, tasks, and project milestones. You can visit the [Atlassian Official Site](https://www.atlassian.com/software/jira) for more info or to download.\n",
        "> \n",
        "> - **Trello**: A more flexible, Kanban-style tool suitable for various project types. Its card-and-list format provides a visual overview of progress. You can visit [Trello.com](https://trello.com/) to sign up or learn more.\n",
        "\n",
        "Problem solved, right? Well... not quite.\n",
        "\n",
        "Consider another scenario where a user asks a question that has nothing to do with software tools:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27616b41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing the \"patch\" on an unrelated question\n",
        "question_2 = \"How many days of annual leave am I entitled to?\"\n",
        "instruction = \" Please make sure to include the official website or download link for the tools in your response.\"\n",
        " \n",
        "new_question_2 = question_2 + instruction\n",
        "ask_llm(new_question_2, query_engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2139c7a9",
      "metadata": {},
      "source": [
        "\n",
        "**Expected Output (Example):**\n",
        "> Your question pertains to company benefits, specifically annual leave. This information is typically recorded in the Employee Handbook or internal HR policy documents. I suggest contacting HR for the exact details. The HR contact is Wei Xiong (xiongwei@educompany.com).\n",
        "> \n",
        "> Regarding your request for tool websites or download links: since your question does not involve any specific tools, I am unable to provide any links.\n",
        "\n",
        "This reveals the fundamental flaw. This \"one-size-fits-all\" approach to concatenating instructions is extremely **fragile**. Not only does it make the model‚Äôs responses feel verbose and awkward, but it also requires you to manually decide which instruction to append for every single query‚Äîwhich is completely impractical in a real-world application.\n",
        "\n",
        "**This is exactly why we need to dive into Prompt Engineering.** As a vital component of **Context Engineering**, it teaches you how to move away from these temporary \"patches.\" Instead, you will learn to build conversation contexts systematically and intelligently, precisely guiding the model on how to respond to different situations with grace and accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c006a5ea-5693-4805-9d2c-ccc0d2214939",
      "metadata": {},
      "source": [
        "## 3. Prompt framework\n",
        "\n",
        "### 3.1 Basic elements\n",
        "\n",
        "When communicating with an LLM, you can think of it as an entity that has been trained to understand human communication patterns. The way you interact should mirror how humans exchange information. Your requirements must be clear and unambiguous. The clearer and more precise your question (or prompt), the better the large language model will understand the core of the query, and the more likely its response will meet your expectations. Generally speaking, the following elements should be clarified in a prompt, and form a prompt framework that can help you construct a complete and effective prompt:\n",
        "\n",
        "|Element|Meaning|\n",
        "|----|----|\n",
        "|Objective|Clearly specify what task the LLM is required to complete, allowing it to focus on specific goals.|\n",
        "|Context|Background information about the task, such as operational processes or task scenarios, clarifying the scope of the discussion for the LLM.|\n",
        "|Role|The role the LLM should assume (e.g., \"You are a helpful assistant,\" \"You are a senior developer\"). This defines the model's persona, voice, and writing style.|\n",
        "|Audience|Specify the target audience for the response (e.g., \"Explain this to a 5-year-old,\"). This helps tailor the complexity and tone of the output.|\n",
        "|Sample|Provide specific input-output examples for the LLM to follow. This is also known as \"few-shot prompting\" and helps the model understand the desired pattern and format.|\n",
        "|Output Format|Specify the format of the output, output type, and range of enumerated values. You can also state content and information that should not be included in the output, which can be further clarified with examples.|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc35341a-3d7c-4881-81e3-935d42a1dcb2",
      "metadata": {},
      "source": [
        "Of course, in addition to the prompt framework discussed above, many problem analysis paradigms can be used to help you articulate clear and specific requirements. For example, SWOT analysis and 5W2H analysis offer structured approaches to thinking through your needs and expressing them more effectively. Additionally, you can also write a meta-prompt‚Äîa prompt designed to optimize or enhance another prompt‚Äîto guide the LLM in refining your initial input.\n",
        "\n",
        "By using such a meta-prompt, you can ask the model to:\n",
        "* Clarify ambiguous statements\n",
        "* Expand vague instructions into detailed guidelines\n",
        "* Add context, tone, format, and examples where necessary\n",
        "* Ensure the final prompt aligns with best practices for interacting with AI models\n",
        "\n",
        "This self-reflection approach improves the quality of your prompts, while deepening your understanding of how to communicate effectively with AI systems. We'll dive deeper into meta-prompting systematically in `Section 4.6 Meta Prompting: Let the LLM Be Your Prompt Coach.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c618996",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_expand = \"\"\"\n",
        "    You are a Prompt Optimization and Expansion Assistant. Your primary role is to receive an arbitrary user input prompt ‚Äî which may be vague, incomplete, or lacking in detail ‚Äî and **refine and expand** it into a more structured, precise, and actionable instruction that can be clearly understood and executed by large language models.\n",
        "\n",
        "    Given the input prompt, you should generate an optimized version according to the following instructions:\n",
        "\n",
        "    Your task is to enhance the clarity of the original prompt while preserving its core intent. The resulting optimized prompt should:\n",
        "    - Be semantically richer and more descriptive.\n",
        "    - Include relevant context, constraints, or formatting instructions as needed.\n",
        "    - Specify desired tone, style, or audience (if applicable).\n",
        "    - Suggest example inputs and outputs where appropriate to guide model behavior.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Output Format:\n",
        "\n",
        "    Please respond with the following structure:\n",
        "\n",
        "    #### 1. Optimized Prompt:\n",
        "    [Here goes the enhanced and expanded version of the original prompt.]\n",
        "\n",
        "    #### 2. Example Input (if applicable):\n",
        "    [Provide one or more example(s) of what a user might input based on the prompt.]\n",
        "\n",
        "    #### 3. Example Output (based on the example input):\n",
        "    [Show what kind of output would be expected from the model when using this prompt with the given example input.]\n",
        "\n",
        "    #### 4. Optimization Notes:\n",
        "    [Briefly explain how you improved the original prompt ‚Äî e.g., added clarity, specified format, included context, etc.]\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Example:\n",
        "\n",
        "    #### Original Prompt:\n",
        "    \"Write a story about animals.\"\n",
        "\n",
        "    #### Optimized Prompt:\n",
        "    \"Compose a short narrative (approximately 300‚Äì500 words) about a group of forest animals who must work together to solve a problem. The story should include at least three distinct animal characters, a central conflict or challenge, and a resolution that highlights themes such as cooperation, courage, or environmental awareness. Use descriptive language to set the scene and convey character emotions. Aim for a tone suitable for children aged 6‚Äì10.\"\n",
        "\n",
        "    #### Example Input:\n",
        "    None required ‚Äî the model generates content autonomously.\n",
        "\n",
        "    #### Example Output:\n",
        "    Once upon a time in Greenleaf Forest, a wise old owl named Oliver noticed that the river had stopped flowing. Concerned, he called a meeting with his neighbors: Mina the mouse, Benny the bear, and Tilly the turtle. Together, they discovered a fallen tree blocking the stream. With teamwork and determination, they cleared the path and restored the water supply. From then on, the animals made sure to keep their home safe and healthy...\n",
        "\n",
        "    #### Optimization Notes:\n",
        "    - Added specific narrative elements (characters, setting, conflict, theme).  \n",
        "    - Defined word count and target audience.  \n",
        "    - Clarified tone and style expectations.  \n",
        "    - Introduced emotional and moral dimensions to enrich storytelling.\n",
        "\n",
        "    ---\n",
        "\n",
        "    Please apply this process to any user-submitted prompt and return your response using the above template.\n",
        "\n",
        "    The original prompt is:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def input_enhanced_llm_output(questions):\n",
        "    expanded_input = prompt_expand + questions\n",
        "    return ask_llm(expanded_input, query_engine)\n",
        "\n",
        "question = \"Please help me write a promotional copy for our company's new book. The general content of the book is xxx(will be given later)\"\n",
        "input_enhanced_llm_output(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1fa6d9",
      "metadata": {},
      "source": [
        "### 3.2 Prompt template\n",
        "\n",
        "In application development, it is impractical to expect end-users to write their own effective prompts from scratch. Instead, developers use the principles of prompt frameworks to create structured prompt templates. This allows developers to simply insert dynamic user inputs into predefined placeholders, ensuring that every final prompt is consistent. This consistency leads to more reliable and standardized LLM responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84e29a3-a017-47c4-907e-2ff897333d1d",
      "metadata": {},
      "source": [
        "In the RAG chatbot created using LlamaIndex, there is a default prompt template that looks like this:\n",
        "\n",
        "- The default template can be viewed using code. You can refer to [the code on the LlamaIndex official website.](https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/) The original LlamaIndex prompt template is:  \n",
        "\n",
        "\n",
        "\n",
        "```text\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "```\n",
        "\n",
        "Both¬†`context_str`¬†and¬†`query_str`¬† are placeholders that get replaced during the retrieval and querying process. The¬†`context_str`¬†is filled with relevant information retrieved from the vector database, while¬†`query_str`¬†is replaced with the user's actual question.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ff8788-4d67-47b8-a4a2-3f3a85e6b0c5",
      "metadata": {},
      "source": [
        "Since the default template is general-purpose , it may not be suitable for all scenarios. For example, you might want to adjust the chatbot's tone, add specific constraints, or define a precise output format. This customization ensures that the model‚Äôs output aligns more closely with your requirements.\n",
        " \n",
        "You can use the following sample code, where¬†`prompt_template_string`¬†represents the new prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05aea996-fdc8-4a35-922f-288313881fa3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:22.442975Z",
          "start_time": "2025-01-15T07:52:22.435217Z"
        }
      },
      "outputs": [],
      "source": [
        "# Build prompt template\n",
        "prompt_template_string = (\n",
        "    \"You are the company's customer service assistant, you need to answer users' questions concisely.\"\n",
        "    \"\\n„ÄêNotes„ÄëÔºö\\n\"\n",
        "    \"1. Answer user questions based on contextual information.\\n\"\n",
        "    \"2. Only answer the user's question, do not output other information\\n\"\n",
        "    \"The following is reference information.\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Question: {query_str}\\n.\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "# Update prompt template\n",
        "rag.update_prompt_template(query_engine, prompt_template_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa99f324",
      "metadata": {},
      "source": [
        "## 4. Techniques for writing effective prompts\n",
        "\n",
        "In Section 3.1, some key elements of prompt design were introduced. This section will explain specific prompt techniques in detail, focusing on practical scenarios and how to apply the elements of a prompt effectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b3eb55-646d-41dd-b801-152d76205bbe",
      "metadata": {},
      "source": [
        "### 4.1 Clearly express requirements and use delimiters\n",
        "\n",
        "Clearly stating your requirements ensures that the content generated by the LLM is highly relevant to the task at hand. These requirements typically include the¬†**task objective**,¬†**context**, and¬†**background information**.\n",
        "\n",
        "Using¬†**delimiters**¬†helps LLMs focus on specific goals, avoid ambiguous interpretations, and minimize unnecessary processing. Delimiters also help structure your prompt in a way that improves readability and model understanding.\n",
        "\n",
        "Here are some commonly used delimiters in English prompt engineering:\n",
        "\n",
        "| Delimiter Type      | Example                         | Use Case / Description |\n",
        "|---------------------|----------------------------------|--------------------------|\n",
        "| Triple Quotes         | `\"\"\"...\"\"\"`                      | For multi-line strings or input blocks |\n",
        "| Triple Hashes         | `### Section ###`                | To mark headings or instruction sections |\n",
        "| Double Angle Brackets | `<<example>>...<</example>>`     | To define custom tags for variables or examples |\n",
        "| XML-like Tags         | `<input>...</input>`             | For structured formatting and content separation |\n",
        "| Dashes                | `--- Section ---`                | To separate paragraphs or logical blocks |\n",
        "| Equal Signs           | `=== Output Format ===`          | Similar to headings; often used for formatting clarity |\n",
        "\n",
        "> ‚ö†Ô∏è Note: Avoid using symbols that appear frequently in your content as delimiters to prevent confusion. For example, if square brackets¬†`[]`¬†are commonly used  in your input text, they may not be suitable as delimiters.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example Usage in an English Prompt\n",
        "\n",
        "```text\n",
        "### Instruction ###\n",
        "You are a helpful assistant. Please summarize the following article.\n",
        "\n",
        "=== Article Content ===\n",
        "\"\"\" \n",
        "Artificial intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence.\n",
        "\"\"\"\n",
        "\n",
        "### Output Format ###\n",
        "Please provide a summary in no more than 50 words.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8918731d-49b6-4132-9ba5-2dce3eee360c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:32.192934Z",
          "start_time": "2025-01-15T07:52:24.619420Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "### Instruction ###\n",
        "Expand and polish the text enclosed in XML tags below to make the copy vivid, creative, and appealing to new employees. \n",
        "The tone should be professional yet welcoming, suitable for onboarding materials.\n",
        "\n",
        "=== Original Text ===\n",
        "<text>\n",
        "New Employee Training Camp Activity\n",
        "</text>\n",
        "\n",
        "=== Output Requirements ===\n",
        "- Enhance the content with engaging language.\n",
        "- Add context about the purpose and benefits of the training camp.\n",
        "- Include key elements such as team-building, mentorship, and orientation.\n",
        "- Keep the final output concise and motivational.\n",
        "\"\"\"\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ae4c28-a2e6-4ad3-995b-7af2d78b19d1",
      "metadata": {},
      "source": [
        "The execution results show that the prompt effectively communicates the task requirements: to expand and enhance the provided text.\n",
        "\n",
        "The core theme of the task is \"**New Employee Training Camp Activities,**\" and the intended audience is \"**new company employees.**\" In the prompt, the delimiter¬†<text>...</text>¬†is used to clearly encapsulate and isolate the original content for processing.\n",
        "\n",
        "By structuring the prompt with clear instructions, appropriate context, and standardized delimiters, the model is better guided to generate output that not only preserves the original meaning but also becomes more vivid, engaging, and tailored to the target audience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e4407d-3cff-4f00-a39b-b8ddfd7cce1d",
      "metadata": {},
      "source": [
        "### 4.2 Define Role and Audience\n",
        "\n",
        "The¬†**role**¬†is the identity the LLM should assume in a specific scenario, such as an expert, advisor, or assistant. The¬†**audience**¬†is the target user group for which the model's output is intended, such as general consumers or students. Defining these elements helps developers tailor the tone, depth, and structure of the content accordingly.\n",
        "The example below demonstrates how different system roles affect the style and content of the generated output. \n",
        "\n",
        "> Pass the prompt template into¬†`prompt_template_string`, and the user question into¬†`question`, then call the¬†`ask_llm`¬†function to execute it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e5f128-8b02-40ca-b789-d577b6e38a3a",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th width = '80px'>Role</th>\n",
        "      <th width = '500px'>LLM Algorithm Engineer</th>\n",
        "      <th width = '500px'>Elementary School Teacher</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody valign = 'top'>\n",
        "    <tr>\n",
        "      <td>Prompt Template</td>\n",
        "      <td>You are a senior large language model algorithm engineer. Please answer the user's question based on the provided information<br>---------------------<br>{context_str}<br>---------------------<br>Question: {query_str}.<br>Answer: \n",
        "</td>\n",
        "      <td>You are an elementary school teacher. Please answer the user's question based on the provided information<br>---------------------<br>{context_str}<br>---------------------<br>Question: {query_str}.<br>Answer: </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>User Question</td>\n",
        "      <td>What is Qwen-VL</td>\n",
        "      <td>What is Qwen-VL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Output Answer</td>\n",
        "      <td>Qwen-VL is a multimodal pre-trained model introduced by Alibaba Cloud. It combines text and image processing capabilities, enabling it to understand and generate text descriptions related to images. Simply put, Qwen-VL can be regarded as an artificial intelligence assistant that can \"describe images in words.\" It not only understands the content of images but also generates corresponding textual descriptions or answers questions related to the images. This technology has broad application prospects in various fields, such as automatic image tagging, visual question-answering systems, and content-based image retrieval. For new media operations, tools like Qwen-VL can help improve content creation efficiency, enhance user experience, and provide users with richer and more diverse ways of presenting information.</td>\n",
        "      <td>Kids, today we‚Äôre going to meet a new friend named Qwen-VL!\\nQwen-VL is like a very smart robot that has \"eyes\" üëÄ and a \"brain\" üß†. It can \"see\" pictures and videos and understand their meaning, just like us!\\nFor example, if you show it a picture of a kitten playing with a ball of yarn, it can not only recognize that it‚Äôs a kitten üê± but also tell you that it‚Äôs playing with a ball of yarn üß∂. Even better, you can ask it, \"What color is the kitten?\", and it will tell you the color of the kitten.\\nIt can also understand the content in videos. For instance, if you show it a video of kids playing soccer ‚öΩÔ∏è, it can tell you who is kicking the ball, where the ball goes, and even guess what might happen next!\\nQwen-VL is like a child who is still learning. It keeps getting smarter and improving, and in the future, it will become even more intelligent, capable of doing more things and helping us better understand the world!</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f06bea-a34b-4e7a-b2b5-918af4c1ebdb",
      "metadata": {},
      "source": [
        "### 4.3 Specifying output formats\n",
        "\n",
        "Sometimes developers need LLMs to produce structured data (such as JSON or XML) that downstream systems can programmatically parse and use in the application workflow. While LLMs excel at generating free-form natural language, they can also reliably produce structured outputs such as JSON, XML, or CSV formats‚Äîas long as the desired format and requirements are clearly specified in the prompt.\n",
        "In the following example, based on the prompt template from Section 3.2, we will act as a Q&A bot to assist in reviewing the quality of company-developed documentation, and return the results in¬†**JSON format**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6692496-b6e4-4d75-aaa2-ef38b0c24f0c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:49.007218Z",
          "start_time": "2025-01-15T06:49:46.080617Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "[Task Requirements]\n",
        "You will be given a sentence or paragraph. Your task is to review it for any spelling or grammatical errors. \n",
        "If there are errors, you should identify them, explain the issue, and provide a corrected version. \n",
        "\n",
        "---\n",
        "[Output Requirements]\n",
        "Please output only in JSON format. Do not include markdown or code blocks.\n",
        "The JSON must contain:\n",
        "- \"label\": 0 if an error is found, 1 if no error.\n",
        "- \"reason\": A brief explanation of the error (if any).\n",
        "- \"correct\": The corrected version of the text (or the original if no error).\n",
        "\n",
        "---\n",
        "[User Input]\n",
        "The following is the user input. Please review it:\n",
        "\"\"\"\n",
        "\n",
        "question_doc = \"Delimiters are special symbols that help large langauge models (LLMs) identify which parts of the prompt should be considered a complete unit of meaning.\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe59cd3-a241-4b28-902e-7ca35966e6a7",
      "metadata": {},
      "source": [
        "As shown in the example results, the prompt¬†`question_task` specified that the output format should be JSON format, and clearly defined the content requirements. The model  generated responses in the required format. This consistent and structured output makes it easier to integrate the model into existing systems.\n",
        "\n",
        "On news websites, blog platforms, or internal knowledge-sharing platforms within enterprises, user-edited or published articles may contain typos, grammatical errors, or even sensitive information. However, traditional manual review methods are prone to oversight. In this case, LLMs can be integrated to automatically review content. If an article is flagged for serious grammatical errors or high-risk sensitive words, its priority for revision will be set to \"high.\" For articles with minor issues, the priority for revision can be set to \"low.\" This approach reduces labor costs and enhances the efficiency and accuracy of the system.\n",
        "\n",
        "Of course, there are many other applications similar to this scenario. Developers can analyze system bottlenecks or focus on data-intensive tasks to explore more use cases for LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f32b7a-9b47-40f1-82cb-e889a3b48048",
      "metadata": {},
      "source": [
        "### 4.4 Providing few-shot examples\n",
        "\n",
        "In the example from Section 4.3, the prompt specified the output format, and the LLM  generated formatted content. However, if we want the model's output  to not only be correctly formatted but also maintain consistency in style and structure, providing a few examples is a highly effective technique. This technique, known as `few-shot prompting`, is like giving the model a \"reference book\" to follow.\n",
        "\n",
        "In the following code example, let‚Äôs first observe the LLM's output without any examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcca16d368baf38",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:20:42.883097Z",
          "start_time": "2025-01-15T07:20:18.423060Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "### üìù Task Requirements\n",
        "You are to generate content based on the user's topic, following the **style and structure** of the example provided below. Do not add any extra text or explanations ‚Äî return only the final Markdown-formatted output.\n",
        "\n",
        "---\n",
        "### üß© Output Format Requirements\n",
        "Please ensure your response strictly follows this format:\n",
        "- Use Markdown syntax (headings, lists, bold, links, image placeholders, etc.)\n",
        "- Keep the tone informative and easy to follow\n",
        "- Structure content in a step-by-step or tutorial format\n",
        "- Avoid markdown code blocks around the final output itself\n",
        "\n",
        "---\n",
        "### üë§ User Input Topic\n",
        "The following is the user's requested topic:\n",
        "\"\"\" \n",
        "\n",
        "question_doc = \"Handmade Keychain Making Tutorial\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f61d316e005c4f",
      "metadata": {},
      "source": [
        "The model generated a tutorial for making handmade keychains, but the content was not concise enough. If you want the model to output content in a specific style and structure‚Äîsuch as focusing only on the theme, list of materials, and step-by-step instructions‚Äîyou can provide a few examples to guide it. This helps the model \"imitate\" the desired format and produce more consistent and structured results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e0f67f703e7ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:39.297041Z",
          "start_time": "2025-01-15T06:50:24.071562Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "### üìù Task Requirements\n",
        "You are to generate content based on the user's topic, following the **style and structure** of the example provided below. Do not add any extra text or explanations ‚Äî return only the final Markdown-formatted output.\n",
        "\n",
        "---\n",
        "### üß© Output Format Requirements\n",
        "Please ensure your response strictly follows this format:\n",
        "- Use Markdown syntax (headings, lists, bold, links, image placeholders, etc.)\n",
        "- Keep the tone informative and easy to follow\n",
        "- Structure content in a step-by-step or tutorial format\n",
        "- Avoid markdown code blocks around the final output itself\n",
        "\n",
        "---\n",
        "### üìö Example Content\n",
        "<example>\n",
        "### Example 1: Making a Simple Bookmark\n",
        "# Simple Bookmark Making Tutorial\n",
        "\n",
        "## Steps\n",
        "1. Choose a piece of colored cardstock.\n",
        "2. Use a pencil to draw a rectangle on the cardstock, with dimensions approximately 2 inches x 6 inches.\n",
        "3. Cut out the rectangle along the pencil lines.\n",
        "4. Personalize the bookmark by decorating it with stickers.\n",
        "5. Done! You now have a unique bookmark.\n",
        "</example>\n",
        "\n",
        "---\n",
        "### üë§ User Input Topic\n",
        "The following is the topic requested by the user:\n",
        "\"\"\"\n",
        "\n",
        "question_doc = \"Making Handmade Greeting Cards\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95447ee9772539d",
      "metadata": {},
      "source": [
        "From the above example results, it's clear that the LLM has fully adhered to the provided example and generated content with a consistent structure and style. When specifying the output format in a prompt, it is recommended to provide a few examples for the LLM to reference. This practice helps ensure that the model's output is more reliable, consistent, and aligned with your expectations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53569b47-1619-4527-a9c8-b53f5e5bdf8d",
      "metadata": {},
      "source": [
        "### 4.5 Giving the model \"thinking\" time\n",
        "\n",
        "For some complex tasks, even well-structured prompts may be insufficient for the LLM to complete the task effectively. In such cases, you can guide the model by allowing it to \"think\" step by step, encouraging it to provide intermediate reasoning before arriving at a final answer. This approach, known as the chain-of-thought (CoT) method, improves performance on tasks that require logical or mathematical reasoning.\n",
        "\n",
        "For example, consider a scenario where you ask the LLM to solve a math problem. The correct answer is $10,500. Initially, you might use a simple prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3569f469-c601-406a-a26d-fb0a74157c68",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:42.416991Z",
          "start_time": "2025-01-15T06:50:41.378053Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "[Background Information]\n",
        "An educational training institution (hereinafter referred to as the \"company\") incurred the following main expenses in the 2023 fiscal year:\n",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times during the year, each trip lasting one week. The specific expenses are as follows:\n",
        "   - Transportation and accommodation fees: average 1600 usd/trip\n",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 usd, which is expected to last for 4 years.\n",
        "\n",
        "[Problem Description]\n",
        "Based on the above background information, complete the following tasks:\n",
        "Calculate the total travel expenses for the year due to teacher business trips, including the amortized cost of teaching materials.\n",
        "\n",
        "[Output Requirements]\n",
        "Provide only the total travel expenses directly, without any other information. \"\"\"\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf9078a-f716-4537-b003-a90d70f988b5",
      "metadata": {},
      "source": [
        "As shown, the initial response is incorrect. To improve accuracy, you can use the CoT method by prompting the LLM to explain its reasoning step by step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cb1715-305c-43ee-bd5e-274d2556ad56",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:55.556383Z",
          "start_time": "2025-01-15T06:50:43.720893Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"An educational training institution (hereinafter referred to as the \"company\") incurred the following major expenses in the 2023 fiscal year:\n",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times throughout the year, with each trip lasting one week. The specific expenses are as follows:\n",
        "   - Transportation and accommodation fees: an average of 1600 usd per trip\n",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 usd, which is expected to last for 4 years.\n",
        "   \n",
        "### Problem Description\n",
        "Based on the above background information, complete the following tasks:\n",
        "Calculate the total travel expenses for the year caused by teacher business trips, including the amortized cost of teaching materials.\n",
        "\n",
        "### Output Requirements\n",
        "Please derive step by step to calculate the total travel expenses.\"\"\"\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ec9cdf-d6e1-469f-9e4c-f91fdf7f1c3e",
      "metadata": {},
      "source": [
        "After optimizing the prompt, the LLM was able to generate an accurate and detailed response. This demonstrates that prompting the model to generate a chain of thought can significantly improve the quality of reasoning-based outputs.\n",
        "\n",
        "While CoT is a powerful technique, there are other methods to enhance the model‚Äôs thinking process, such as tree of thoughts (ToT) and graph of thoughts (GOT). However, current LLMs still rely heavily on CoT for structured reasoning. As AI technology advances, models are gradually moving toward more sophisticated multi-agent systems (MAS), which can handle complex tasks more efficiently. You can explore this further in <2_6_Extend_FAQ_Bot_Capabilities_with_Plugins>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7cda3a",
      "metadata": {},
      "source": [
        "### 4.6 Meta Prompting: Let the LLM Be Your Prompt Coach\n",
        "\n",
        "Writing a perfect prompt on the first try is often very difficult. A more common workflow is:\n",
        "1. Write the first version of the prompt.\n",
        "2. Run it and analyze which parts of the output don't meet expectations.\n",
        "3. Summarize the problems, think about how to improve, then modify the prompt.\n",
        "4. Repeat this iteration process until satisfied.\n",
        "\n",
        "Think about it‚Äîisn't this also your typical path for optimizing prompts? While this process is effective, it heavily relies on experience and is quite time-consuming.\n",
        "\n",
        "At this point, you might think: since LLMs are so powerful, **can we let the model itself handle this analysis, summarization, and improvement iteration process?** Having it play the role of a \"prompt review expert\" to help us analyze and optimize prompts would undoubtedly be more efficient.\n",
        "\n",
        "The answer is yes. This method of **having you and the model \"discuss\" how to optimize the prompt itself** is called **Meta Prompting**.\n",
        "\n",
        "To further understand this powerful technique, let's practice it hands-on.\n",
        "\n",
        "#### Step 1: A Less-Than-Ideal Initial Prompt\n",
        "\n",
        "Suppose your task is to optimize the Q&A bot's responses to make them more friendly and clearly structured when answering new employees' questions about \"company benefits.\" You might start with a simple prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bb2906",
      "metadata": {},
      "outputs": [],
      "source": [
        "# In a real RAG application, this text would be retrieved from your vector database.\n",
        "# Here we use a string to simulate it for experimentation.\n",
        "retrieved_text = \"\"\"\n",
        "Regarding company benefits, we provide comprehensive health insurance covering employees and their immediate family members.\n",
        "Annual health checkups are standard. Additionally, there are 15 days of paid annual leave and 5 days of paid sick leave per year.\n",
        "We also provide a monthly transportation subsidy of 500 yuan and a meal subsidy of 300 yuan.\n",
        "To encourage employee growth, the company has an annual education and training fund of up to 8000 yuan, which employees can apply to use for courses or professional books.\n",
        "For fitness, the company has partnerships with multiple gyms where employees can enjoy discounted rates.\n",
        "\"\"\"\n",
        "\n",
        "# This is a very basic prompt that simply concatenates the task and information.\n",
        "initial_prompt = f\"\"\"\n",
        "Based on the following information, answer new employees' questions about company benefits.\n",
        "\n",
        "„ÄêReference Information„Äë\n",
        "{retrieved_text}\n",
        "\"\"\"\n",
        "\n",
        "# Let's see what effect this \"plain\" prompt produces.\n",
        "response = llm.invoke(initial_prompt)\n",
        "print(\"--- Initial Response ---\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54e7737a",
      "metadata": {},
      "source": [
        "This response contains all the information, but you might feel that for a newly hired employee, it seems somewhat bland and disorganized. It simply restates the text without emphasis or a warm, welcoming tone.\n",
        "\n",
        "Clearly, this result doesn't meet your expectations. Now, instead of racking your brain on how to modify it, you can try a more efficient method: **let the LLM help you optimize**.\n",
        "\n",
        "#### Step 2: Build a Meta Prompt to Get Optimization Suggestions from the LLM\n",
        "\n",
        "Now, you're dissatisfied with this bland response. You can construct a \"Meta Prompt\" that clearly describes your goals (friendly, structured, highlighting key points) to the model, and feed both your less-than-ideal initial prompt and its output to the LLM, requesting it to act as a \"prompt engineering expert\" to help you improve.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33550722",
      "metadata": {},
      "outputs": [],
      "source": [
        "# You need to clearly express your dissatisfaction and expectations‚Äîthis is key to helping the AI coach understand your intent.\n",
        "meta_prompt = f\"\"\"\n",
        "I'm optimizing a prompt for our company's new employee Q&A bot, with the goal of answering questions about \"company benefits.\"\n",
        "\n",
        "This is my first attempt:\n",
        "---\n",
        "{initial_prompt}\n",
        "---\n",
        "\n",
        "This is the output it generated:\n",
        "---\n",
        "{response}\n",
        "---\n",
        "\n",
        "This output isn't good enough. I want the bot's responses to be more engaging, clearly structured, and help new employees quickly grasp the key points. Specific requirements:\n",
        "1. **Tone**: Friendly, warm, with a welcoming feeling for new colleagues.\n",
        "2. **Structure**: Use clear bullet points (e.g., lists starting with emojis) to organize content.\n",
        "3. **Content**: Categorize benefits into groups like \"Health & Leave,\" \"Subsidies & Incentives,\" etc.\n",
        "\n",
        "Please act as a prompt engineering expert and rewrite this prompt to achieve the above goals.\n",
        "\"\"\"\n",
        "\n",
        "# Now, let the AI coach get to work and generate an optimized version of the prompt for you.\n",
        "optimization_suggestion = llm.invoke(meta_prompt)\n",
        "print(\"--- Optimization Suggestions from AI Coach --\")\n",
        "print(optimization_suggestion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6449ae",
      "metadata": {},
      "source": [
        "Observe the suggestions from the AI coach‚Äîyou'll find that the optimized prompt it provides likely applies multiple techniques you learned in previous sections, such as:\n",
        "\n",
        "* **Defining a role** (e.g., \"You are a warm, friendly onboarding buddy\")\n",
        "* **Clear task description** (e.g., \"Based on the reference information provided, generate an introduction about company benefits\")\n",
        "* **Specifying output format and style** (e.g., \"Use a warm welcome greeting,\" \"Use lists starting with emojis\")\n",
        "\n",
        "This proves that having the model play an expert role to optimize prompts is entirely feasible.\n",
        "\n",
        "#### Step 3: Use the Optimized Prompt\n",
        "\n",
        "Now you can directly use the prompt that this \"AI coach\" customized for your task and see how it performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed48c9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a hypothetical optimized prompt suggested by the AI coach\n",
        "# In practice, you can directly use the output from `optimization_suggestion`\n",
        "# Here we manually construct one that follows the suggestions for demonstration\n",
        "\n",
        "optimized_prompt = f\"\"\"\n",
        "„ÄêRole„Äë\n",
        "You are a warm, friendly Onboarding Buddy. Your task is to welcome new colleagues and clearly introduce the company's benefits policy.\n",
        "\n",
        "„ÄêTask„Äë\n",
        "Based on the provided„ÄêReference Information„Äë, generate an introduction about company benefits.\n",
        "\n",
        "„ÄêOutput Requirements„Äë\n",
        "1. Start with an enthusiastic welcome greeting.\n",
        "2. Organize the benefits information into categories, such as \"Health & Leave,\" \"Subsidies & Incentives,\" etc.\n",
        "3. Use a relevant emoji before each benefit item to make it more engaging.\n",
        "4. End with best wishes for the new colleague.\n",
        "\n",
        "„ÄêReference Information„Äë\n",
        "{retrieved_text}\n",
        "\"\"\"\n",
        "\n",
        "# Call the model again with the optimized prompt\n",
        "final_response = llm.invoke(optimized_prompt)\n",
        "print(\"--- Response Using Optimized Prompt ---\")\n",
        "print(final_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92c258dc",
      "metadata": {},
      "source": [
        "\n",
        "Through this iteration process, you'll get a response with a warm tone and clear structure‚Äîone that will undoubtedly leave a better first impression on new employees.\n",
        "\n",
        "This example demonstrates the tremendous value of Meta Prompting in improving user experience. It's not just about extracting information, but about **how to better present information**. When you encounter a tricky prompt problem, don't forget that your model itself is the best coach. By clearly describing your goals and challenges, you can guide it to build more powerful, precise, and \"human-touch\" prompts for you.\n",
        "\n",
        "In the initial approach above, we described our expected **qualitative goals** (like \"friendly,\" \"clearer structure\") to the \"AI coach,\" and it directly generated an optimized prompt for us. This method is convenient, but it has a key limitation: the AI coach's understanding of these qualitative goals may not be precise enough, and your description of requirements may not be specific enough, leading to uncertainty in optimization results.\n",
        "\n",
        "To address this problem and make the optimization process more controllable and precise, we need to upgrade from \"qualitative guidance\" to \"quantitative alignment.\" Instead of giving a vague goal, it's better to provide a perfect **\"reference answer\"** as a precise target. The following advanced approach will show you how to use this \"reference answer\" to let the LLM gradually and precisely approach the optimal result through automated gap analysis and iteration. This is a more engineering-oriented improvement approach.\n",
        "\n",
        "#### Multi-Round Iteration: Introducing Reference Answers for Gap Analysis\n",
        "\n",
        "In the previous example, you played the leading role, receiving the \"AI coach's\" suggestions and manually applying them. But this process can be further automated and made more precise. Rather than having the evaluator give a vague \"good\" or \"bad\" judgment, a more advanced method is to introduce a **\"Reference Answer.\"**\n",
        "\n",
        "This \"reference answer\" is your ideal perfect answer, which can be written by human experts or generated by the most powerful model using a very detailed prompt. The goal of iterative optimization becomes: **continuously modify the prompt so that the gap between its generated response and this \"reference answer\" becomes smaller and smaller**.\n",
        "\n",
        "This process is like a self-correcting flow with precise guidance:\n",
        "\n",
        "1. **Set Reference Answer**: First, define a high-quality, ideal \"reference answer.\"\n",
        "2. **Generate**: Use the current prompt to be optimized to generate a response.\n",
        "3. **Analyze Gap**: Have an \"evaluator\" LLM (Critic) compare the \"generated response\" with the \"reference answer\" and output a detailed \"gap analysis report\" pointing out specific differences in tone, structure, content, format, etc.\n",
        "4. **Optimize**: Pass the \"gap analysis report\" along with the original prompt and generated response to an \"optimizer\" LLM. Its task is to rewrite the prompt based on this report to specifically address the issues identified, thereby reducing the gap.\n",
        "5. **Repeat**: Replace the old prompt with the optimized new one, then go back to step 2, until the \"evaluator\" considers the gap small enough or the maximum iterations are reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd70af2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Set Reference Answer\n",
        "reference_answer = \"\"\"\n",
        "üëã Welcome to our big family! I'm excited to introduce our awesome benefits:\n",
        "\n",
        "**üè• Health & Leave, We've Got You Covered:**\n",
        "- **Comprehensive Health Insurance**: Covers you and your family‚Äîwork with peace of mind.\n",
        "- **Annual Health Checkup**: Your health matters to us, always.\n",
        "- **Paid Annual Leave**: A full 15 days to explore poetry and distant places!\n",
        "- **Paid Sick Leave**: 5 days to rest and recover your energy.\n",
        "\n",
        "**üí∞ Subsidies & Incentives, Cheering You On:**\n",
        "- **Transportation Subsidy**: 500 yuan monthly for an easier commute.\n",
        "- **Meal Subsidy**: 300 yuan monthly‚Äîadd an extra drumstick to your lunch!\n",
        "- **Education & Training Fund**: Up to 8000 yuan annually‚Äîinvest in yourself, the future is bright.\n",
        "- **Gym Discounts**: Partnered with multiple gyms‚Äîdon't forget to exercise no matter how busy!\n",
        "\n",
        "Hope these benefits make you feel the company's care! Looking forward to creating more value together! üéâ\n",
        "\"\"\"\n",
        "\n",
        "# 2. Define gap analysis and optimization functions\n",
        "def analyze_gap(generated_response, reference):\n",
        "    gap_analysis_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are a text comparison expert.\n",
        "    „ÄêTask„ÄëPlease compare in detail the gaps between the„ÄêGenerated Response„Äëand the„ÄêReference Answer„Äë.\n",
        "    „ÄêReference Answer„Äë\n",
        "    {reference}\n",
        "    ---\n",
        "    „ÄêGenerated Response„Äë\n",
        "    {generated_response}\n",
        "    ---\n",
        "    „ÄêRequirements„Äë\n",
        "    Please output a detailed gap analysis report covering tone, structure, content details, format (such as emoji usage), etc. If there's almost no gap between them, simply answer \"Gap is minimal.\"\n",
        "    \"\"\"\n",
        "    return llm.invoke(gap_analysis_prompt)\n",
        "\n",
        "def optimize_prompt_with_gap_analysis(current_prompt, generated_response, gap_report):\n",
        "    optimization_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are a top-tier prompt engineer.\n",
        "    „ÄêTask„ÄëBased on the provided \"Gap Analysis Report,\" optimize the \"Current Prompt\" so it can generate output closer to the \"Reference Answer.\"\n",
        "    ---\n",
        "    „ÄêCurrent Prompt„Äë\n",
        "    {current_prompt}\n",
        "    ---\n",
        "    „ÄêGenerated Response„Äë\n",
        "    {generated_response}\n",
        "    ---\n",
        "    „ÄêGap Analysis Report„Äë\n",
        "    {gap_report}\n",
        "    ---\n",
        "    „ÄêRequirements„Äë\n",
        "    Please return only the optimized new prompt without any other explanations.\n",
        "    \"\"\"\n",
        "    return llm.invoke(optimization_prompt)\n",
        "\n",
        "# 3. Iterative optimization loop\n",
        "current_prompt = initial_prompt\n",
        "for i in range(3): # Maximum 3 iterations\n",
        "    print(f\"--- Iteration {i+1} ---\")\n",
        "    generated_response = llm.invoke(current_prompt.format(retrieved_text=retrieved_text))\n",
        "    print(f\"Generated Response (partial):\\n{generated_response[:100]}...\")\n",
        "    \n",
        "    gap_report = analyze_gap(generated_response, reference_answer)\n",
        "    print(f\"Gap Analysis Report:\\n{gap_report}\")\n",
        "    \n",
        "    if \"Gap is minimal\" in gap_report or \"Â∑ÆË∑ùÂæàÂ∞è\" in gap_report:\n",
        "        print(\"\\nEvaluation passed, optimization complete!\")\n",
        "        break\n",
        "    \n",
        "    print(\"\\nEvaluation not passed, optimizing prompt based on gap analysis report...\")\n",
        "    current_prompt = optimize_prompt_with_gap_analysis(current_prompt, generated_response, gap_report)\n",
        "else:\n",
        "    print(\"\\nMaximum iterations reached, stopping optimization.\")\n",
        "\n",
        "final_prompt_based_on_reference = current_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5c8423",
      "metadata": {},
      "source": [
        "This automated iteration process demonstrates the true power of Meta Prompting. It frees you from tedious manual adjustments, letting the model explore and discover the best ways of expression on its own, and provides foundational ideas for designing more complex **AI Self-Improvement** systems.\n",
        "\n",
        "#### Effectiveness Evaluation: Quantifying Your Optimization Results\n",
        "\n",
        "When you have multiple versions of prompts (e.g., initial version vs. single-optimization version vs. multi-round iteration final version), how do you objectively prove which is better? Beyond intuitive feelings, a more scientific approach is **quantitative evaluation**.\n",
        "\n",
        "You can leverage the LLM again, having it play the role of a \"Grader\" to score responses generated by different prompts according to a series of criteria.\n",
        "\n",
        "For example, you can define scoring criteria:\n",
        "\n",
        "* **Friendliness**: 1-5 points\n",
        "* **Clarity of Structure**: 1-5 points\n",
        "* **Information Accuracy**: 1-5 points\n",
        "\n",
        "Then, construct a \"Grader Prompt\" that passes the scoring criteria and the response to be evaluated to the LLM, having it output a structured scoring result (like JSON).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee0fad29",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Three typical response samples with obvious quality differences\n",
        "# Poor: Simple listing of information, no structure or emotion\n",
        "poor_response = \"Company benefits: Health insurance, family can use. 15 days annual leave, 5 days sick leave. Transportation subsidy 500, meal subsidy 300. Training fund 8000. Gym has discounts.\"\n",
        "\n",
        "# Medium: Has basic structure and categories, but bland tone\n",
        "medium_response = \"\"\"\n",
        "Company Benefits:\n",
        "1. üí¶Health and Leave:\n",
        "   - Health insurance (including family)\n",
        "   - Annual checkup\n",
        "   - 15 days annual leave and 5 days sick leave\n",
        "2. üí∞Subsidies and Incentives:\n",
        "   - Monthly 500 transportation subsidy and 300 meal subsidy\n",
        "   - 8000 yuan/year education and training fund\n",
        "   - Partner gym discounts\n",
        "\"\"\"\n",
        "\n",
        "# Good: Clear structure, friendly tone, strong visual appeal (directly use our reference answer)\n",
        "good_response = reference_answer \n",
        "\n",
        "# Design more detailed evaluation dimensions\n",
        "def grade_response_detailed(response_to_grade):\n",
        "    grader_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are an experienced internal communications and employee experience evaluator.\n",
        "    „ÄêTask„ÄëPlease rate the provided \"Company Benefits Introduction\" text on a 1-5 scale across the following four dimensions.\n",
        "    \n",
        "    „ÄêScoring Dimensions„Äë\n",
        "    1. **welcoming_tone**: 1 means cold and stiff tone, 5 means very warm and infectious.\n",
        "    2. **structuring**: 1 means chaotic and disorganized, 5 means clear categories and strong logic.\n",
        "    3. **visual_appeal**: 1 means dull and boring, 5 means good use of emojis, bold text, etc., very eye-catching.\n",
        "    4. **completeness**: 1 means serious information gaps, 5 means all key benefit information is complete.\n",
        "\n",
        "    „ÄêText to Evaluate„Äë\n",
        "    {response_to_grade}\n",
        "    ---\n",
        "    „ÄêOutput Requirements„Äë\n",
        "    Please strictly return your scores in JSON format without any explanations. For example:\n",
        "    {{\"welcoming_tone\": 5, \"structuring\": 4, \"visual_appeal\": 5, \"completeness\": 5}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        raw_output = llm.invoke(grader_prompt)\n",
        "        # Extract JSON portion\n",
        "        json_str = raw_output[raw_output.find('{'):raw_output.rfind('}')+1]\n",
        "        return json.loads(json_str)\n",
        "    except (json.JSONDecodeError, IndexError):\n",
        "        # Error handling, return default low scores when parsing fails\n",
        "        return {\"welcoming_tone\": 1, \"structuring\": 1, \"visual_appeal\": 1, \"completeness\": 1}\n",
        "\n",
        "# Score the three typical samples\n",
        "scores = {\n",
        "    \"Original Answer\": grade_response_detailed(poor_response),\n",
        "    \"Single Iteration Optimize\": grade_response_detailed(medium_response),\n",
        "    \"Multi-turn Iteration Optimize\": grade_response_detailed(good_response)\n",
        "}\n",
        "\n",
        "# Convert scores to DataFrame and visualize\n",
        "df = pd.DataFrame(scores)\n",
        "df = df.reset_index().rename(columns={'index': 'Dim'})\n",
        "df_long = df.melt(id_vars='Dim', var_name='Version', value_name='Score')\n",
        "\n",
        "# --- Grouped bar chart ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(\n",
        "    data=df_long,\n",
        "    x=\"Dim\",\n",
        "    y=\"Score\",\n",
        "    hue=\"Version\",\n",
        "    palette=\"viridis\"\n",
        ")\n",
        "\n",
        "# Add value labels to each bar\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height == 0:\n",
        "        continue\n",
        "    ax.annotate(\n",
        "        f\"{height}\",\n",
        "        (p.get_x() + p.get_width() / 2., height),\n",
        "        ha='center', va='center',\n",
        "        xytext=(0, 5),\n",
        "        textcoords='offset points',\n",
        "        fontsize=11\n",
        "    )\n",
        "\n",
        "ax.set_ylim(0, 6)\n",
        "ax.set_ylabel('Score (1-5)', fontsize=12)\n",
        "ax.set_xlabel('')\n",
        "ax.set_title('Evaluation', fontsize=20)\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512d2180",
      "metadata": {},
      "source": [
        "\n",
        "You can see that the LLM's response quality improves to varying degrees with each iteration. Through this quantitative evaluation, you can not only intuitively see the improvement from each optimization but also transform vague feelings of \"good\" or \"bad\" into clear, measurable data. This provides you with a scientific method to validate and iterate your prompt strategies, ensuring every improvement is evidence-based, ultimately delivering truly high-quality user experiences.\n",
        "\n",
        "### 4.7 Let the LLM Help You Build a Custom \"AI Judge\"\n",
        "\n",
        "The automated optimization loop in the previous section is very powerful, but it raises a key engineering question: when does this loop stop?\n",
        "\n",
        "The key issue is that relying on vague evaluations like \"is the gap big or small\" is unreliable. You need an \"**AI Judge**\" that can give a clear \"**‚úÖ Pass**\" or \"**‚ùå Fail**\" conclusion based on preset quantified standards for each optimization result, thereby providing a clear and reliable stopping condition for your automated workflow.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <a href=\"https://img.alicdn.com/imgextra/i4/O1CN01Ie6KDU1pMWUJ2Ky25_!!6000000005346-2-tps-1115-614.png\" target=\"_blank\">\n",
        "      <img src=\"https://img.alicdn.com/imgextra/i4/O1CN01Ie6KDU1pMWUJ2Ky25_!!6000000005346-2-tps-1115-614.png\" width=\"700\">\n",
        "    </a>\n",
        "  <p>Figure: Constructing an \"AI Judge\" via Meta-Prompting to replace the <code>analyze_gap()</code> method and enhance judgment accuracy.</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "#### The \"AI Judge\" Training Process\n",
        "\n",
        "The process of building an \"AI Judge\" can be seen as training a machine learning classifier. The \"AI Judge\" prompt you're building is the \"model\" to be trained, and the standard samples you prepare are the \"dataset.\" The entire process is using data to \"train\" your \"model.\"\n",
        "\n",
        "1. **Prepare Dataset**: First create a batch of high-quality labeled samples (containing both \"good answers\" and \"bad answers\").\n",
        "2. **Iterative \"Training\"**:\n",
        "   - a. Automatically split this data proportionally (e.g., 7:3) into \"training set\" and \"evaluation set.\"\n",
        "   - b. Use the current \"judge prompt\" to make \"predictions\" on both the **training set** and **evaluation set**, recording the accuracy scores for each.\n",
        "   \n",
        "   Why monitor both scores? Tracking changes in these two scores helps you determine if the \"model\" is \"overfitting\"‚Äîmeaning it just \"memorized\" the training questions but didn't truly improve its judgment ability on new questions (evaluation set).\n",
        "   \n",
        "   - c. Find the \"wrong answers\" on the **training set**.\n",
        "   - d. Take these \"wrong answers\" along with your analysis, and through Meta Prompting, have the LLM help you optimize the \"judge prompt\" to generate a new version.\n",
        "   - e. Repeat steps b-d. You can stop \"training\" at any time.\n",
        "\n",
        "3. **Stop \"Training\"**:\n",
        "   In each iteration, you need to intervene and analyze:\n",
        "   - Are both training accuracy and evaluation accuracy steadily improving?\n",
        "   - Has evaluation accuracy stagnated or suddenly dropped (possible overfitting)?\n",
        "   - Do the newly generated \"wrong answers\" expose ambiguity or contradictions in the samples themselves? (Poor sample quality)\n",
        "\n",
        "> Note: If sample quality is poor, causing inconsistent evaluation standards or ambiguous judgments on similar questions, the LLM may never be able to construct a well-performing \"AI Judge.\" At this point, you can stop iteration and first optimize your sample data.\n",
        "\n",
        "Finally, you decide when to stop \"training\" and adopt the version of the prompt that performs best on the evaluation set as your final \"**AI Judge**.\"\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "    <a href=\"https://img.alicdn.com/imgextra/i1/O1CN01gJT1AY22xhe34lZwM_!!6000000007187-2-tps-1864-598.png\" target=\"_blank\">\n",
        "    <img src=\"https://img.alicdn.com/imgextra/i1/O1CN01gJT1AY22xhe34lZwM_!!6000000007187-2-tps-1864-598.png\" width=\"800\"/>\n",
        "    </a>\n",
        "  <p>Figure: Iterating the AI Judge via ML training; stop based on train vs. eval performance.</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "#### 4.7.1 Prepare and Split the Dataset\n",
        "\n",
        "First, you need a batch of samples carefully labeled by you or domain experts. This is the foundation of the entire process and the cornerstone of quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2778ce00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare a batch of high-quality labeled samples\n",
        "# In practice, these samples can be uploaded to AI platforms (like Bailian) to create evaluation sets\n",
        "labeled_samples = [\n",
        "    # --- 3 samples for \"training\" ---\n",
        "    {\n",
        "        \"id\": \"train_01\",\n",
        "        \"response\": \"Company benefits: Health insurance, family can use. 15 days annual leave, 5 days sick leave. Transportation subsidy 500, meal subsidy 300. Training fund 8000. Gym has discounts.\", \n",
        "        \"label\": \"Fail\", \n",
        "    },\n",
        "    {\n",
        "        \"id\": \"train_02\",\n",
        "        \"response\": \"\"\"Company benefits include:\\n1. Health and leave: Health insurance, annual checkup, 15 days annual leave and 5 days sick leave.\\n2. Subsidies and incentives: Monthly 500 transportation subsidy and 300 meal subsidy, plus 8000 yuan training fund.\"\"\", \n",
        "        \"label\": \"Fail\", \n",
        "    },\n",
        "    {\n",
        "        \"id\": \"train_03\",\n",
        "        \"response\": \"\"\"üëã Welcome! We've prepared awesome benefits for you:\\n- üè• Comprehensive health insurance and annual checkup\\n- üå¥ 15 days annual leave + 5 days sick leave\\n- üí∞ Monthly transportation and meal subsidies\\n- üéì Up to 8000 yuan training fund\\nLooking forward to working with you! üéâ\"\"\", \n",
        "        \"label\": \"Pass\", \n",
        "    },\n",
        "    \n",
        "    # --- 2 samples for \"evaluation\" ---\n",
        "    {\n",
        "        \"id\": \"eval_01\",\n",
        "        \"response\": \"We have health insurance, annual leave, sick leave, transportation and meal subsidies, training fund and gym discounts.\", \n",
        "        \"label\": \"Fail\", \n",
        "    },\n",
        "    {\n",
        "        \"id\": \"eval_02\",\n",
        "        \"response\": \"\"\"Hello, new colleague! Company benefits are great:\\n- Health includes insurance and checkups.\\n- Leave includes annual and sick leave.\\n- Money includes subsidies and education fund.\\nWish you a happy work experience!\"\"\", \n",
        "        \"label\": \"Pass\", \n",
        "    },\n",
        "]\n",
        "\n",
        "# Split dataset proportionally (manual simulation here)\n",
        "# In real projects, use train_test_split from scikit-learn for random splitting\n",
        "train_set = labeled_samples[:3]\n",
        "eval_set = labeled_samples[3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca622ab",
      "metadata": {},
      "source": [
        "\n",
        "#### 4.7.2 Iterative \"Training\"\n",
        "\n",
        "By having the model continuously learn from \"wrong answers,\" the \"judge prompt\" becomes smarter and smarter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11564b9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a helper function to evaluate prompt accuracy on a specified dataset\n",
        "def evaluate_judge_prompt(judge_prompt, dataset):\n",
        "    correct_predictions = 0\n",
        "    misjudged_cases = []\n",
        "    \n",
        "    for sample in dataset:\n",
        "        prompt = judge_prompt.format(response_to_judge=sample[\"response\"])\n",
        "        \n",
        "        # Simulate LLM call, actual: predicted_label = llm.invoke(prompt)\n",
        "        # For demonstration, manually simulate different prompt version behaviors\n",
        "        predicted_label = \"Pass\" if \"Welcome\" in sample[\"response\"] or \"Hello\" in sample[\"response\"] else \"Fail\"\n",
        "        if \"v1\" in judge_prompt:\n",
        "             if \"1.\" in sample[\"response\"]: predicted_label = \"Pass\"\n",
        "\n",
        "        is_correct = (predicted_label == sample[\"label\"])\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "        else:\n",
        "            misjudged_cases.append({\n",
        "                \"response\": sample[\"response\"],\n",
        "                \"current_judgment\": predicted_label,\n",
        "                \"correct_judgment\": sample[\"label\"]\n",
        "            })\n",
        "            \n",
        "    accuracy = correct_predictions / len(dataset)\n",
        "    return accuracy, misjudged_cases\n",
        "\n",
        "# ----------------- Iterative \"Training\" Begins -----------------\n",
        "\n",
        "# a. Prepare initial version of \"judge prompt\" (v1.0)\n",
        "judge_prompt_v1 = \"\"\"\n",
        "# v1\n",
        "„ÄêRole„ÄëYou are an employee experience expert.\n",
        "„ÄêTask„ÄëJudge whether the„ÄêResponse to Evaluate„Äëis qualified.\n",
        "„ÄêEvaluation Criteria„Äë\n",
        "1. Clear structure: Information listed in points or categories.\n",
        "2. Complete information: Mentions core benefits.\n",
        "„ÄêResponse to Evaluate„Äë\n",
        "---\n",
        "{response_to_judge}\n",
        "---\n",
        "„ÄêOutput Requirements„ÄëPlease only answer \"Pass\" or \"Fail\".\n",
        "\"\"\"\n",
        "\n",
        "current_judge_prompt = judge_prompt_v1\n",
        "max_iterations = 2\n",
        "\n",
        "for i in range(max_iterations):\n",
        "    print(f\"--- Iteration {i+1} ---\")\n",
        "    \n",
        "    # b. Make \"predictions\" on training and evaluation sets and record scores\n",
        "    train_accuracy, misjudged_on_train = evaluate_judge_prompt(current_judge_prompt, train_set)\n",
        "    eval_accuracy, _ = evaluate_judge_prompt(current_judge_prompt, eval_set)\n",
        "    \n",
        "    print(f\"Training Set Accuracy: {train_accuracy:.0%}\")\n",
        "    print(f\"Evaluation Set Accuracy: {eval_accuracy:.0%}\")\n",
        "\n",
        "    if not misjudged_on_train:\n",
        "        print(\"\\nNo errors on training set, training complete.\")\n",
        "        break\n",
        "        \n",
        "    # c. Find \"wrong answers\" on training set\n",
        "    first_error = misjudged_on_train[0]\n",
        "    print(f\"\\nFound wrong answer: Model misjudged a response that should be '{first_error['correct_judgment']}' as '{first_error['current_judgment']}'\")\n",
        "\n",
        "    # d. Package \"wrong answers\" and have LLM help optimize the prompt\n",
        "    judge_optimizer_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are a top-tier prompt engineer.\n",
        "    „ÄêBackground„ÄëMy current judge prompt made an error when evaluating a sample.\n",
        "    \n",
        "    „ÄêMy Current Judge Prompt„Äë\n",
        "    ---\n",
        "    {current_judge_prompt}\n",
        "    ---\n",
        "    \n",
        "    „ÄêError Case„Äë\n",
        "    - Response to evaluate: \"{first_error['response']}\"\n",
        "    - My tool's wrong judgment: \"{first_error['current_judgment']}\"\n",
        "    - Expected correct judgment: \"{first_error['correct_judgment']}\"\n",
        "    - Reason I think it erred: Current criteria are too loose, didn't emphasize 'warm and friendly' tone.\n",
        "    \n",
        "    „ÄêTask„ÄëPlease rewrite my judge prompt to make criteria stricter and correct the above error.\n",
        "    „ÄêRequirements„ÄëPlease only return the optimized new prompt.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Optimizing prompt based on wrong answers...\")\n",
        "    new_judge_prompt = \"\"\"\n",
        "    # v2\n",
        "    „ÄêRole„ÄëYou are an internal communications expert pursuing ultimate employee experience, with a discerning eye.\n",
        "    „ÄêTask„ÄëStrictly judge whether the„ÄêResponse to Evaluate„Äëis qualified.\n",
        "    „ÄêEvaluation Criteria„Äë\n",
        "    1. **Welcoming Atmosphere (Required)**: Must have clear, warm welcome greeting.\n",
        "    2. **Structured Presentation (Required)**: Must use lists or sections.\n",
        "    „ÄêResponse to Evaluate„Äë\n",
        "    ---\n",
        "    {response_to_judge}\n",
        "    ---\n",
        "    „ÄêOutput Requirements„ÄëPlease only answer \"Pass\" or \"Fail\".\n",
        "    \"\"\"\n",
        "    \n",
        "    # e. Update to new version prompt, enter next iteration\n",
        "    current_judge_prompt = new_judge_prompt\n",
        "    print(\"-\" * 20 + \"\\n\")\n",
        "\n",
        "print(\"\\n--- Iteration Ended ---\")\n",
        "final_judge_prompt = current_judge_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e57a51",
      "metadata": {},
      "source": [
        "\n",
        "#### 4.7.3 \"Good Enough\" Means You Can \"Stop Training\"\n",
        "\n",
        "In the iteration loop above, you can clearly see the score changes:\n",
        "- Iteration 1:\n",
        "    - Training Set Accuracy: 67% (v1 prompt misjudged train_02)\n",
        "    - Evaluation Set Accuracy: 100% (v1 prompt happened to judge both correctly)\n",
        "- Iteration 2:\n",
        "    - Training Set Accuracy: 100% (v2 prompt corrected the error)\n",
        "    - Evaluation Set Accuracy: 100%\n",
        "\n",
        "In this simple example, evaluation accuracy stayed high throughout. But in real scenarios, you might see evaluation accuracy stagnate or even decline.\n",
        "\n",
        "Based on the above data, you'll find that the v2 version prompt achieved 100% accuracy on both training and evaluation sets. Therefore, you can adopt `judge_prompt_v2` as your final, validated \"AI Judge.\" If evaluation results are unsatisfactory, you may need to go back to the beginning and check and optimize your `labeled_samples`.\n",
        "\n",
        "#### 4.7.4 Use the \"AI Judge\" to Guide Automated Optimization\n",
        "\n",
        "Now you have an \"AI Judge\" (`final_judge_prompt`) that's been repeatedly validated and tested, ready for deployment.\n",
        "\n",
        "You need to use it to replace the evaluation step in section 4.6 that compared differences between two samples to determine usability, making the entire workflow more rigorous.\n",
        "\n",
        "**New Workflow:**\n",
        "1. **Generate**: Use the current \"generation prompt\" to produce a response.\n",
        "2. **Judge**: Call your trained \"AI Judge prompt\" to make a \"Pass/Fail\" judgment on the response.\n",
        "3. **Decide & Optimize**:\n",
        "    - If the judgment is \"**Pass**,\" the loop ends‚Äîoptimization successful.\n",
        "    - If the judgment is \"**Fail**,\" pass the \"generation prompt,\" \"failed response,\" and the \"**AI Judge's rulebook**\" (i.e., its prompt) together to the \"optimizer,\" letting it generate a new version prompt based on the clear failure criteria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0638b596",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a more powerful optimization function that directly references the \"judge's\" rules\n",
        "def optimize_with_judge_rules(current_prompt, failed_response, judge_prompt):\n",
        "    \"\"\"Call LLM to optimize generation prompt based on judge's rules\"\"\"\n",
        "    optimizer_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are a top-tier prompt engineer.\n",
        "    „ÄêBackground„ÄëI have a \"generation prompt,\" but the response it generates failed the \"AI Judge's\" review.\n",
        "    \n",
        "    „ÄêGeneration Prompt„Äë\n",
        "    ---\n",
        "    {current_prompt}\n",
        "    ---\n",
        "    \n",
        "    „ÄêIts Failed Response„Äë\n",
        "    ---\n",
        "    {failed_response}\n",
        "    ---\n",
        "    \n",
        "    „ÄêAI Judge's Rulebook (Reason for Failure)„Äë\n",
        "    ---\n",
        "    {judge_prompt}\n",
        "    ---\n",
        "    \n",
        "    „ÄêTask„Äë\n",
        "    Please carefully study the \"AI Judge's Rulebook\" and rewrite the \"Generation Prompt\" to ensure the new prompt can generate a response that passes the rulebook's review.\n",
        "    \n",
        "    „ÄêRequirements„Äë\n",
        "    Please only return the optimized new \"Generation Prompt\" without any other explanations.\n",
        "    \"\"\"\n",
        "    new_prompt = llm.invoke(optimizer_prompt)\n",
        "    return new_prompt\n",
        "\n",
        "# --- Run the automated optimization loop driven by \"AI Judge\" ---\n",
        "current_generating_prompt = initial_prompt\n",
        "max_iterations = 5\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"Starting the final automated optimization workflow driven by 'AI Judge'\")\n",
        "print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "for i in range(max_iterations):\n",
        "    print(f\"--- Final Workflow: Iteration {i+1} ---\")\n",
        "    \n",
        "    # 1. Generate response\n",
        "    prompt_for_generator = current_generating_prompt.format(retrieved_text=retrieved_text)\n",
        "    generated_response = llm.invoke(prompt_for_generator)\n",
        "    \n",
        "    print(f\"Generated Response:\\n---\\n{generated_response}\\n---\")\n",
        "    \n",
        "    # 2. Call \"AI Judge\" for judgment\n",
        "    judge_prompt_filled = final_judge_prompt.format(response_to_judge=generated_response)\n",
        "    judgment = llm.invoke(judge_prompt_filled)\n",
        "    \n",
        "    judgment_cleaned = judgment.strip().replace(\".\", \"\")\n",
        "    print(f\"'AI Judge' Verdict: {judgment_cleaned}\")\n",
        "\n",
        "    # 3. Decision\n",
        "    if judgment_cleaned == \"Pass\" or judgment_cleaned == \"ÈÄöËøá\":\n",
        "        print(\"\\n‚úÖ Optimization successful! Generated response passed the 'AI Judge' review.\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"‚ùå Did not pass review, optimizing based on 'Judge's' rules...\")\n",
        "        current_generating_prompt = optimize_with_judge_rules(\n",
        "            current_generating_prompt, \n",
        "            generated_response, \n",
        "            final_judge_prompt\n",
        "        )\n",
        "        print(\"-\" * 20 + \"\\n\")\n",
        "else:\n",
        "    print(\"\\nMaximum iterations reached, stopping optimization.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"Final Adopted Generation Prompt:\")\n",
        "print(\"=\"*30)\n",
        "print(current_generating_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96397dc1",
      "metadata": {},
      "source": [
        "The \"**AI Judge**\" makes your prompt optimization workflow more controllable:\n",
        "- Whenever the \"generation prompt\" produces a new response, first have the \"AI Judge\" score it.\n",
        "- **Passed means optimization successful; failed means use this \"bad answer\" as new material to continue iterating your generation prompt**.\n",
        "\n",
        "You can always adopt this approach: **first build the \"judge,\" then optimize the \"player.\"** Build a measurable, trustworthy, automatable \"**Prompt Optimization Engineering**\" system."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2773f275",
      "metadata": {},
      "source": [
        "## 5. Reasoning Models\n",
        "\n",
        "The prompt techniques and frameworks discussed earlier can be broadly applied to general-purpose LLMs (such as Qwen2.5-max, GPT-4, DeepSeek-V3), which are designed for general conversation, knowledge Q&A, text generation, and other broad scenarios. In addition to general-purpose LLMs, there is another category of models specifically designed for \"reasoning\" ‚Äî `Reasoning Models`.\n",
        "\n",
        "### 5.1 What Are Reasoning Models?\n",
        "\n",
        "You may have already learned about Alibaba Cloud's reasoning model [Qwen3](https://help.aliyun.com/zh/model-studio/deep-thinking) through the extended reading in section 2.1. For convenience in subsequent calls in this section, we've rewritten the code provided in section 2.1. You can try running the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0f362f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "def reasoning_model_response(user_prompt, system_prompt=\"You are a programming assistant.\", model=\"qwen-plus\"):\n",
        "    \"\"\"\n",
        "    prompt: User input prompt\n",
        "    model: Using qwen-plus as an example, can be replaced with other reasoning model names like: deepseek-r1\n",
        "    \"\"\"\n",
        "    # Initialize client\n",
        "    client = OpenAI(\n",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "        base_url=os.getenv(\"DASHSCOPE_API_BASE\")\n",
        "    )\n",
        "\n",
        "    # Initialize state variable\n",
        "    is_answering = False\n",
        "\n",
        "    # Make streaming request\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        extra_body={\n",
        "            \"enable_thinking\": True,\n",
        "            \"thinking_budget\": 4000\n",
        "        },\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    # Print thinking process title\n",
        "    print(\"\\n\" + \"=\" * 20 + \"Thinking Process\" + \"=\" * 20 + \"\\n\")\n",
        "\n",
        "    # Process streaming response\n",
        "    for chunk in completion:\n",
        "        if chunk.choices:\n",
        "            delta = chunk.choices[0].delta\n",
        "            if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:\n",
        "                # Process thinking content\n",
        "                print(delta.reasoning_content, end='', flush=True)\n",
        "            else:\n",
        "                # Switch to answer output mode\n",
        "                if delta.content != \"\" and not is_answering:\n",
        "                    print(\"\\n\" + \"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")\n",
        "                    is_answering = True\n",
        "                # Process answer content\n",
        "                if delta.content:\n",
        "                    print(delta.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb63fefe",
      "metadata": {},
      "outputs": [],
      "source": [
        "reasoning_model_response(user_prompt=\"Who are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1e4817",
      "metadata": {},
      "source": [
        "\n",
        "Through this example, you can see that reasoning models have an additional \"`thinking process`\" compared to general-purpose LLMs. It's like solving a math problem where someone first works through it step by step on scratch paper rather than giving the answer directly. This reduces \"off-the-cuff\" errors from the model. During the step-by-step thinking process, if a contradiction is found at a certain step, the model can go back to check and readjust its approach. Showing reasoning steps also makes it easier for people to understand and verify the logic by following the model's line of thinking.\n",
        "\n",
        "Compared to general-purpose LLMs, reasoning models are usually more reliable when solving complex problems, such as in scenarios requiring rigorous reasoning like math problem solving, code writing, or legal case analysis. This doesn't mean reasoning models are always better‚Äîboth types of models have their respective use cases. The following table compares these two types of models across some typical dimensions:\n",
        "\n",
        "| Dimension | Reasoning Model | General-Purpose Model |\n",
        "|-----------|-----------------|----------------------|\n",
        "| Design Goal | Focused on **logical reasoning, multi-step problem solving, mathematical calculations** and other tasks requiring deep analysis | Oriented toward **general conversation, knowledge Q&A, text generation** and other broad scenarios |\n",
        "| Training Data Focus | Large amounts of **math solutions, code logic, scientific reasoning** datasets to enhance reasoning ability | Covers **encyclopedias, literature, conversations** and massive data from multiple domains |\n",
        "| Typical Output Characteristics | Output includes **complete derivation** steps, emphasizing the completeness of logical chains | Output is **concise and direct**, focusing on natural language expression of results |\n",
        "| Response Speed | Complex reasoning tasks **respond slower** (requires multi-step calculations) | Regular tasks **respond faster** (primarily single-step generation) |\n",
        "\n",
        "Reasoning model or general-purpose model? How to choose? Here are some recommendations:\n",
        "- **Clear general tasks**: For clearly defined problems, **general-purpose models** can usually handle them well.\n",
        "- **Complex tasks**: For very complex tasks that require relatively **more precise and reliable** answers, **reasoning models** are recommended. These tasks might be:\n",
        "    - Ambiguous tasks: Very little information related to the task, and you cannot provide the model with relatively clear guidance.\n",
        "    - Needle in a haystack: Passing large amounts of unstructured data to extract the most relevant information or find correlations/differences.\n",
        "    - Debugging and improving code: Need to review and further debug, improve large amounts of code.\n",
        "- **Speed and cost**: Generally speaking, reasoning models take longer for inference. If you're sensitive to time and cost and the task complexity isn't high, **general-purpose models** might be a better choice.\n",
        "\n",
        "Of course, you can also combine both types of models in your application: use reasoning models for Agent planning and decision-making, and use general-purpose models for task execution.\n",
        "\n",
        "### 5.2 Prompt Techniques Suitable for Reasoning Models\n",
        "\n",
        "Reasoning models can provide detailed and well-formatted responses even when facing relatively ambiguous tasks. You can still use **prompt techniques** to ensure a baseline quality for reasoning model inference:\n",
        "\n",
        "#### Technique 1: Keep task prompts concise and clear, provide sufficient background information\n",
        "\n",
        "The **clear expression of requirements** introduced in section 4.1 also applies to reasoning models. Although reasoning models are very capable, they cannot \"read minds.\" You need to keep prompts **concise and clear** so the reasoning model can focus on the core task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ada2b565",
      "metadata": {},
      "outputs": [],
      "source": [
        "bad_prompt=\"\"\"\n",
        "def example(a):\n",
        "  b = []\n",
        "  for i in range(len(a)):\n",
        "    b.append(a[i]*2)\n",
        "  return sum(b)\n",
        "\"\"\"\n",
        "\n",
        "reasoning_model_response(user_prompt=bad_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fad3115",
      "metadata": {},
      "source": [
        "Through the above example, you can see that even if you only give the reasoning model a piece of code, it can still provide a very rich answer through a series of reasoning. However, the returned reasoning may contain a lot of information you don't care about. You can try to clarify the **task objective** to get more targeted suggestions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1954518f",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_A=\"\"\"\n",
        "What's wrong with the following Python code? How can it be optimized?\n",
        "def example(a):\n",
        "  b = []\n",
        "  for i in range(len(a)):\n",
        "    b.append(a[i]*2)\n",
        "  return sum(b)\n",
        "\"\"\"\n",
        "\n",
        "reasoning_model_response(user_prompt=prompt_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a425bbd6",
      "metadata": {},
      "source": [
        "Similarly, you can further limit the scope by combining techniques from section **4.2 Defining Role and Audience** and **4.3 Specifying Output Format** to ensure results meet your expectations.\n",
        "\n",
        "Also, if the prompt is complex, you can use **delimiters** to help the model understand your intent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14e0766e",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_B=\"\"\"\n",
        "<audience>Junior Python Developer</audience>\n",
        "\n",
        "<task>Function performance optimization, optimize the code in code.</task>\n",
        "\n",
        "<format>\n",
        "If there are multiple optimization solutions, please output in the following format:\n",
        "„ÄêOptimization Solution X„Äë\n",
        "Problem Description: [Description]\n",
        "Optimization Solution: [Description]\n",
        "Sample Code: [Code Block]\n",
        "</format>\n",
        "\n",
        "<code>\n",
        "def example(a):\n",
        "  b = []\n",
        "  for i in range(len(a)):\n",
        "    b.append(a[i]*2)\n",
        "  return sum(b)\n",
        "</code>\n",
        "\"\"\"\n",
        "\n",
        "reasoning_model_response(user_prompt=prompt_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c26a58af",
      "metadata": {},
      "source": [
        "#### Technique 2: Avoid Chain-of-Thought Prompting\n",
        "\n",
        "In section 4.5, you learned about using Chain-of-Thought (COT) techniques to make LLMs think deeply and improve response quality.\n",
        "\n",
        "Generally, you don't need to prompt reasoning models to \"think step by step\" or \"explain your reasoning\" because they inherently engage in deep thinking. Your prompting might actually limit the reasoning model's performance. Unless you need the model to reason strictly according to a fixed approach‚Äîwhich rarely happens.\n",
        "\n",
        "#### Technique 3: Adjust Prompts Based on Model Responses\n",
        "\n",
        "Reasoning models, because of their response format (including the **thinking process**), are naturally suited for you to analyze their thinking and reasoning process, making it easier for you to adjust prompts.\n",
        "\n",
        "Therefore, you don't need to worry about whether the prompt is perfect enough. Just keep conversing with the reasoning model, supplementing information and refining the prompt along the way.\n",
        "\n",
        "For example, when your description is **too abstract** or you **cannot describe accurately**, you can use the **adding examples** technique from section 4.4 to clarify this information. These examples can sometimes be selected from the conversation history with the model.\n",
        "\n",
        "This process can be repeated multiple times‚Äîkeep trying and adjusting prompts, letting the model reason and iterate, until it meets your requirements.\n",
        "\n",
        "#### Technique 4: Let Reasoning Models Be Your \"Prompt Coach\"\n",
        "\n",
        "In section 4.6, you learned about **Meta Prompting**‚Äîletting LLMs help you optimize prompts. So, which type of model is best suited to play the role of this \"coach\"?\n",
        "\n",
        "The answer is **reasoning models**.\n",
        "\n",
        "Because reasoning models excel at step-by-step thinking and logical derivation, they perform particularly well at analyzing the pros and cons of a prompt and systematically proposing improvement suggestions. They can not only give you a better prompt but also clearly show \"why\" this change would be better, allowing you to learn the essence of prompt engineering during the optimization process.\n",
        "\n",
        "Let's revisit the example from section 4.6, but this time, we'll use the `reasoning_model_response` function defined in section 6.1 to act as our prompt coach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "799f4f6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll reuse the company benefits scenario from section 4.6\n",
        "retrieved_text = \"\"\"\n",
        "Regarding company benefits, we provide comprehensive health insurance covering employees and their immediate family members.\n",
        "Annual health checkups are standard. Additionally, there are 15 days of paid annual leave and 5 days of paid sick leave per year.\n",
        "We also provide a monthly transportation subsidy of 500 yuan and a meal subsidy of 300 yuan.\n",
        "To encourage employee growth, the company has an annual education and training fund of up to 8000 yuan, which employees can apply to use for courses or professional books.\n",
        "For fitness, the company has partnerships with multiple gyms where employees can enjoy discounted rates.\n",
        "\"\"\"\n",
        "\n",
        "# Initial, relatively simple prompt\n",
        "initial_prompt = f\"\"\"\n",
        "Based on the following information, answer new employees' questions about company benefits.\n",
        "\n",
        "„ÄêReference Information„Äë\n",
        "{retrieved_text}\n",
        "\"\"\"\n",
        "\n",
        "# Assume this is the less-than-ideal output from a general-purpose model\n",
        "initial_response = \"\"\"\n",
        "Our company provides comprehensive health insurance covering employees and their families. There are 15 days of paid annual leave and 5 days of sick leave per year. There's also a monthly 500 yuan transportation subsidy and 300 yuan meal subsidy. The company provides an 8000 yuan annual education fund and has partnership discounts with gyms.\n",
        "\"\"\"\n",
        "\n",
        "# Build our Meta Prompt, requesting the reasoning model to help optimize\n",
        "meta_prompt_for_reasoning = f\"\"\"\n",
        "I'm optimizing a prompt for our company's new employee Q&A bot, with the goal of answering questions about \"company benefits.\"\n",
        "\n",
        "This is my first attempt:\n",
        "---\\n{initial_prompt}\\n---\n",
        "\n",
        "This is the output it generated:\n",
        "---\\n{initial_response}\\n---\n",
        "\n",
        "This output isn't good enough. I want the bot's responses to be more engaging, clearly structured, and help new employees quickly grasp the key points. Specific requirements:\n",
        "1. **Tone**: Friendly, warm, with a welcoming feeling for new colleagues.\n",
        "2. **Structure**: Use clear bullet points (e.g., lists starting with emojis) to organize content.\n",
        "3. **Content**: Categorize benefits into groups like \"Health & Leave,\" \"Subsidies & Incentives,\" etc.\n",
        "\n",
        "Please act as a prompt engineering expert and rewrite this prompt to achieve the above goals.\n",
        "In your final answer, only provide the optimized prompt itself, without any other explanatory text.\n",
        "\"\"\"\n",
        "\n",
        "# Use reasoning model to get optimization suggestions\n",
        "reasoning_model_response(user_prompt=meta_prompt_for_reasoning, system_prompt=\"You are a top-tier prompt engineering expert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b2605f",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Through the above example, you can clearly see how the reasoning model analyzes the shortcomings of the original prompt step by step and, combined with our requirements (friendly tone, clear structure, engaging content, etc.), ultimately constructs an optimized prompt with more rigorous structure and clearer intent.\n",
        "\n",
        "This fully demonstrates the importance of using the right tool for the right task. Behind this is the trade-off between **task complexity** and **execution cost (time and money)**:\n",
        "\n",
        "- **General-Purpose Models (Non-Reasoning Models)**:\n",
        "    - **Advantages**: Fast execution speed, lower cost.\n",
        "    - **Use Cases**: Suitable for executing relatively straightforward, clearly defined tasks, such as information extraction, format conversion, or simple Q&A based on an already-optimized prompt.\n",
        "\n",
        "- **Reasoning Models**:\n",
        "    - **Advantages**: Excel at handling complex, ambiguous, or tasks requiring deep logical derivation.\n",
        "    - **Use Cases**: More suitable for executing \"meta-tasks,\" such as analyzing and optimizing the definition of another task (i.e., the prompt itself), complex planning, or debugging code.\n",
        "    - **Cost**: Due to the need for multi-step thinking, response time is usually longer and costs are relatively higher.\n",
        "\n",
        "When building your own LLM applications, **hybrid use of both types of models** is often the strategy for achieving the best cost-effectiveness. You can learn from this approach to build an intelligent system with \"clear division of labor\":\n",
        "\n",
        "When tasks require deep thinking or planning, bring in reasoning models to act as the \"**Planner**\" or \"**Analyst**\" role, letting them decompose complex tasks or optimize workflows; then, hand off the decomposed, simpler subtasks to **general-purpose models** or other tools for efficient, low-cost execution. This collaborative model allows you to maintain high-quality output while effectively controlling application response time and operating costs.\n",
        "\n",
        "## ‚úÖ Section Summary\n",
        "\n",
        "Through studying this section, you've mastered more advanced techniques in **Context Engineering**, from designing fine-grained prompts to implementing intelligent intent routing. Looking back at the past two sections, we've equipped the Q&A bot with two core capabilities:\n",
        "\n",
        "1. **Knowledge Filling (via RAG)**: Solved the problem of the model \"not knowing\" private information.\n",
        "2. **Behavior Guidance (via Prompts and Control Flow)**: Solved the problems of \"how to do\" and \"what to do\" for the model.\n",
        "\n",
        "The common core of these techniques is the essence of context engineering:\n",
        "\n",
        "By carefully designing and filling the model's context window to guide and control its generation of desired outputs‚Äîthis is crucial for you to use LLMs effectively.\n",
        "\n",
        "In the actual process of deploying LLM applications, the prompt part is often co-designed with domain experts. Therefore, instead of hardcoding prompts in your engineering code, you should consider making them configurable‚Äîeven the application workflow can be configurable. This makes it easier for domain experts to participate in designing prompts and the entire workflow. Alibaba Cloud Model Studio provides visual [application building](https://help.aliyun.com/zh/model-studio/user-guide/application-introduction#7c79befb2djg9) capabilities, allowing prompt writing and visual construction of the entire complex application workflow on the page‚Äîvery suitable for LLM application development projects that require participation from domain experts without technical backgrounds.\n",
        "\n",
        "In the next section, you'll learn automated evaluation methods to test the Q&A bot's performance. You can use quantitative metrics to evaluate the optimization effects you've achieved through context engineering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3166b70c",
      "metadata": {},
      "source": [
        "## üî• Quiz\n",
        "\n",
        "### üîç Multiple choice question\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>Which of the following prompt elements is used to explicitly define the task that the LLM needs to complete‚ùì (Select 1.)</b>\n",
        "\n",
        "- A. Role\n",
        "- B. Audience\n",
        "- C. Objective\n",
        "- D. Context\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: C**</br>\n",
        "üìù **Explanation**:</br>\n",
        "- The Objective explicitly defines the operation or outcome that the large language model (LLM) is required to perform. Other options do not directly define the task itself.\n",
        "- Role defines the identity that the LLMs should assume, Audience specifies the target group, and Context provides background information.\n",
        "\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>You have inherited a complex, undocumented prompt designed for code generation. Before directly modifying this \"black-box\" prompt, what preparatory steps would you take to ensure your optimization efforts are effective? (Select all that apply.)</b>\n",
        "\n",
        "- A. Collect concrete examples of poor model output (\"bad cases\").\n",
        "- B. Immediately start drafting a new, simpler version from scratch.\n",
        "- C. Clearly define the desired output criteria and format in writing.\n",
        "- D. Read through the entire prompt and try to identify logical flaws based on intuition.\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: A, C**</br>\n",
        "üìù **Explanation**:</br>\n",
        "This question tests what core inputs are necessary before starting a structured optimization process. Effective optimization requires a clear definition of the \"problem\" and the \"goal.\"\n",
        "- Option A (bad cases) and Option C (desired output) are the key steps for defining the \"problem\" and the \"goal,\" respectively. They serve as the core inputs for a structured optimization approach like meta-prompting.\n",
        "- Options B (starting from scratch) and D (relying on intuition) are high-risk, inefficient methods because they do not rely on a structured, evidence-based approach.\n",
        "\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>When using a reasoning LLM (such as `qwen3-235b-a22b-thinking-2507`) for a complex reasoning task, which of the following prompting techniques are recommended‚ùì (Select all that apply.)</b>\n",
        "\n",
        "- A. Provide simple and clear task instructions.\n",
        "- B. Supplement with sufficient background information.\n",
        "- C. If there are specific user groups or task requirements, define a role or audience.\n",
        "- D. You must include phrases like \"think step by step\" or \"explain your reasoning\" in the prompt to activate the model's deep thinking mode.\n",
        "- E. When a description is too abstract or cannot be accurately described, clarify it by adding examples.\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: A, B, C, E**</br>\n",
        "üìù **Explanation**:</br>\n",
        "- Reasoning LLMs (like `qwen3-235b-a22b-thinking-2507`) are designed to engage in deep, chain-of-thought style reasoning by default, without needing an explicit instruction to do so.\n",
        "\n",
        "</div>\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b046be71",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
