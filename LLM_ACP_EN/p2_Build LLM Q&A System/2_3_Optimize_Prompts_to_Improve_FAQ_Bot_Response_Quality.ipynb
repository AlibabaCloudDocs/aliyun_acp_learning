{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92010cd5",
      "metadata": {},
      "source": [
        "# 2.3 Optimizing prompts to improve the quality of responses from Q&A bots\n",
        "\n",
        "## üöÑ Preface  \n",
        "\n",
        "The Q&A bot from the previous section can already answer questions about company rules and regulations, but this is only the first step in building an excellent user experience. Users often expect more personalized and precise interactions. This section introduces prompt engineering techniques, teaching you how to improve or control the output of the Q&A bot by optimizing prompts‚Äîfor example, adjusting tone, standardizing formats, or even enabling it to handle tasks like text summarization, inference, and transformation.\n",
        "\n",
        "## üçÅ Goals\n",
        "\n",
        "After completing this section , you will know about:\n",
        "\n",
        "* How prompt frameworks and templates work\n",
        "* Prompt techniques and their best practices\n",
        "* How to apply LLMs in engineering to handle various tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0e80fa",
      "metadata": {},
      "source": [
        "## 1. Previous content recap\n",
        "\n",
        "In the previous section, through the RAG method, the LLM has already obtained the company's private knowledge. For ease of invocation, this was encapsulated into several functions and saved in chatbot/rag.py. You can now quickly invoke it using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa6756d-47a4-4b11-8074-d992d6ce327b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:03.577636Z",
          "start_time": "2025-01-15T06:49:03.573248Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from config.load_key import load_key, display_config_summary\n",
        "load_key()\n",
        "display_config_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91138451-fbc1-49ff-a268-2ece341b10bd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:11.770743Z",
          "start_time": "2025-01-15T06:49:11.611835Z"
        }
      },
      "outputs": [],
      "source": [
        "from chatbot import rag, llm\n",
        "# Load index\n",
        "# The previous section has already built the index, so the index can be loaded directly here. If you need to rebuild the index, you can add a line of code: rag.indexing()\n",
        "index = rag.load_index(persist_path=\"knowledge_base/test\")\n",
        "query_engine = rag.create_query_engine(index=index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2d8d11-5a07-441c-9cd6-6ba10bf80603",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:16.961802Z",
          "start_time": "2025-01-15T06:49:16.959373Z"
        }
      },
      "outputs": [],
      "source": [
        "# Define the question-answering function\n",
        "def ask_llm(question, query_engine):\n",
        "  streaming_response = query_engine.query(question)\n",
        "  streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1715f36",
      "metadata": {},
      "source": [
        "## 2. Optimizing Prompts to Improve Response Quality \n",
        "\n",
        "In the previous section, you obtained the management tools used internally by the company through the RAG method. However, one of your colleagues wants that the Q&A bot to provide not only the tool name but also a link to the tool. You could ask all your colleagues to include a note like \"provide the download address\" in their questions, but this approach is inefficient. You decide to enhance the program by automatically supplementing user questions with additional requirements for the response content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12a310a-660c-4cd3-9d23-8819bdefb8bb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:34.992776Z",
          "start_time": "2025-01-15T06:49:29.417135Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"What tools should our company use for project management?\"\n",
        "instruction = \"If it's a tool consultation question, be sure to provide the download link.\"\n",
        "new_question = question + instruction\n",
        "ask_llm(new_question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806fadd6-673a-4542-b3e2-fe36310b30ad",
      "metadata": {},
      "source": [
        "After adjusting how prompts are provided, the Q&A bot's responses better align with your colleagues' requirements. Therefore, the choice of prompt significantly determines the quality of the LLM's responses. Next, you can refer to some prompt frameworks to construct your prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c006a5ea-5693-4805-9d2c-ccc0d2214939",
      "metadata": {},
      "source": [
        "## 3. Prompt framework\n",
        "\n",
        "### 3.1 Basic elements\n",
        "\n",
        "When communicating with an LLM, you can think of it as an entity that has been trained to understand human communication patterns. The way you interact should mirror how humans exchange information. Your requirements must be clear and unambiguous. The clearer and more precise your question (or prompt), the better the large language model will understand the core of the query, and the more likely its response will meet your expectations. Generally speaking, the following elements should be clarified in a prompt, and form a prompt framework that can help you construct a complete and effective prompt:\n",
        "\n",
        "|Element|Meaning|\n",
        "|----|----|\n",
        "|Objective|Clearly specify what task the LLM is required to complete, allowing it to focus on specific goals.|\n",
        "|Context|Background information about the task, such as operational processes or task scenarios, clarifying the scope of the discussion for the LLM.|\n",
        "|Role|The role the LLM should assume (e.g., \"You are a helpful assistant,\" \"You are a senior developer\"). This defines the model's persona, voice, and writing style.|\n",
        "|Audience|Specify the target audience for the response (e.g., \"Explain this to a 5-year-old,\"). This helps tailor the complexity and tone of the output.|\n",
        "|Sample|Provide specific input-output examples for the LLM to follow. This is also known as \"few-shot prompting\" and helps the model understand the desired pattern and format.|\n",
        "|Output Format|Specify the format of the output, output type, and range of enumerated values. You can also state content and information that should not be included in the output, which can be further clarified with examples.|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc35341a-3d7c-4881-81e3-935d42a1dcb2",
      "metadata": {},
      "source": [
        "Of course, in addition to the prompt framework discussed above, many problem analysis paradigms can be used to help you articulate clear and specific requirements. For example, SWOT analysis and 5W2H analysis offer structured approaches to thinking through your needs and expressing them more effectively. Additionally, you can also write a meta-prompt‚Äîa prompt designed to optimize or enhance another prompt‚Äîto guide the LLM in refining your initial input.\n",
        "\n",
        "By using such a meta-prompt, you can ask the model to:\n",
        "* Clarify ambiguous statements\n",
        "* Expand vague instructions into detailed guidelines\n",
        "* Add context, tone, format, and examples where necessary\n",
        "* Ensure the final prompt aligns with best practices for interacting with AI models\n",
        "\n",
        "This self-reflection approach improves the quality of your prompts, while deepening your understanding of how to communicate effectively with AI systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c618996",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_expand = \"\"\"\n",
        "    You are a Prompt Optimization and Expansion Assistant. Your primary role is to receive an arbitrary user input prompt ‚Äî which may be vague, incomplete, or lacking in detail ‚Äî and **refine and expand** it into a more structured, precise, and actionable instruction that can be clearly understood and executed by large language models.\n",
        "\n",
        "    Given the input prompt, you should generate an optimized version according to the following instructions:\n",
        "\n",
        "    Your task is to enhance the clarity of the original prompt while preserving its core intent. The resulting optimized prompt should:\n",
        "    - Be semantically richer and more descriptive.\n",
        "    - Include relevant context, constraints, or formatting instructions as needed.\n",
        "    - Specify desired tone, style, or audience (if applicable).\n",
        "    - Suggest example inputs and outputs where appropriate to guide model behavior.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Output Format:\n",
        "\n",
        "    Please respond with the following structure:\n",
        "\n",
        "    #### 1. Optimized Prompt:\n",
        "    [Here goes the enhanced and expanded version of the original prompt.]\n",
        "\n",
        "    #### 2. Example Input (if applicable):\n",
        "    [Provide one or more example(s) of what a user might input based on the prompt.]\n",
        "\n",
        "    #### 3. Example Output (based on the example input):\n",
        "    [Show what kind of output would be expected from the model when using this prompt with the given example input.]\n",
        "\n",
        "    #### 4. Optimization Notes:\n",
        "    [Briefly explain how you improved the original prompt ‚Äî e.g., added clarity, specified format, included context, etc.]\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Example:\n",
        "\n",
        "    #### Original Prompt:\n",
        "    \"Write a story about animals.\"\n",
        "\n",
        "    #### Optimized Prompt:\n",
        "    \"Compose a short narrative (approximately 300‚Äì500 words) about a group of forest animals who must work together to solve a problem. The story should include at least three distinct animal characters, a central conflict or challenge, and a resolution that highlights themes such as cooperation, courage, or environmental awareness. Use descriptive language to set the scene and convey character emotions. Aim for a tone suitable for children aged 6‚Äì10.\"\n",
        "\n",
        "    #### Example Input:\n",
        "    None required ‚Äî the model generates content autonomously.\n",
        "\n",
        "    #### Example Output:\n",
        "    Once upon a time in Greenleaf Forest, a wise old owl named Oliver noticed that the river had stopped flowing. Concerned, he called a meeting with his neighbors: Mina the mouse, Benny the bear, and Tilly the turtle. Together, they discovered a fallen tree blocking the stream. With teamwork and determination, they cleared the path and restored the water supply. From then on, the animals made sure to keep their home safe and healthy...\n",
        "\n",
        "    #### Optimization Notes:\n",
        "    - Added specific narrative elements (characters, setting, conflict, theme).  \n",
        "    - Defined word count and target audience.  \n",
        "    - Clarified tone and style expectations.  \n",
        "    - Introduced emotional and moral dimensions to enrich storytelling.\n",
        "\n",
        "    ---\n",
        "\n",
        "    Please apply this process to any user-submitted prompt and return your response using the above template.\n",
        "\n",
        "    The original prompt is:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def input_enhanced_llm_output(questions):\n",
        "    expanded_input = prompt_expand + questions\n",
        "    return ask_llm(expanded_input, query_engine)\n",
        "\n",
        "question = \"Please help me write a promotional copy for our company's new book. The general content of the book is xxx(will be given later)\"\n",
        "input_enhanced_llm_output(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1fa6d9",
      "metadata": {},
      "source": [
        "### 3.2 Prompt template\n",
        "\n",
        "In application development, it is impractical to expect end-users to write their own effective prompts from scratch. Instead, developers use the principles of prompt frameworks to create structured prompt templates. This allows developers to simply insert dynamic user inputs into predefined placeholders, ensuring that every final prompt is consistent. This consistency leads to more reliable and standardized LLM responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84e29a3-a017-47c4-907e-2ff897333d1d",
      "metadata": {},
      "source": [
        "In the RAG chatbot created using LlamaIndex, there is a default prompt template that looks like this:\n",
        "\n",
        "- The default template can be viewed using code. You can refer to [the code on the LlamaIndex official website.](https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/) The original LlamaIndex prompt template is:  \n",
        "\n",
        "\n",
        "\n",
        "```text\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "```\n",
        "\n",
        "Both¬†`context_str`¬†and¬†`query_str`¬† are placeholders that get replaced during the retrieval and querying process. The¬†`context_str`¬†is filled with relevant information retrieved from the vector database, while¬†`query_str`¬†is replaced with the user's actual question.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ff8788-4d67-47b8-a4a2-3f3a85e6b0c5",
      "metadata": {},
      "source": [
        "Since the default template is general-purpose , it may not be suitable for all scenarios. For example, you might want to adjust the chatbot's tone, add specific constraints, or define a precise output format. This customization ensures that the model‚Äôs output aligns more closely with your requirements.\n",
        " \n",
        "You can use the following sample code, where¬†`prompt_template_string`¬†represents the new prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05aea996-fdc8-4a35-922f-288313881fa3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:22.442975Z",
          "start_time": "2025-01-15T07:52:22.435217Z"
        }
      },
      "outputs": [],
      "source": [
        "# Build prompt template\n",
        "prompt_template_string = (\n",
        "    \"You are the company's customer service assistant, you need to answer users' questions concisely.\"\n",
        "    \"\\n„ÄêNotes„ÄëÔºö\\n\"\n",
        "    \"1. Answer user questions based on contextual information.\\n\"\n",
        "    \"2. Only answer the user's question, do not output other information\\n\"\n",
        "    \"The following is reference information.\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Question: {query_str}\\n.\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "# Update prompt template\n",
        "rag.update_prompt_template(query_engine, prompt_template_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa99f324",
      "metadata": {},
      "source": [
        "## 4. Techniques for writing effective prompts\n",
        "\n",
        "In Section 3.1, some key elements of prompt design were introduced. This section will explain specific prompt techniques in detail, focusing on practical scenarios and how to apply the elements of a prompt effectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b3eb55-646d-41dd-b801-152d76205bbe",
      "metadata": {},
      "source": [
        "### 4.1 Clearly express requirements and use delimiters\n",
        "\n",
        "Clearly stating your requirements ensures that the content generated by the LLM is highly relevant to the task at hand. These requirements typically include the¬†**task objective**,¬†**context**, and¬†**background information**.\n",
        "\n",
        "Using¬†**delimiters**¬†helps LLMs focus on specific goals, avoid ambiguous interpretations, and minimize unnecessary processing. Delimiters also help structure your prompt in a way that improves readability and model understanding.\n",
        "\n",
        "Here are some commonly used delimiters in English prompt engineering:\n",
        "\n",
        "| Delimiter Type      | Example                         | Use Case / Description |\n",
        "|---------------------|----------------------------------|--------------------------|\n",
        "| Triple Quotes         | `\"\"\"...\"\"\"`                      | For multi-line strings or input blocks |\n",
        "| Triple Hashes         | `### Section ###`                | To mark headings or instruction sections |\n",
        "| Double Angle Brackets | `<<example>>...<</example>>`     | To define custom tags for variables or examples |\n",
        "| XML-like Tags         | `<input>...</input>`             | For structured formatting and content separation |\n",
        "| Dashes                | `--- Section ---`                | To separate paragraphs or logical blocks |\n",
        "| Equal Signs           | `=== Output Format ===`          | Similar to headings; often used for formatting clarity |\n",
        "\n",
        "> ‚ö†Ô∏è Note: Avoid using symbols that appear frequently in your content as delimiters to prevent confusion. For example, if square brackets¬†`[]`¬†are commonly used  in your input text, they may not be suitable as delimiters.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example Usage in an English Prompt\n",
        "\n",
        "```text\n",
        "### Instruction ###\n",
        "You are a helpful assistant. Please summarize the following article.\n",
        "\n",
        "=== Article Content ===\n",
        "\"\"\" \n",
        "Artificial intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence.\n",
        "\"\"\"\n",
        "\n",
        "### Output Format ###\n",
        "Please provide a summary in no more than 50 words.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8918731d-49b6-4132-9ba5-2dce3eee360c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:32.192934Z",
          "start_time": "2025-01-15T07:52:24.619420Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "### Instruction ###\n",
        "Expand and polish the text enclosed in XML tags below to make the copy vivid, creative, and appealing to new employees. \n",
        "The tone should be professional yet welcoming, suitable for onboarding materials.\n",
        "\n",
        "=== Original Text ===\n",
        "<text>\n",
        "New Employee Training Camp Activity\n",
        "</text>\n",
        "\n",
        "=== Output Requirements ===\n",
        "- Enhance the content with engaging language.\n",
        "- Add context about the purpose and benefits of the training camp.\n",
        "- Include key elements such as team-building, mentorship, and orientation.\n",
        "- Keep the final output concise and motivational.\n",
        "\"\"\"\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ae4c28-a2e6-4ad3-995b-7af2d78b19d1",
      "metadata": {},
      "source": [
        "The execution results show that the prompt effectively communicates the task requirements: to expand and enhance the provided text.\n",
        "\n",
        "The core theme of the task is \"**New Employee Training Camp Activities,**\" and the intended audience is \"**new company employees.**\" In the prompt, the delimiter¬†<text>...</text>¬†is used to clearly encapsulate and isolate the original content for processing.\n",
        "\n",
        "By structuring the prompt with clear instructions, appropriate context, and standardized delimiters, the model is better guided to generate output that not only preserves the original meaning but also becomes more vivid, engaging, and tailored to the target audience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e4407d-3cff-4f00-a39b-b8ddfd7cce1d",
      "metadata": {},
      "source": [
        "### 4.2 Define Role and Audience\n",
        "\n",
        "The¬†**role**¬†is the identity the LLM should assume in a specific scenario, such as an expert, advisor, or assistant. The¬†**audience**¬†is the target user group for which the model's output is intended, such as general consumers or students. Defining these elements helps developers tailor the tone, depth, and structure of the content accordingly.\n",
        "The example below demonstrates how different system roles affect the style and content of the generated output. \n",
        "\n",
        "> Pass the prompt template into¬†`prompt_template_string`, and the user question into¬†`question`, then call the¬†`ask_llm`¬†function to execute it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e5f128-8b02-40ca-b789-d577b6e38a3a",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th width = '80px'>Role</th>\n",
        "      <th width = '500px'>LLM Algorithm Engineer</th>\n",
        "      <th width = '500px'>Elementary School Teacher</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody valign = 'top'>\n",
        "    <tr>\n",
        "      <td>Prompt Template</td>\n",
        "      <td>You are a senior large language model algorithm engineer. Please answer the user's question based on the provided information<br>---------------------<br>{context_str}<br>---------------------<br>Question: {query_str}.<br>Answer: \n",
        "</td>\n",
        "      <td>You are an elementary school teacher. Please answer the user's question based on the provided information<br>---------------------<br>{context_str}<br>---------------------<br>Question: {query_str}.<br>Answer: </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>User Question</td>\n",
        "      <td>What is Qwen-VL</td>\n",
        "      <td>What is Qwen-VL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Output Answer</td>\n",
        "      <td>Qwen-VL is a multimodal pre-trained model introduced by Alibaba Cloud. It combines text and image processing capabilities, enabling it to understand and generate text descriptions related to images. Simply put, Qwen-VL can be regarded as an artificial intelligence assistant that can \"describe images in words.\" It not only understands the content of images but also generates corresponding textual descriptions or answers questions related to the images. This technology has broad application prospects in various fields, such as automatic image tagging, visual question-answering systems, and content-based image retrieval. For new media operations, tools like Qwen-VL can help improve content creation efficiency, enhance user experience, and provide users with richer and more diverse ways of presenting information.</td>\n",
        "      <td>Kids, today we‚Äôre going to meet a new friend named Qwen-VL!\\nQwen-VL is like a very smart robot that has \"eyes\" üëÄ and a \"brain\" üß†. It can \"see\" pictures and videos and understand their meaning, just like us!\\nFor example, if you show it a picture of a kitten playing with a ball of yarn, it can not only recognize that it‚Äôs a kitten üê± but also tell you that it‚Äôs playing with a ball of yarn üß∂. Even better, you can ask it, \"What color is the kitten?\", and it will tell you the color of the kitten.\\nIt can also understand the content in videos. For instance, if you show it a video of kids playing soccer ‚öΩÔ∏è, it can tell you who is kicking the ball, where the ball goes, and even guess what might happen next!\\nQwen-VL is like a child who is still learning. It keeps getting smarter and improving, and in the future, it will become even more intelligent, capable of doing more things and helping us better understand the world!</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f06bea-a34b-4e7a-b2b5-918af4c1ebdb",
      "metadata": {},
      "source": [
        "### 4.3 Specifying output formats\n",
        "\n",
        "Sometimes developers need LLMs to produce structured data (such as JSON or XML) that downstream systems can programmatically parse and use in the application workflow. While LLMs excel at generating free-form natural language, they can also reliably produce structured outputs such as JSON, XML, or CSV formats‚Äîas long as the desired format and requirements are clearly specified in the prompt.\n",
        "In the following example, based on the prompt template from Section 3.2, we will act as a Q&A bot to assist in reviewing the quality of company-developed documentation, and return the results in¬†**JSON format**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6692496-b6e4-4d75-aaa2-ef38b0c24f0c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:49.007218Z",
          "start_time": "2025-01-15T06:49:46.080617Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "[Task Requirements]\n",
        "You will be given a sentence or paragraph. Your task is to review it for any spelling or grammatical errors. \n",
        "If there are errors, you should identify them, explain the issue, and provide a corrected version. \n",
        "\n",
        "---\n",
        "[Output Requirements]\n",
        "Please output only in JSON format. Do not include markdown or code blocks.\n",
        "The JSON must contain:\n",
        "- \"label\": 0 if an error is found, 1 if no error.\n",
        "- \"reason\": A brief explanation of the error (if any).\n",
        "- \"correct\": The corrected version of the text (or the original if no error).\n",
        "\n",
        "---\n",
        "[User Input]\n",
        "The following is the user input. Please review it:\n",
        "\"\"\"\n",
        "\n",
        "question_doc = \"Delimiters are special symbols that help large langauge models (LLMs) identify which parts of the prompt should be considered a complete unit of meaning.\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe59cd3-a241-4b28-902e-7ca35966e6a7",
      "metadata": {},
      "source": [
        "As shown in the example results, the prompt¬†`question_task` specified that the output format should be JSON format, and clearly defined the content requirements. The model  generated responses in the required format. This consistent and structured output makes it easier to integrate the model into existing systems.\n",
        "\n",
        "On news websites, blog platforms, or internal knowledge-sharing platforms within enterprises, user-edited or published articles may contain typos, grammatical errors, or even sensitive information. However, traditional manual review methods are prone to oversight. In this case, LLMs can be integrated to automatically review content. If an article is flagged for serious grammatical errors or high-risk sensitive words, its priority for revision will be set to \"high.\" For articles with minor issues, the priority for revision can be set to \"low.\" This approach reduces labor costs and enhances the efficiency and accuracy of the system.\n",
        "\n",
        "Of course, there are many other applications similar to this scenario. Developers can analyze system bottlenecks or focus on data-intensive tasks to explore more use cases for LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f32b7a-9b47-40f1-82cb-e889a3b48048",
      "metadata": {},
      "source": [
        "### 4.4 Providing few-shot examples\n",
        "\n",
        "In the example from Section 4.3, the prompt specified the output format, and the LLM  generated formatted content. However, if we want the model's output  to not only be correctly formatted but also maintain consistency in style and structure, providing a few examples is a highly effective technique. This technique, known as `few-shot prompting`, is like giving the model a \"reference book\" to follow.\n",
        "\n",
        "In the following code example, let‚Äôs first observe the LLM's output without any examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcca16d368baf38",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:20:42.883097Z",
          "start_time": "2025-01-15T07:20:18.423060Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "### üìù Task Requirements\n",
        "You are to generate content based on the user's topic, following the **style and structure** of the example provided below. Do not add any extra text or explanations ‚Äî return only the final Markdown-formatted output.\n",
        "\n",
        "---\n",
        "### üß© Output Format Requirements\n",
        "Please ensure your response strictly follows this format:\n",
        "- Use Markdown syntax (headings, lists, bold, links, image placeholders, etc.)\n",
        "- Keep the tone informative and easy to follow\n",
        "- Structure content in a step-by-step or tutorial format\n",
        "- Avoid markdown code blocks around the final output itself\n",
        "\n",
        "---\n",
        "### üë§ User Input Topic\n",
        "The following is the user's requested topic:\n",
        "\"\"\" \n",
        "\n",
        "question_doc = \"Handmade Keychain Making Tutorial\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f61d316e005c4f",
      "metadata": {},
      "source": [
        "The model generated a tutorial for making handmade keychains, but the content was not concise enough. If you want the model to output content in a specific style and structure‚Äîsuch as focusing only on the theme, list of materials, and step-by-step instructions‚Äîyou can provide a few examples to guide it. This helps the model \"imitate\" the desired format and produce more consistent and structured results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e0f67f703e7ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:39.297041Z",
          "start_time": "2025-01-15T06:50:24.071562Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "### üìù Task Requirements\n",
        "You are to generate content based on the user's topic, following the **style and structure** of the example provided below. Do not add any extra text or explanations ‚Äî return only the final Markdown-formatted output.\n",
        "\n",
        "---\n",
        "### üß© Output Format Requirements\n",
        "Please ensure your response strictly follows this format:\n",
        "- Use Markdown syntax (headings, lists, bold, links, image placeholders, etc.)\n",
        "- Keep the tone informative and easy to follow\n",
        "- Structure content in a step-by-step or tutorial format\n",
        "- Avoid markdown code blocks around the final output itself\n",
        "\n",
        "---\n",
        "### üìö Example Content\n",
        "<example>\n",
        "### Example 1: Making a Simple Bookmark\n",
        "# Simple Bookmark Making Tutorial\n",
        "\n",
        "## Steps\n",
        "1. Choose a piece of colored cardstock.\n",
        "2. Use a pencil to draw a rectangle on the cardstock, with dimensions approximately 2 inches x 6 inches.\n",
        "3. Cut out the rectangle along the pencil lines.\n",
        "4. Personalize the bookmark by decorating it with stickers.\n",
        "5. Done! You now have a unique bookmark.\n",
        "</example>\n",
        "\n",
        "---\n",
        "### üë§ User Input Topic\n",
        "The following is the topic requested by the user:\n",
        "\"\"\"\n",
        "\n",
        "question_doc = \"Making Handmade Greeting Cards\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95447ee9772539d",
      "metadata": {},
      "source": [
        "From the above example results, it's clear that the LLM has fully adhered to the provided example and generated content with a consistent structure and style. When specifying the output format in a prompt, it is recommended to provide a few examples for the LLM to reference. This practice helps ensure that the model's output is more reliable, consistent, and aligned with your expectations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53569b47-1619-4527-a9c8-b53f5e5bdf8d",
      "metadata": {},
      "source": [
        "### 4.5 Giving the model \"thinking\" time\n",
        "\n",
        "For some complex tasks, even well-structured prompts may be insufficient for the LLM to complete the task effectively. In such cases, you can guide the model by allowing it to \"think\" step by step, encouraging it to provide intermediate reasoning before arriving at a final answer. This approach, known as the chain-of-thought (CoT) method, improves performance on tasks that require logical or mathematical reasoning.\n",
        "\n",
        "For example, consider a scenario where you ask the LLM to solve a math problem. The correct answer is $10,500. Initially, you might use a simple prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3569f469-c601-406a-a26d-fb0a74157c68",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:42.416991Z",
          "start_time": "2025-01-15T06:50:41.378053Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "[Background Information]\n",
        "An educational training institution (hereinafter referred to as the \"company\") incurred the following main expenses in the 2023 fiscal year:\n",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times during the year, each trip lasting one week. The specific expenses are as follows:\n",
        "   - Transportation and accommodation fees: average 1600 usd/trip\n",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 usd, which is expected to last for 4 years.\n",
        "\n",
        "[Problem Description]\n",
        "Based on the above background information, complete the following tasks:\n",
        "Calculate the total travel expenses for the year due to teacher business trips, including the amortized cost of teaching materials.\n",
        "\n",
        "[Output Requirements]\n",
        "Provide only the total travel expenses directly, without any other information. \"\"\"\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf9078a-f716-4537-b003-a90d70f988b5",
      "metadata": {},
      "source": [
        "As shown, the initial response is incorrect. To improve accuracy, you can use the CoT method by prompting the LLM to explain its reasoning step by step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cb1715-305c-43ee-bd5e-274d2556ad56",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:55.556383Z",
          "start_time": "2025-01-15T06:50:43.720893Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"An educational training institution (hereinafter referred to as the \"company\") incurred the following major expenses in the 2023 fiscal year:\n",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times throughout the year, with each trip lasting one week. The specific expenses are as follows:\n",
        "   - Transportation and accommodation fees: an average of 1600 usd per trip\n",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 usd, which is expected to last for 4 years.\n",
        "   \n",
        "### Problem Description\n",
        "Based on the above background information, complete the following tasks:\n",
        "Calculate the total travel expenses for the year caused by teacher business trips, including the amortized cost of teaching materials.\n",
        "\n",
        "### Output Requirements\n",
        "Please derive step by step to calculate the total travel expenses.\"\"\"\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ec9cdf-d6e1-469f-9e4c-f91fdf7f1c3e",
      "metadata": {},
      "source": [
        "After optimizing the prompt, the LLM was able to generate an accurate and detailed response. This demonstrates that prompting the model to generate a chain of thought can significantly improve the quality of reasoning-based outputs.\n",
        "\n",
        "While CoT is a powerful technique, there are other methods to enhance the model‚Äôs thinking process, such as tree of thoughts (ToT) and graph of thoughts (GOT). However, current LLMs still rely heavily on CoT for structured reasoning. As AI technology advances, models are gradually moving toward more sophisticated multi-agent systems (MAS), which can handle complex tasks more efficiently. You can explore this further in <2_6_Extend_FAQ_Bot_Capabilities_with_Plugins>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7cda3a",
      "metadata": {},
      "source": [
        "### 4.6 Meta Prompting: Let the LLM Be Your Prompt Coach\n",
        "\n",
        "Writing a perfect prompt on the first try is often very difficult. A more common workflow is:\n",
        "1. Write the first version of the prompt.\n",
        "2. Run it and analyze which parts of the output don't meet expectations.\n",
        "3. Summarize the problems, think about how to improve, then modify the prompt.\n",
        "4. Repeat this iteration process until satisfied.\n",
        "\n",
        "Think about it‚Äîisn't this also your typical path for optimizing prompts? While this process is effective, it heavily relies on experience and is quite time-consuming.\n",
        "\n",
        "At this point, you might think: since LLMs are so powerful, **can we let the model itself handle this analysis, summarization, and improvement iteration process?** Having it play the role of a \"prompt review expert\" to help us analyze and optimize prompts would undoubtedly be more efficient.\n",
        "\n",
        "The answer is yes. This method of **having you and the model \"discuss\" how to optimize the prompt itself** is called **Meta Prompting**.\n",
        "\n",
        "To further understand this powerful technique, let's practice it hands-on.\n",
        "\n",
        "#### Step 1: A Less-Than-Ideal Initial Prompt\n",
        "\n",
        "Suppose your task is to optimize the Q&A bot's responses to make them more friendly and clearly structured when answering new employees' questions about \"company benefits.\" You might start with a simple prompt:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bb2906",
      "metadata": {},
      "outputs": [],
      "source": [
        "# In a real RAG application, this text would be retrieved from your vector database.\n",
        "# Here we use a string to simulate it for experimentation.\n",
        "retrieved_text = \"\"\"\n",
        "Regarding company benefits, we provide comprehensive health insurance covering employees and their immediate family members.\n",
        "Annual health checkups are standard. Additionally, there are 15 days of paid annual leave and 5 days of paid sick leave per year.\n",
        "We also provide a monthly transportation subsidy of 500 yuan and a meal subsidy of 300 yuan.\n",
        "To encourage employee growth, the company has an annual education and training fund of up to 8000 yuan, which employees can apply to use for courses or professional books.\n",
        "For fitness, the company has partnerships with multiple gyms where employees can enjoy discounted rates.\n",
        "\"\"\"\n",
        "\n",
        "# This is a very basic prompt that simply concatenates the task and information.\n",
        "initial_prompt = f\"\"\"\n",
        "Based on the following information, answer new employees' questions about company benefits.\n",
        "\n",
        "„ÄêReference Information„Äë\n",
        "{retrieved_text}\n",
        "\"\"\"\n",
        "\n",
        "# Let's see what effect this \"plain\" prompt produces.\n",
        "response = llm.invoke(initial_prompt)\n",
        "print(\"--- Initial Response ---\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54e7737a",
      "metadata": {},
      "source": [
        "This response contains all the information, but you might feel that for a newly hired employee, it seems somewhat bland and disorganized. It simply restates the text without emphasis or a warm, welcoming tone.\n",
        "\n",
        "Clearly, this result doesn't meet your expectations. Now, instead of racking your brain on how to modify it, you can try a more efficient method: **let the LLM help you optimize**.\n",
        "\n",
        "#### Step 2: Build a Meta Prompt to Get Optimization Suggestions from the LLM\n",
        "\n",
        "Now, you're dissatisfied with this bland response. You can construct a \"Meta Prompt\" that clearly describes your goals (friendly, structured, highlighting key points) to the model, and feed both your less-than-ideal initial prompt and its output to the LLM, requesting it to act as a \"prompt engineering expert\" to help you improve.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33550722",
      "metadata": {},
      "outputs": [],
      "source": [
        "# You need to clearly express your dissatisfaction and expectations‚Äîthis is key to helping the AI coach understand your intent.\n",
        "meta_prompt = f\"\"\"\n",
        "I'm optimizing a prompt for our company's new employee Q&A bot, with the goal of answering questions about \"company benefits.\"\n",
        "\n",
        "This is my first attempt:\n",
        "---\n",
        "{initial_prompt}\n",
        "---\n",
        "\n",
        "This is the output it generated:\n",
        "---\n",
        "{response}\n",
        "---\n",
        "\n",
        "This output isn't good enough. I want the bot's responses to be more engaging, clearly structured, and help new employees quickly grasp the key points. Specific requirements:\n",
        "1. **Tone**: Friendly, warm, with a welcoming feeling for new colleagues.\n",
        "2. **Structure**: Use clear bullet points (e.g., lists starting with emojis) to organize content.\n",
        "3. **Content**: Categorize benefits into groups like \"Health & Leave,\" \"Subsidies & Incentives,\" etc.\n",
        "\n",
        "Please act as a prompt engineering expert and rewrite this prompt to achieve the above goals.\n",
        "\"\"\"\n",
        "\n",
        "# Now, let the AI coach get to work and generate an optimized version of the prompt for you.\n",
        "optimization_suggestion = llm.invoke(meta_prompt)\n",
        "print(\"--- Optimization Suggestions from AI Coach --\")\n",
        "print(optimization_suggestion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6449ae",
      "metadata": {},
      "source": [
        "Observe the suggestions from the AI coach‚Äîyou'll find that the optimized prompt it provides likely applies multiple techniques you learned in previous sections, such as:\n",
        "\n",
        "* **Defining a role** (e.g., \"You are a warm, friendly onboarding buddy\")\n",
        "* **Clear task description** (e.g., \"Based on the reference information provided, generate an introduction about company benefits\")\n",
        "* **Specifying output format and style** (e.g., \"Use a warm welcome greeting,\" \"Use lists starting with emojis\")\n",
        "\n",
        "This proves that having the model play an expert role to optimize prompts is entirely feasible.\n",
        "\n",
        "#### Step 3: Use the Optimized Prompt\n",
        "\n",
        "Now you can directly use the prompt that this \"AI coach\" customized for your task and see how it performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed48c9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a hypothetical optimized prompt suggested by the AI coach\n",
        "# In practice, you can directly use the output from `optimization_suggestion`\n",
        "# Here we manually construct one that follows the suggestions for demonstration\n",
        "\n",
        "optimized_prompt = f\"\"\"\n",
        "Based on the following information, answer new employees' questions about company benefits. Ensure the response is friendly, warm, clearly structured, and well-categorized.\n",
        "\n",
        "„ÄêReference Information„Äë\n",
        "...\n",
        "\n",
        "„ÄêResponse Template„Äë\n",
        "üéâ Welcome to [Company Name]! We're thrilled to have you as part of our family. Here's some important information about company benefits that we hope will be helpful:\n",
        "\n",
        "### üè• Health & Leave\n",
        "- **Comprehensive Health Insurance**: We provide full coverage for all employees and their immediate family members, ensuring you and your family's health is protected.\n",
        "- **Annual Health Checkup**: Every employee can enjoy a free comprehensive checkup each year to stay informed about your health status.\n",
        "- **Paid Leave**: 15 days of paid annual leave plus 5 days of paid sick leave each year, so you can rest and recharge outside of work.\n",
        "\n",
        "### üí∏ Subsidies & Incentives\n",
        "- **Transportation Subsidy**: 500 yuan monthly to help ease your commuting costs.\n",
        "- **Meal Subsidy**: 300 yuan monthly so you can enjoy great meals on workdays.\n",
        "- **Education & Training Fund**: The company values employee growth and development, offering up to 8000 yuan annually that you can apply for courses or professional books to improve yourself.\n",
        "\n",
        "### üèãÔ∏è‚Äç‚ôÇÔ∏è Fitness & Recreation\n",
        "- **Gym Discounts**: We've partnered with multiple gyms where employees can enjoy discounted rates to stay in great shape.\n",
        "\n",
        "If you have any questions or need further assistance, please feel free to contact the HR department. We look forward to every day you spend at [Company Name] being full of energy and happiness!\n",
        "\n",
        "„ÄêReference Information„Äë\n",
        "{retrieved_text}\n",
        "\"\"\"\n",
        "\n",
        "# Call the model again with the optimized prompt\n",
        "final_response = llm.invoke(optimized_prompt)\n",
        "print(\"--- Response Using Optimized Prompt ---\")\n",
        "print(final_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92c258dc",
      "metadata": {},
      "source": [
        "\n",
        "Through this iteration process, you'll get a response with a warm tone and clear structure‚Äîone that will undoubtedly leave a better first impression on new employees.\n",
        "\n",
        "This example demonstrates the tremendous value of Meta Prompting in improving user experience. It's not just about extracting information, but about **how to better present information**. When you encounter a tricky prompt problem, don't forget that your model itself is the best coach. By clearly describing your goals and challenges, you can guide it to build more powerful, precise, and \"human-touch\" prompts for you.\n",
        "\n",
        "In the initial approach above, we described our expected **qualitative goals** (like \"friendly,\" \"clearer structure\") to the \"AI coach,\" and it directly generated an optimized prompt for us. This method is convenient, but it has a key limitation: the AI coach's understanding of these qualitative goals may not be precise enough, and your description of requirements may not be specific enough, leading to uncertainty in optimization results.\n",
        "\n",
        "To address this problem and make the optimization process more controllable and precise, we need to upgrade from \"qualitative guidance\" to \"quantitative alignment.\" Instead of giving a vague goal, it's better to provide a perfect **\"reference answer\"** as a precise target. The following advanced approach will show you how to use this \"reference answer\" to let the LLM gradually and precisely approach the optimal result through automated gap analysis and iteration. This is a more engineering-oriented improvement approach.\n",
        "\n",
        "#### Multi-Round Iteration: Introducing Reference Answers for Gap Analysis\n",
        "\n",
        "In the previous example, you played the leading role, receiving the \"AI coach's\" suggestions and manually applying them. But this process can be further automated and made more precise. Rather than having the evaluator give a vague \"good\" or \"bad\" judgment, a more advanced method is to introduce a **\"Reference Answer.\"**\n",
        "\n",
        "This \"reference answer\" is your ideal perfect answer, which can be written by human experts or generated by the most powerful model using a very detailed prompt. The goal of iterative optimization becomes: **continuously modify the prompt so that the gap between its generated response and this \"reference answer\" becomes smaller and smaller**.\n",
        "\n",
        "This process is like a self-correcting flow with precise guidance:\n",
        "\n",
        "1. **Set Reference Answer**: First, define a high-quality, ideal \"reference answer.\"\n",
        "2. **Generate**: Use the current prompt to be optimized to generate a response.\n",
        "3. **Analyze Gap**: Have an \"evaluator\" LLM (Critic) compare the \"generated response\" with the \"reference answer\" and output a detailed \"gap analysis report\" pointing out specific differences in tone, structure, content, format, etc.\n",
        "4. **Optimize**: Pass the \"gap analysis report\" along with the original prompt and generated response to an \"optimizer\" LLM. Its task is to rewrite the prompt based on this report to specifically address the issues identified, thereby reducing the gap.\n",
        "5. **Repeat**: Replace the old prompt with the optimized new one, then go back to step 2, until the \"evaluator\" considers the gap small enough or the maximum iterations are reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd70af2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Set Reference Answer\n",
        "reference_answer = \"\"\"\n",
        "üëã Welcome to our big family! I'm excited to introduce our awesome benefits:\n",
        "\n",
        "**üè• Health & Leave, We've Got You Covered:**\n",
        "- **Comprehensive Health Insurance**: Covers you and your family‚Äîwork with peace of mind.\n",
        "- **Annual Health Checkup**: Your health matters to us, always.\n",
        "- **Paid Annual Leave**: A full 15 days to explore poetry and distant places!\n",
        "- **Paid Sick Leave**: 5 days to rest and recover your energy.\n",
        "\n",
        "**üí∞ Subsidies & Incentives, Cheering You On:**\n",
        "- **Transportation Subsidy**: 500 yuan monthly for an easier commute.\n",
        "- **Meal Subsidy**: 300 yuan monthly‚Äîadd an extra drumstick to your lunch!\n",
        "- **Education & Training Fund**: Up to 8000 yuan annually‚Äîinvest in yourself, the future is bright.\n",
        "- **Gym Discounts**: Partnered with multiple gyms‚Äîdon't forget to exercise no matter how busy!\n",
        "\n",
        "Hope these benefits make you feel the company's care! Looking forward to creating more value together! üéâ\n",
        "\"\"\"\n",
        "\n",
        "# 2. Define gap analysis and optimization functions\n",
        "def analyze_gap(generated_response, reference):\n",
        "    gap_analysis_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are a text comparison expert.\n",
        "    „ÄêTask„ÄëPlease compare in detail the gaps between the„ÄêGenerated Response„Äëand the„ÄêReference Answer„Äë.\n",
        "    „ÄêReference Answer„Äë\n",
        "    {reference}\n",
        "    ---\n",
        "    „ÄêGenerated Response„Äë\n",
        "    {generated_response}\n",
        "    ---\n",
        "    „ÄêRequirements„Äë\n",
        "    Please output a detailed gap analysis report covering tone, structure, content details, format (such as emoji usage), etc. If there's almost no gap between them, simply answer \"Gap is minimal.\"\n",
        "    \"\"\"\n",
        "    return llm.invoke(gap_analysis_prompt)\n",
        "\n",
        "def optimize_prompt_with_gap_analysis(current_prompt, generated_response, gap_report):\n",
        "    optimization_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are a top-tier prompt engineer.\n",
        "    „ÄêTask„ÄëBased on the provided \"Gap Analysis Report,\" optimize the \"Current Prompt\" so it can generate output closer to the \"Reference Answer.\"\n",
        "    ---\n",
        "    „ÄêCurrent Prompt„Äë\n",
        "    {current_prompt}\n",
        "    ---\n",
        "    „ÄêGenerated Response„Äë\n",
        "    {generated_response}\n",
        "    ---\n",
        "    „ÄêGap Analysis Report„Äë\n",
        "    {gap_report}\n",
        "    ---\n",
        "    „ÄêRequirements„Äë\n",
        "    Please return only the optimized new prompt without any other explanations.\n",
        "    \"\"\"\n",
        "    return llm.invoke(optimization_prompt)\n",
        "\n",
        "# 3. Iterative optimization loop\n",
        "current_prompt = initial_prompt\n",
        "for i in range(3): # Maximum 3 iterations\n",
        "    print(f\"--- Iteration {i+1} ---\")\n",
        "    generated_response = llm.invoke(current_prompt.format(retrieved_text=retrieved_text))\n",
        "    print(f\"Generated Response (partial):\\n{generated_response[:100]}...\")\n",
        "    \n",
        "    gap_report = analyze_gap(generated_response, reference_answer)\n",
        "    print(f\"Gap Analysis Report:\\n{gap_report}\")\n",
        "    \n",
        "    if \"Gap is minimal\" in gap_report or \"Â∑ÆË∑ùÂæàÂ∞è\" in gap_report:\n",
        "        print(\"\\nEvaluation passed, optimization complete!\")\n",
        "        break\n",
        "    \n",
        "    print(\"\\nEvaluation not passed, optimizing prompt based on gap analysis report...\")\n",
        "    current_prompt = optimize_prompt_with_gap_analysis(current_prompt, generated_response, gap_report)\n",
        "else:\n",
        "    print(\"\\nMaximum iterations reached, stopping optimization.\")\n",
        "\n",
        "final_prompt_based_on_reference = current_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5c8423",
      "metadata": {},
      "source": [
        "This automated iteration process demonstrates the true power of Meta Prompting. It frees you from tedious manual adjustments, letting the model explore and discover the best ways of expression on its own, and provides foundational ideas for designing more complex **AI Self-Improvement** systems.\n",
        "\n",
        "#### Effectiveness Evaluation: Quantifying Your Optimization Results\n",
        "\n",
        "When you have multiple versions of prompts (e.g., initial version vs. single-optimization version vs. multi-round iteration final version), how do you objectively prove which is better? Beyond intuitive feelings, a more scientific approach is **quantitative evaluation**.\n",
        "\n",
        "You can leverage the LLM again, having it play the role of a \"Grader\" to score responses generated by different prompts according to a series of criteria.\n",
        "\n",
        "For example, you can define scoring criteria:\n",
        "\n",
        "* **Friendliness**: 1-5 points\n",
        "* **Clarity of Structure**: 1-5 points\n",
        "* **Information Accuracy**: 1-5 points\n",
        "\n",
        "Then, construct a \"Grader Prompt\" that passes the scoring criteria and the response to be evaluated to the LLM, having it output a structured scoring result (like JSON).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee0fad29",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Three typical response samples with obvious quality differences\n",
        "# Poor: Simple listing of information, no structure or emotion\n",
        "poor_response = \"Company benefits: Health insurance, family can use. 15 days annual leave, 5 days sick leave. Transportation subsidy 500, meal subsidy 300. Training fund 8000. Gym has discounts.\"\n",
        "\n",
        "# Medium: Has basic structure and categories, but bland tone\n",
        "medium_response = \"\"\"\n",
        "Company Benefits:\n",
        "1. üí¶Health and Leave:\n",
        "   - Health insurance (including family)\n",
        "   - Annual checkup\n",
        "   - 15 days annual leave and 5 days sick leave\n",
        "2. üí∞Subsidies and Incentives:\n",
        "   - Monthly 500 transportation subsidy and 300 meal subsidy\n",
        "   - 8000 yuan/year education and training fund\n",
        "   - Partner gym discounts\n",
        "\"\"\"\n",
        "\n",
        "# Good: Clear structure, friendly tone, strong visual appeal (directly use our reference answer)\n",
        "good_response = reference_answer \n",
        "\n",
        "# Design more detailed evaluation dimensions\n",
        "def grade_response_detailed(response_to_grade):\n",
        "    grader_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are an experienced internal communications and employee experience evaluator.\n",
        "    „ÄêTask„ÄëPlease rate the provided \"Company Benefits Introduction\" text on a 1-5 scale across the following four dimensions.\n",
        "    \n",
        "    „ÄêScoring Dimensions„Äë\n",
        "    1. **welcoming_tone**: 1 means cold and stiff tone, 5 means very warm and infectious.\n",
        "    2. **structuring**: 1 means chaotic and disorganized, 5 means clear categories and strong logic.\n",
        "    3. **visual_appeal**: 1 means dull and boring, 5 means good use of emojis, bold text, etc., very eye-catching.\n",
        "    4. **completeness**: 1 means serious information gaps, 5 means all key benefit information is complete.\n",
        "\n",
        "    „ÄêText to Evaluate„Äë\n",
        "    {response_to_grade}\n",
        "    ---\n",
        "    „ÄêOutput Requirements„Äë\n",
        "    Please strictly return your scores in JSON format without any explanations. For example:\n",
        "    {{\"welcoming_tone\": 5, \"structuring\": 4, \"visual_appeal\": 5, \"completeness\": 5}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        raw_output = llm.invoke(grader_prompt)\n",
        "        # Extract JSON portion\n",
        "        json_str = raw_output[raw_output.find('{'):raw_output.rfind('}')+1]\n",
        "        return json.loads(json_str)\n",
        "    except (json.JSONDecodeError, IndexError):\n",
        "        # Error handling, return default low scores when parsing fails\n",
        "        return {\"welcoming_tone\": 1, \"structuring\": 1, \"visual_appeal\": 1, \"completeness\": 1}\n",
        "\n",
        "# Score the three typical samples\n",
        "scores = {\n",
        "    \"Original Answer\": grade_response_detailed(poor_response),\n",
        "    \"Single Iteration Optimize\": grade_response_detailed(medium_response),\n",
        "    \"Multi-turn Iteration Optimize\": grade_response_detailed(good_response)\n",
        "}\n",
        "\n",
        "# Convert scores to DataFrame and visualize\n",
        "df = pd.DataFrame(scores)\n",
        "df = df.reset_index().rename(columns={'index': 'Dim'})\n",
        "df_long = df.melt(id_vars='Dim', var_name='Version', value_name='Score')\n",
        "\n",
        "# --- Grouped bar chart ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(\n",
        "    data=df_long,\n",
        "    x=\"Dim\",\n",
        "    y=\"Score\",\n",
        "    hue=\"Version\",\n",
        "    palette=\"viridis\"\n",
        ")\n",
        "\n",
        "# Add value labels to each bar\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    if height == 0:\n",
        "        continue\n",
        "    ax.annotate(\n",
        "        f\"{height}\",\n",
        "        (p.get_x() + p.get_width() / 2., height),\n",
        "        ha='center', va='center',\n",
        "        xytext=(0, 5),\n",
        "        textcoords='offset points',\n",
        "        fontsize=11\n",
        "    )\n",
        "\n",
        "ax.set_ylim(0, 6)\n",
        "ax.set_ylabel('Score (1-5)', fontsize=12)\n",
        "ax.set_xlabel('')\n",
        "ax.set_title('Evaluation', fontsize=20)\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512d2180",
      "metadata": {},
      "source": [
        "\n",
        "You can see that the LLM's response quality improves to varying degrees with each iteration. Through this quantitative evaluation, you can not only intuitively see the improvement from each optimization but also transform vague feelings of \"good\" or \"bad\" into clear, measurable data. This provides you with a scientific method to validate and iterate your prompt strategies, ensuring every improvement is evidence-based, ultimately delivering truly high-quality user experiences.\n",
        "\n",
        "### 4.7 Let the LLM Help You Build a Custom \"AI Judge\"\n",
        "\n",
        "The automated optimization loop in the previous section is very powerful, but it raises a key engineering question: when does this loop stop?\n",
        "\n",
        "The key issue is that relying on vague evaluations like \"is the gap big or small\" is unreliable. You need an \"**AI Judge**\" that can give a clear \"**‚úÖ Pass**\" or \"**‚ùå Fail**\" conclusion based on preset quantified standards for each optimization result, thereby providing a clear and reliable stopping condition for your automated workflow.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <a href=\"https://img.alicdn.com/imgextra/i4/O1CN01Ie6KDU1pMWUJ2Ky25_!!6000000005346-2-tps-1115-614.png\" target=\"_blank\">\n",
        "      <img src=\"https://img.alicdn.com/imgextra/i4/O1CN01Ie6KDU1pMWUJ2Ky25_!!6000000005346-2-tps-1115-614.png\" width=\"700\">\n",
        "    </a>\n",
        "  <p>Figure: Constructing an \"AI Judge\" via Meta-Prompting to replace the <code>analyze_gap()</code> method and enhance judgment accuracy.</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "#### The \"AI Judge\" Training Process\n",
        "\n",
        "The process of building an \"AI Judge\" can be seen as training a machine learning classifier. The \"AI Judge\" prompt you're building is the \"model\" to be trained, and the standard samples you prepare are the \"dataset.\" The entire process is using data to \"train\" your \"model.\"\n",
        "\n",
        "1. **Prepare Dataset**: First create a batch of high-quality labeled samples (containing both \"good answers\" and \"bad answers\").\n",
        "2. **Iterative \"Training\"**:\n",
        "   - a. Automatically split this data proportionally (e.g., 7:3) into \"training set\" and \"evaluation set.\"\n",
        "   - b. Use the current \"judge prompt\" to make \"predictions\" on both the **training set** and **evaluation set**, recording the accuracy scores for each.\n",
        "   \n",
        "   Why monitor both scores? Tracking changes in these two scores helps you determine if the \"model\" is \"overfitting\"‚Äîmeaning it just \"memorized\" the training questions but didn't truly improve its judgment ability on new questions (evaluation set).\n",
        "   \n",
        "   - c. Find the \"wrong answers\" on the **training set**.\n",
        "   - d. Take these \"wrong answers\" along with your analysis, and through Meta Prompting, have the LLM help you optimize the \"judge prompt\" to generate a new version.\n",
        "   - e. Repeat steps b-d. You can stop \"training\" at any time.\n",
        "\n",
        "3. **Stop \"Training\"**:\n",
        "   In each iteration, you need to intervene and analyze:\n",
        "   - Are both training accuracy and evaluation accuracy steadily improving?\n",
        "   - Has evaluation accuracy stagnated or suddenly dropped (possible overfitting)?\n",
        "   - Do the newly generated \"wrong answers\" expose ambiguity or contradictions in the samples themselves? (Poor sample quality)\n",
        "\n",
        "> Note: If sample quality is poor, causing inconsistent evaluation standards or ambiguous judgments on similar questions, the LLM may never be able to construct a well-performing \"AI Judge.\" At this point, you can stop iteration and first optimize your sample data.\n",
        "\n",
        "Finally, you decide when to stop \"training\" and adopt the version of the prompt that performs best on the evaluation set as your final \"**AI Judge**.\"\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "    <a href=\"https://img.alicdn.com/imgextra/i1/O1CN01gJT1AY22xhe34lZwM_!!6000000007187-2-tps-1864-598.png\" target=\"_blank\">\n",
        "    <img src=\"https://img.alicdn.com/imgextra/i1/O1CN01gJT1AY22xhe34lZwM_!!6000000007187-2-tps-1864-598.png\" width=\"800\"/>\n",
        "    </a>\n",
        "  <p>Figure: Iterating the AI Judge via ML training; stop based on train vs. eval performance.</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "#### 4.7.1 Prepare and Split the Dataset\n",
        "\n",
        "First, you need a batch of samples carefully labeled by you or domain experts. This is the foundation of the entire process and the cornerstone of quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2778ce00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare a batch of high-quality labeled samples\n",
        "# In practice, these samples can be uploaded to AI platforms (like Bailian) to create evaluation sets\n",
        "labeled_samples = [\n",
        "    # --- 3 samples for \"training\" ---\n",
        "    {\n",
        "        \"id\": \"train_01\",\n",
        "        \"response\": \"Company benefits: Health insurance, family can use. 15 days annual leave, 5 days sick leave. Transportation subsidy 500, meal subsidy 300. Training fund 8000. Gym has discounts.\", \n",
        "        \"label\": \"Fail\", \n",
        "    },\n",
        "    {\n",
        "        \"id\": \"train_02\",\n",
        "        \"response\": \"\"\"Company benefits include:\\n1. Health and leave: Health insurance, annual checkup, 15 days annual leave and 5 days sick leave.\\n2. Subsidies and incentives: Monthly 500 transportation subsidy and 300 meal subsidy, plus 8000 yuan training fund.\"\"\", \n",
        "        \"label\": \"Fail\", \n",
        "    },\n",
        "    {\n",
        "        \"id\": \"train_03\",\n",
        "        \"response\": \"\"\"üëã Welcome! We've prepared awesome benefits for you:\\n- üè• Comprehensive health insurance and annual checkup\\n- üå¥ 15 days annual leave + 5 days sick leave\\n- üí∞ Monthly transportation and meal subsidies\\n- üéì Up to 8000 yuan training fund\\nLooking forward to working with you! üéâ\"\"\", \n",
        "        \"label\": \"Pass\", \n",
        "    },\n",
        "    \n",
        "    # --- 2 samples for \"evaluation\" ---\n",
        "    {\n",
        "        \"id\": \"eval_01\",\n",
        "        \"response\": \"We have health insurance, annual leave, sick leave, transportation and meal subsidies, training fund and gym discounts.\", \n",
        "        \"label\": \"Fail\", \n",
        "    },\n",
        "    {\n",
        "        \"id\": \"eval_02\",\n",
        "        \"response\": \"\"\"Hello, new colleague! Company benefits are great:\\n- Health includes insurance and checkups.\\n- Leave includes annual and sick leave.\\n- Money includes subsidies and education fund.\\nWish you a happy work experience!\"\"\", \n",
        "        \"label\": \"Pass\", \n",
        "    },\n",
        "]\n",
        "\n",
        "# Split dataset proportionally (manual simulation here)\n",
        "# In real projects, use train_test_split from scikit-learn for random splitting\n",
        "train_set = labeled_samples[:3]\n",
        "eval_set = labeled_samples[3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca622ab",
      "metadata": {},
      "source": [
        "\n",
        "#### 4.7.2 Iterative \"Training\"\n",
        "\n",
        "By having the model continuously learn from \"wrong answers,\" the \"judge prompt\" becomes smarter and smarter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11564b9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a helper function to evaluate prompt accuracy on a specified dataset\n",
        "def evaluate_judge_prompt(judge_prompt, dataset):\n",
        "    correct_predictions = 0\n",
        "    misjudged_cases = []\n",
        "    \n",
        "    for sample in dataset:\n",
        "        prompt = judge_prompt.format(response_to_judge=sample[\"response\"])\n",
        "        \n",
        "        # Simulate LLM call, actual: predicted_label = llm.invoke(prompt)\n",
        "        # For demonstration, manually simulate different prompt version behaviors\n",
        "        predicted_label = \"Pass\" if \"Welcome\" in sample[\"response\"] or \"Hello\" in sample[\"response\"] else \"Fail\"\n",
        "        if \"v1\" in judge_prompt:\n",
        "             if \"1.\" in sample[\"response\"]: predicted_label = \"Pass\"\n",
        "\n",
        "        is_correct = (predicted_label == sample[\"label\"])\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "        else:\n",
        "            misjudged_cases.append({\n",
        "                \"response\": sample[\"response\"],\n",
        "                \"current_judgment\": predicted_label,\n",
        "                \"correct_judgment\": sample[\"label\"]\n",
        "            })\n",
        "            \n",
        "    accuracy = correct_predictions / len(dataset)\n",
        "    return accuracy, misjudged_cases\n",
        "\n",
        "# ----------------- Iterative \"Training\" Begins -----------------\n",
        "\n",
        "# a. Prepare initial version of \"judge prompt\" (v1.0)\n",
        "judge_prompt_v1 = \"\"\"\n",
        "# v1\n",
        "„ÄêRole„ÄëYou are an employee experience expert.\n",
        "„ÄêTask„ÄëJudge whether the„ÄêResponse to Evaluate„Äëis qualified.\n",
        "„ÄêEvaluation Criteria„Äë\n",
        "1. Clear structure: Information listed in points or categories.\n",
        "2. Complete information: Mentions core benefits.\n",
        "„ÄêResponse to Evaluate„Äë\n",
        "---\n",
        "{response_to_judge}\n",
        "---\n",
        "„ÄêOutput Requirements„ÄëPlease only answer \"Pass\" or \"Fail\".\n",
        "\"\"\"\n",
        "\n",
        "current_judge_prompt = judge_prompt_v1\n",
        "max_iterations = 2\n",
        "\n",
        "for i in range(max_iterations):\n",
        "    print(f\"--- Iteration {i+1} ---\")\n",
        "    \n",
        "    # b. Make \"predictions\" on training and evaluation sets and record scores\n",
        "    train_accuracy, misjudged_on_train = evaluate_judge_prompt(current_judge_prompt, train_set)\n",
        "    eval_accuracy, _ = evaluate_judge_prompt(current_judge_prompt, eval_set)\n",
        "    \n",
        "    print(f\"Training Set Accuracy: {train_accuracy:.0%}\")\n",
        "    print(f\"Evaluation Set Accuracy: {eval_accuracy:.0%}\")\n",
        "\n",
        "    if not misjudged_on_train:\n",
        "        print(\"\\nNo errors on training set, training complete.\")\n",
        "        break\n",
        "        \n",
        "    # c. Find \"wrong answers\" on training set\n",
        "    first_error = misjudged_on_train[0]\n",
        "    print(f\"\\nFound wrong answer: Model misjudged a response that should be '{first_error['correct_judgment']}' as '{first_error['current_judgment']}'\")\n",
        "\n",
        "    # d. Package \"wrong answers\" and have LLM help optimize the prompt\n",
        "    judge_optimizer_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are a top-tier prompt engineer.\n",
        "    „ÄêBackground„ÄëMy current judge prompt made an error when evaluating a sample.\n",
        "    \n",
        "    „ÄêMy Current Judge Prompt„Äë\n",
        "    ---\n",
        "    {current_judge_prompt}\n",
        "    ---\n",
        "    \n",
        "    „ÄêError Case„Äë\n",
        "    - Response to evaluate: \"{first_error['response']}\"\n",
        "    - My tool's wrong judgment: \"{first_error['current_judgment']}\"\n",
        "    - Expected correct judgment: \"{first_error['correct_judgment']}\"\n",
        "    - Reason I think it erred: Current criteria are too loose, didn't emphasize 'warm and friendly' tone.\n",
        "    \n",
        "    „ÄêTask„ÄëPlease rewrite my judge prompt to make criteria stricter and correct the above error.\n",
        "    „ÄêRequirements„ÄëPlease only return the optimized new prompt.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Optimizing prompt based on wrong answers...\")\n",
        "    new_judge_prompt = \"\"\"\n",
        "    # v2\n",
        "    „ÄêRole„ÄëYou are an internal communications expert pursuing ultimate employee experience, with a discerning eye.\n",
        "    „ÄêTask„ÄëStrictly judge whether the„ÄêResponse to Evaluate„Äëis qualified.\n",
        "    „ÄêEvaluation Criteria„Äë\n",
        "    1. **Welcoming Atmosphere (Required)**: Must have clear, warm welcome greeting.\n",
        "    2. **Structured Presentation (Required)**: Must use lists or sections.\n",
        "    „ÄêResponse to Evaluate„Äë\n",
        "    ---\n",
        "    {response_to_judge}\n",
        "    ---\n",
        "    „ÄêOutput Requirements„ÄëPlease only answer \"Pass\" or \"Fail\".\n",
        "    \"\"\"\n",
        "    \n",
        "    # e. Update to new version prompt, enter next iteration\n",
        "    current_judge_prompt = new_judge_prompt\n",
        "    print(\"-\" * 20 + \"\\n\")\n",
        "\n",
        "print(\"\\n--- Iteration Ended ---\")\n",
        "final_judge_prompt = current_judge_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e57a51",
      "metadata": {},
      "source": [
        "\n",
        "#### 4.7.3 \"Good Enough\" Means You Can \"Stop Training\"\n",
        "\n",
        "In the iteration loop above, you can clearly see the score changes:\n",
        "- Iteration 1:\n",
        "    - Training Set Accuracy: 67% (v1 prompt misjudged train_02)\n",
        "    - Evaluation Set Accuracy: 100% (v1 prompt happened to judge both correctly)\n",
        "- Iteration 2:\n",
        "    - Training Set Accuracy: 100% (v2 prompt corrected the error)\n",
        "    - Evaluation Set Accuracy: 100%\n",
        "\n",
        "In this simple example, evaluation accuracy stayed high throughout. But in real scenarios, you might see evaluation accuracy stagnate or even decline.\n",
        "\n",
        "Based on the above data, you'll find that the v2 version prompt achieved 100% accuracy on both training and evaluation sets. Therefore, you can adopt `judge_prompt_v2` as your final, validated \"AI Judge.\" If evaluation results are unsatisfactory, you may need to go back to the beginning and check and optimize your `labeled_samples`.\n",
        "\n",
        "#### 4.7.4 Use the \"AI Judge\" to Guide Automated Optimization\n",
        "\n",
        "Now you have an \"AI Judge\" (`final_judge_prompt`) that's been repeatedly validated and tested, ready for deployment.\n",
        "\n",
        "You need to use it to replace the evaluation step in section 4.6 that compared differences between two samples to determine usability, making the entire workflow more rigorous.\n",
        "\n",
        "**New Workflow:**\n",
        "1. **Generate**: Use the current \"generation prompt\" to produce a response.\n",
        "2. **Judge**: Call your trained \"AI Judge prompt\" to make a \"Pass/Fail\" judgment on the response.\n",
        "3. **Decide & Optimize**:\n",
        "    - If the judgment is \"**Pass**,\" the loop ends‚Äîoptimization successful.\n",
        "    - If the judgment is \"**Fail**,\" pass the \"generation prompt,\" \"failed response,\" and the \"**AI Judge's rulebook**\" (i.e., its prompt) together to the \"optimizer,\" letting it generate a new version prompt based on the clear failure criteria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0638b596",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a more powerful optimization function that directly references the \"judge's\" rules\n",
        "def optimize_with_judge_rules(current_prompt, failed_response, judge_prompt):\n",
        "    \"\"\"Call LLM to optimize generation prompt based on judge's rules\"\"\"\n",
        "    optimizer_prompt = f\"\"\"\n",
        "    „ÄêRole„ÄëYou are a top-tier prompt engineer.\n",
        "    „ÄêBackground„ÄëI have a \"generation prompt,\" but the response it generates failed the \"AI Judge's\" review.\n",
        "    \n",
        "    „ÄêGeneration Prompt„Äë\n",
        "    ---\n",
        "    {current_prompt}\n",
        "    ---\n",
        "    \n",
        "    „ÄêIts Failed Response„Äë\n",
        "    ---\n",
        "    {failed_response}\n",
        "    ---\n",
        "    \n",
        "    „ÄêAI Judge's Rulebook (Reason for Failure)„Äë\n",
        "    ---\n",
        "    {judge_prompt}\n",
        "    ---\n",
        "    \n",
        "    „ÄêTask„Äë\n",
        "    Please carefully study the \"AI Judge's Rulebook\" and rewrite the \"Generation Prompt\" to ensure the new prompt can generate a response that passes the rulebook's review.\n",
        "    \n",
        "    „ÄêRequirements„Äë\n",
        "    Please only return the optimized new \"Generation Prompt\" without any other explanations.\n",
        "    \"\"\"\n",
        "    new_prompt = llm.invoke(optimizer_prompt)\n",
        "    return new_prompt\n",
        "\n",
        "# --- Run the automated optimization loop driven by \"AI Judge\" ---\n",
        "current_generating_prompt = initial_prompt\n",
        "max_iterations = 5\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"Starting the final automated optimization workflow driven by 'AI Judge'\")\n",
        "print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "for i in range(max_iterations):\n",
        "    print(f\"--- Final Workflow: Iteration {i+1} ---\")\n",
        "    \n",
        "    # 1. Generate response\n",
        "    prompt_for_generator = current_generating_prompt.format(retrieved_text=retrieved_text)\n",
        "    generated_response = llm.invoke(prompt_for_generator)\n",
        "    \n",
        "    print(f\"Generated Response:\\n---\\n{generated_response}\\n---\")\n",
        "    \n",
        "    # 2. Call \"AI Judge\" for judgment\n",
        "    judge_prompt_filled = final_judge_prompt.format(response_to_judge=generated_response)\n",
        "    judgment = llm.invoke(judge_prompt_filled)\n",
        "    \n",
        "    judgment_cleaned = judgment.strip().replace(\".\", \"\")\n",
        "    print(f\"'AI Judge' Verdict: {judgment_cleaned}\")\n",
        "\n",
        "    # 3. Decision\n",
        "    if judgment_cleaned == \"Pass\" or judgment_cleaned == \"ÈÄöËøá\":\n",
        "        print(\"\\n‚úÖ Optimization successful! Generated response passed the 'AI Judge' review.\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"‚ùå Did not pass review, optimizing based on 'Judge's' rules...\")\n",
        "        current_generating_prompt = optimize_with_judge_rules(\n",
        "            current_generating_prompt, \n",
        "            generated_response, \n",
        "            final_judge_prompt\n",
        "        )\n",
        "        print(\"-\" * 20 + \"\\n\")\n",
        "else:\n",
        "    print(\"\\nMaximum iterations reached, stopping optimization.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"Final Adopted Generation Prompt:\")\n",
        "print(\"=\"*30)\n",
        "print(current_generating_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96397dc1",
      "metadata": {},
      "source": [
        "The \"**AI Judge**\" makes your prompt optimization workflow more controllable:\n",
        "- Whenever the \"generation prompt\" produces a new response, first have the \"AI Judge\" score it.\n",
        "- **Passed means optimization successful; failed means use this \"bad answer\" as new material to continue iterating your generation prompt**.\n",
        "\n",
        "You can always adopt this approach: **first build the \"judge,\" then optimize the \"player.\"** Build a measurable, trustworthy, automatable \"**Prompt Optimization Engineering**\" system."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6214d05f-8154-44f7-943b-7f04196a3275",
      "metadata": {},
      "source": [
        "## 5. Using LLMs for intent recognition\n",
        "\n",
        "After learning about prompt engineering techniques, your colleagues have realized that the Q&A bot can not only be used to retrieve information, but also assist  in reviewing documents and translating content. While the Q&A bot is generally effective at identifying issues, it still has some limitations, as shown in the following example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21013ec-b0e4-4428-9bb1-e2f309df563a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:02.890690Z",
          "start_time": "2025-01-15T06:51:00.165152Z"
        }
      },
      "outputs": [],
      "source": [
        "rag.ask('Please help me review this sentence: Does a technical content engineer need to design and develop high-quality educational materials and courses?', query_engine=query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25dd1116-048c-4599-ba9d-98750978504a",
      "metadata": {},
      "source": [
        "From the experimental results above, it's clear that the Q&A robot responded directly to the user's statement without recognizing the intended task of reviewing a sentence for errors. To understand why, let's look at what content the Q&A bot retrieved from the knowledge base before generating its response.\n",
        "\n",
        "The following excerpt  shows part of the retrieved content:\n",
        "\n",
        "```\n",
        "Core Responsibilities\n",
        "Integrating¬†educational¬†theory¬†with¬†technical¬†practice,¬†support¬†learners'¬†growth¬†and¬†development¬†through¬†high-quality¬†content¬†creation.\n",
        "```\n",
        "\n",
        "You can refer to <2_4_Automate_Evaluation_of_FAQ_Bot_Performance> to learn how to view the specific fragments retrieved  from the knowledge base.\n",
        "\n",
        "As seen from the retrieved content, most of the information relates to the responsibilities of a technical content engineer. This context likely influenced the Q&A bot's response, causing it to focus on content creation or editing rather than performing the requested document review.\n",
        "\n",
        "Since contextual information greatly impacts the responses generated by LLMs, it is not always necessary to use the RAG method for every query. You can leverage LLMs to first perform¬†intent recognitionclassifying the type of user question.\n",
        "\n",
        "For example:\n",
        "\n",
        "- If the task involves¬†document review,¬†grammar checking, or¬†content translation, the input should bypass the RAG pipeline and be handled directly by the LLM.\n",
        "- Only for¬†internal knowledge queries‚Äîsuch as \"What are the guidelines for writing API documentation?\"‚Äîshould the RAG pipeline be used to generate answers based on the knowledge base."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df6389a-cfa2-450e-b7fe-72e24dd79838",
      "metadata": {},
      "source": [
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01nWw7IO2AGCuuujq8G_!!6000000008175-2-tps-3329-1398.png\" width=\"1000\">  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fe0489-872c-466b-855a-3768a5127f9e",
      "metadata": {},
      "source": [
        "There are two main methods for using LLMs to perform intent recognition:\n",
        "\n",
        "- Using prompts: By designing specific prompts, you can guide the LLM to generate responses that meet your expectations. This method does not require modifying the model‚Äôs parameters; instead it relies on well-constructed inputs to activate knowledge already embedded within the model.\n",
        "- Fine-tuning the model: Based on a pre-trained foundational model, you can further train the model using specific annotated data to improve its ability to classify intents. This process involves adjusting some or all of the model‚Äôs parameters to better suit your use case.\n",
        "\n",
        "In this section, we will focus on helping the LLM perform intent recognition by crafting effective prompts. You can explore the method of fine-tuning in the subsection <2_7_Improve_Model_Accuracy_and_Efficiency_via_Fine_Tuning>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe46f635-ac13-488d-9ab7-89b49ae8878d",
      "metadata": {},
      "source": [
        "### 5.1 Intent recognition\n",
        "\n",
        "Next, we will construct prompts to enable LLMs to classify questions. Since many applications require structured output after intent recognition‚Äîsuch as for document review or integration with RAG pipelines‚Äîthe following prompting techniques will be used to ensure accurate classification of user queries:\n",
        "\n",
        "* Specify the output format: Clearly define the output format to make classification results standardized and easy to parse.\n",
        "* Few-shot examples: Include examples to help the LLM understand the characteristics and classification rules of each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ff599e-a7fa-41d8-b734-d949e9f93900",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:10.939753Z",
          "start_time": "2025-01-15T06:51:09.207243Z"
        }
      },
      "outputs": [],
      "source": [
        "from chatbot import llm\n",
        "\n",
        "# Build the prompt\n",
        "prompt = '''\n",
        "[Role Background]\n",
        "You are a question classification router, responsible for determining the type of user questions and categorizing them into one of the following three categories:\n",
        "1. Internal Company Document Query\n",
        "2. Content Translation\n",
        "3. Document Review\n",
        "\n",
        "[Task Requirements]\n",
        "Your task is to judge the intent based on the user's input content and select only the most appropriate category. Output only the category name without additional explanation. The judgment criteria are as follows:\n",
        "\n",
        "- If the question involves company policies, processes, internal tools, or job descriptions and responsibilities, choose \"Internal Company Document Query\".\n",
        "- If the input contains any foreign language or words like \"translation\", choose \"Content Translation\".\n",
        "- If the question involves checking or summarizing external documents or link content, choose \"Document Review\".\n",
        "- The user's previous inputs have no relation to the question classification; consider each dialogue independently for classification.\n",
        "\n",
        "[Few-shot Examples]\n",
        "Example 1: User input: \"What are the commonly used project management tools within our company?\"\n",
        "Category: Internal Company Document Query\n",
        "\n",
        "Example 2: User input: \"Please translate the following sentence into French: How can we finish the assignment on time?\"\n",
        "Category: Content Translation\n",
        "\n",
        "Example 3: User input: \"Please review the document at this link: https://www.alibabacloud.com/help/en/model-studio/models#daf364417cjit\"\n",
        "Category: Document Review\n",
        "\n",
        "Example 4: User input: \"Please review the following content: Does a technical content engineer need to design and develop high-quality educational materials and courses?\"\n",
        "Category: Document Review\n",
        "\n",
        "Example 5: User input: \"What are the core responsibilities of a technical content engineer?\"\n",
        "Category: Internal Company Document Query\n",
        "\n",
        "[User Input]\n",
        "The following is the user's input, please determine the classification:\n",
        "'''\n",
        "\n",
        "# Get the type of question\n",
        "def get_question_type(question):\n",
        "    return llm.invoke(prompt + question)\n",
        "\n",
        "print(get_question_type('https://www.promptingguide.ai/techniques/fewshot'), '\\n')\n",
        "print(get_question_type('Translate the following sentence into German: That is a big one I dont know why'), '\\n')\n",
        "print(get_question_type('As a technical content engineer, what should I pay attention to?'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f74506a-504d-47a2-bb9d-e17198294445",
      "metadata": {},
      "source": [
        "By providing clear output formats and few-shot examples, the Q&A bot can more accurately identify question types and produce responses in the expected format. This optimization makes classification tasks more standardized and lays the foundation for incorporating intent recognition into the Q&A bot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb001699-4451-4e1f-808d-1b38a80c9431",
      "metadata": {},
      "source": [
        "### 5.2 Applying intent recognition to Q&A bots\n",
        "\n",
        "Once the Q&A bot can recognize the intent of a user's question, it can first classify the type of query, then use tailored prompts and workflows to generate the most appropriate response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d8de14-53ee-4d5f-8f5a-0dfd9bf4e0ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:13.272321Z",
          "start_time": "2025-01-15T06:51:13.111875Z"
        }
      },
      "outputs": [],
      "source": [
        "def ask_llm_route(question):\n",
        "    question_type = get_question_type(question)\n",
        "    print(f'Question: {question}\\nType: {question_type}')\n",
        "  \n",
        "    reviewer_prompt = \"\"\"\n",
        "    [Role Background]\n",
        "    You are a document error-checking expert responsible for identifying obvious errors in documents or web content.\n",
        "    [Task Requirements]\n",
        "    - Your response should be concise.\n",
        "    - If there are no obvious issues, reply directly with 'No issues'.\\n\n",
        "    [Input as follows]\\n\"\"\"\n",
        "  \n",
        "    translator_prompt = \"\"\"\n",
        "   [Task Requirements]\n",
        "    You are a translation expert who identifies text in different languages and translates it into the target language. If not specified, the target language is French.\n",
        "    [Input as follows]\\n\"\"\"\n",
        "\n",
        "    if question_type == 'Document Review':\n",
        "        return llm.invoke(reviewer_prompt + question)\n",
        "    elif question_type == 'Internal Company Document Query':\n",
        "        return rag.ask(question, query_engine=query_engine)\n",
        "    elif question_type == 'Content Translation':\n",
        "        return llm.invoke(translator_prompt + question)\n",
        "    else:\n",
        "        return \"Unable to recognize the question type. Please re-enter.\"\n",
        "\n",
        "query_engine =rag.create_query_engine(index=rag.load_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d6ec83-8a99-45a5-a305-5088adc89be0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:22.925058Z",
          "start_time": "2025-01-15T06:51:14.191174Z"
        }
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "print(ask_llm_route('https://www.promptingguide.ai/techniques/fewshot'), '\\n')\n",
        "\n",
        "# Question 2\n",
        "print(ask_llm_route('Please help me check this document: Do technicel content engineers need to optimize and update content and collaborate across departments?'), '\\n')\n",
        "\n",
        "# Question 3\n",
        "print(ask_llm_route('Do technical content engineers need to optimize and update content and collaborate across departments?'), '\\n')\n",
        "\n",
        "# Question 4:\n",
        "print(ask_llm_route('Translate the following sentence into French: A true master always carries the heart of a student.'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a95edd-ca52-434f-b6b5-6b0537fdfec6",
      "metadata": {},
      "source": [
        "As shown in the above experiments, the optimized Q&A bot does not always go through the RAG pipeline for every question. This approach not only saves computational resources but also prevents the knowledge base from interfering with the model's reasoning process, which could otherwise lead to lower-quality or irrelevant responses.\n",
        "\n",
        "- Resource savings: For tasks such as checking document errors, the LLM can respond without needing to retrieve reference materials. This highlights that previous implementations may have unnecessarily consumed resources by always using the RAG pipeline.\n",
        "- Avoiding misunderstandings: In earlier versions, the system retrieved reference materials for every query. These recalled text segments could sometimes confuse the LLM, affecting its ability to accurately understand the question and leading to irrelevant or off-target responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7c107a1",
      "metadata": {},
      "source": [
        "## 6. Reasoning LLMs\n",
        "\n",
        "The prompt techniques and prompt frameworks discussed earlier are widely applicable to general-purpose LLMs, such as Qwen2.5-max, GPT-4, and DeepSeek-V3. These models are designed for a broad range of tasks, including general conversation, knowledge-based Q&A, and text generation. \n",
        "\n",
        "In addition to general-purpose LLMs, there is another category of models specifically designed for reasoning‚ÄîReasoning LLMs. These models excel in tasks that require logical thinking, multi-step problem-solving, and mathematical computation.\n",
        "\n",
        "### 6.1 What Are reasoning LLMs?\n",
        "\n",
        "You may have already learned about Alibaba Cloud's reasoning LLM [QwQ](https://www.alibabacloud.com/help/en/model-studio/models#fd7d313ca5khs), through the extended reading in Section 2.1. To make it easier to use in this section, the code from Section 2.1 has been updated. Try running the following example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500b6e30",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "def reasoning_model_response(user_prompt, system_prompt=\"You are a programming assistant.\", model=\"qwq-plus\"):\n",
        "    \"\"\"\n",
        "    prompt: The prompt input by the user\n",
        "    model: Here we use qwq-32b as an example. You can replace it with other inference model names as needed, such as: deepseek-r1\n",
        "    \"\"\"\n",
        "    # Initialize client\n",
        "    client = OpenAI(\n",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "        base_url=os.getenv(\"DASHSCOPE_API_BASE\")\n",
        "    )\n",
        "\n",
        "    # Initialize status variables\n",
        "    is_answering = False\n",
        "\n",
        "    # Initiate streaming request\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        # messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    # Print thinking process title\n",
        "    print(\"\\n\" + \"=\" * 20 + \"Thinking Process\" + \"=\" * 20 + \"\\n\")\n",
        "\n",
        "    # Handle streaming response\n",
        "    for chunk in completion:\n",
        "        if chunk.choices:\n",
        "            delta = chunk.choices[0].delta\n",
        "            if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:\n",
        "                # Process thinking process content\n",
        "                print(delta.reasoning_content, end='', flush=True)\n",
        "            else:\n",
        "                # Switch to answer output mode\n",
        "                if delta.content != \"\" and not is_answering:\n",
        "                    print(\"\\n\" + \"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")\n",
        "                    is_answering = True\n",
        "                # Process answer content\n",
        "                if delta.content:\n",
        "                    print(delta.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e65df7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "reasoning_model_response(user_prompt=\"Who are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fea9f93",
      "metadata": {},
      "source": [
        "As shown in the example, reasoning LLMs include an additional \"thinking process\" step, which mimics how humans approach complex problems. This allows the model to break down the problem logically before providing a final answer. This structured approach helps reduce errors and improves transparency in the model‚Äôs decision-making process.\n",
        "\n",
        "During the thinking process, if the model detects a contradiction, it can backtrack and revise its logic, making it easier for users to understand and verify the reasoning path. This feature is especially useful in scenarios requiring rigorous logic, such as mathematical problem-solving, legal analysis, or scientific reasoning.\n",
        "\n",
        "Compared with general LLMs, reasoning models are usually more reliable when solving complex problems, such as mathematical problem-solving, code writing, legal case analysis, and other scenarios that require rigorous reasoning. This does not mean that reasoning models are necessarily better; both types of models have their own application scenarios. The following table compares these two types of models across several key dimensions:\n",
        "\n",
        "| Dimension            | Reasoning Model  | General Model  |\n",
        "|-------------------|------------------|------------|\n",
        "| Design Goal     | Focuses on tasks requiring deep analysis such as **logical reasoning, multi-step problem solving, and mathematical calculations** | Targets broad scenarios such as **general conversation, knowledge-based Q&A, text generation |\n",
        "| Training Data Emphasis  | Enhanced reasoning capabilities through large datasets of **math problem solutions, code logic, scientific reasoning**, etc. | Covers massive data across multiple domains such as **encyclopedias, literature, conversations**, etc. |\n",
        "| Typical Output Characteristics | Outputs include **complete derivations**, focusing on the integrity of logical chains | Outputs are **concise and direct**, emphasizing natural language expression of results |\n",
        "| Response Speed | Complex reasoning tasks result in **slower responses** (requires multi-step calculations) | Routine tasks have **faster responses** (mainly single-step generation) |\n",
        "\n",
        "This comparison highlights that reasoning models excel in structured, analytical tasks, while general LLMs are more versatile for everyday use cases. Choosing the right model depends on the specific requirements of the task at hand. \n",
        "\n",
        "Here are some tips:\n",
        "* **Clearly defined general tasks**: For problems that are well-defined and straightforward,¬†general models¬†typically perform well.\n",
        "* **Complex tasks**: For highly complex tasks that require ¬†**more precise and reliable**¬†answers, ¬†**reasoning models** are recommended. These tasks may include:\n",
        "    * Ambiguous tasks: When there is very little task-related information available, and clear guidance cannot be provided to the model.\n",
        "    * Finding a needle in a haystack: When dealing with large amounts of unstructured data, extracting the most relevant information, or identifying connections and differences.\n",
        "    * Debugging and improving code: When the task involves reviewing and refining large volumes of code.\n",
        "* **Speed and cost**: Reasoning models generally take longer to process due to their detailed reasoning steps. If time and cost are critical factors and the task complexity is low,¬†**general models**¬†may be a more efficient choice.\n",
        "\n",
        "In addition, you can combine both types of models in your application: use the reasoning model for planning and decision-making, and the general model for executing specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580c0b84",
      "metadata": {},
      "source": [
        "### 6.2 Prompt techniques suitable for reasoning LLMs\n",
        "\n",
        "Reasoning models can provide detailed and well-structured responses even when faced with relatively ambiguous tasks. You can still ensure the baseline quality of reasoning in large language models through **prompting techniques**:<br>\n",
        "\n",
        "#### Technique One: Keep task prompts concise and clear, providing sufficient background information\n",
        "\n",
        "The¬†**clear expression of requirements**¬†introduced in Section 4.1 also applies to reasoning models. Although these models are highly capable, they cannot \"read minds.\" Therefore, it's important to keep your prompts¬†**concise and clear**, allowing the reasoning model to focus on the core task. Providing enough context ensures that the model understands the problem and can generate accurate, logical responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5cc8ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def example(a):\n",
        "  b = []\n",
        "  for i in range(len(a)):\n",
        "    b.append(a[i]*2)\n",
        "  return sum(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e6f145",
      "metadata": {},
      "source": [
        "Through the above example, you can see that even if you only provide a large inference model with a piece of code, it can still generate detailed and thoughtful responses through a series of reasoning steps. However, the generated reasoning may contain a lot of information that is not directly relevant to your needs. To get more focused and targeted results, it's important to clearly define the **task objective** and guide the model toward the specific outcome you want. Here's an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c90bd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_A = \"\"\"\n",
        "What's wrong with the following Python code? How to optimize it?\n",
        "def example(a):\n",
        "    b = []\n",
        "    for i in range(len(a)):\n",
        "        b.append(a[i]*2)\n",
        "    return sum(b)\n",
        "\"\"\"\n",
        "\n",
        "reasoning_model_response(user_prompt=prompt_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5500ab0",
      "metadata": {},
      "source": [
        "Similarly, you can further refine the scope by combining techniques from this section, such as¬†**4.2 Define roles and audience**¬†and¬†**4.3 Specifying output formats**, to ensure the results align with your expectations.Additionally, when the prompt is relatively complex, using delimiters can help the model better understand your intent and focus on the key elements of the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d81507",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_B = \"\"\"\n",
        "<audience>Beginner Python developers</audience>\n",
        "\n",
        "<task>Function performance optimization, optimize the code in the code.</task>\n",
        "\n",
        "<format>\n",
        "If there are multiple optimization solutions, please output them in the following format:\n",
        "[Optimization Solution X]\n",
        "Problem Description: [Description]\n",
        "Optimization Solution: [Description]\n",
        "Example Code: [Code Block]\n",
        "</format>\n",
        "\n",
        "<code>\n",
        "def example(a):\n",
        "  b = []\n",
        "  for i in range(len(a)):\n",
        "    b.append(a[i]*2)\n",
        "  return sum(b)\n",
        "</code>\n",
        "\"\"\"\n",
        "\n",
        "reasoning_model_response(user_prompt=prompt_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c163ae4c",
      "metadata": {},
      "source": [
        "#### Technique two: Avoid Explicit CoT prompts\n",
        "In Section 4.5, you learned how to improve response quality by using the chain-of-thought (CoT)technique with LLMs. However, for reasoning models, it is generally unnecessary to prompt them with phrases like \"think step by step\" or \"explain your reasoning.\" These models are designed to perform deep thinking naturally. Adding such instructions may actually limit their performance unless you specifically need the model to follow a strict reasoning path, which is uncommon.\n",
        "\n",
        "\n",
        "#### Technique three: Adjust prompts based on model responses\n",
        "Reasoning models are well-suited for analyzing their own thought processes, because their responses often include¬†**reasoning steps**. This makes it easier for you to refine prompts based on the model's output. You don‚Äôt need to worry about whether your initial prompt is perfect‚Äîsimply continue engaging with the model, providing additional information, and refining your prompts during the conversation.\n",
        "\n",
        "For example, if your descriptions are¬†too abstract¬†or¬†not accurately described, you can use the technique of¬†adding examples, as¬†discussed in Section 4.4, to clarify the task. These examples can often be drawn from previous interactions with the model.This iterative process can be repeated multiple times, continuously adjusting prompts and allowing the model to refine its reasoning until it meets your expectations.\n",
        "\n",
        "#### Technique four: Collaborative task completion between reasoning models and general models\n",
        "Reasoning models act like \"super-smart colleagues,\" effectively assisting you with tasks like reasoning and planning. However, for execution tasks involving repetitive actions, reasoning models may overthink, which could be inefficient.\n",
        "\n",
        "A better approach is to let reasoning models and general models work together, leveraging their respective strengths:\n",
        "* Reasoning models handle \"slow thinking\" tasks such as planning, analysis, and problem-solving.\n",
        "* General models focus on \"fast thinking\" tasks, such as executing specific actions or generating content quickly.\n",
        "\n",
        "Try running and understanding the following project, which demonstrates how these models can collaborate:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01807cca",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "from typing import Generator\n",
        "\n",
        "class TaskPlanningSystem:\n",
        "    def __init__(self):\n",
        "        # Initialize client connection\n",
        "        self.client = OpenAI(\n",
        "            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "            base_url=os.getenv(\"DASHSCOPE_API_BASE\")\n",
        "        )\n",
        "        \n",
        "        # System toolset (can be extended as needed)\n",
        "        self.tools = {\n",
        "            \"create_task\": {\n",
        "                \"desc\": \"Create a new task item\",\n",
        "                \"params\": {\"description\": \"str\", \"priority\": \"int\"}\n",
        "            },\n",
        "            \"schedule_event\": {\n",
        "                \"desc\": \"Schedule an event\", \n",
        "                \"params\": {\"title\": \"str\", \"time\": \"datetime\"}\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def generate_plan(self, user_request: str) -> Generator[str, None, None]:\n",
        "        \"\"\"Stream-generated task planning\"\"\"\n",
        "        # Build planning prompt\n",
        "        system_prompt = f\"\"\"You are a senior task planner, please convert the user's requirements into executable steps:\n",
        "        \n",
        "        Available tools (format: [[tool name]]):\n",
        "        {json.dumps(self.tools, indent=2)}\n",
        "        \n",
        "        Output requirements:\n",
        "        1. Use Markdown list format\n",
        "        2. Each step should indicate the tool name\n",
        "        3. Include necessary parameter examples\n",
        "        \"\"\"\n",
        "        model=\"qwq-plus\"\n",
        "        # Initiate streaming request\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_request}\n",
        "            ],\n",
        "            stream=True,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Print thinking process title\n",
        "        print(f\"\\n\\n=== [Planning Thinking Phase] Using reasoning model: {model}===\")\n",
        "        is_answering = False\n",
        "        # Process streaming response\n",
        "        for chunk in completion:\n",
        "            if chunk.choices:\n",
        "                delta = chunk.choices[0].delta\n",
        "                if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:\n",
        "                    # Process thinking process content\n",
        "                    reasoning_content = delta.reasoning_content\n",
        "                    yield reasoning_content\n",
        "                else:\n",
        "                    # Switch to answer output mode\n",
        "                    if delta.content != \"\" and not is_answering:\n",
        "                        print(f\"\\n\\n=== [Planning Generation Phase] Using reasoning model: {model}===\")\n",
        "                        is_answering = True\n",
        "                    # Process answer content\n",
        "                    if delta.content:\n",
        "                        content = delta.content\n",
        "                        yield content\n",
        "\n",
        "    def execute_plan(self, plan: str) -> dict:\n",
        "        \"\"\"Execute the generated task plan\"\"\"\n",
        "        # Call general model to parse the plan\n",
        "        analysis_prompt = f\"\"\"Please parse the following task plan and generate executable instructions:\n",
        "        \n",
        "        Plan content:\n",
        "        {plan}\n",
        "        \n",
        "        Output requirements:\n",
        "        - The returned content must be in JSON format only, do not include other information, do not output code blocks.\n",
        "        - Include tool names and parameters\n",
        "        - Example:\n",
        "            {{\"steps\": [\n",
        "                {{\"tool\": \"create_task\", \"params\": {{\"description\": \"...\"}}}}\n",
        "            ]}}\n",
        "        \"\"\"\n",
        "        model=\"qwen-plus\"\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        print(f\"\\n\\nUsing general model {model} to format output:\\n{response.choices[0].message.content}\")\n",
        "        # Parse execution instructions\n",
        "        try:\n",
        "            instructions = json.loads(response.choices[0].message.content)\n",
        "            return self._run_instructions(instructions)\n",
        "        except json.JSONDecodeError:\n",
        "            return {\"error\": \"Instruction parsing failed\"}\n",
        "\n",
        "    def _run_instructions(self, instructions: dict) -> dict:\n",
        "        \"\"\"Actually execute tool calls\"\"\"\n",
        "        results = []\n",
        "        for step in instructions.get(\"steps\", []):\n",
        "            tool = step.get(\"tool\")\n",
        "            params = step.get(\"params\", {})\n",
        "            \n",
        "            # Execute tool call (this is an example implementation)\n",
        "            if tool == \"create_task\":\n",
        "                results.append({\n",
        "                    \"tool\": tool,\n",
        "                    \"result\": f\"Task created: {params.get('description')}\"\n",
        "                })\n",
        "            elif tool == \"schedule_event\":\n",
        "                results.append({\n",
        "                    \"tool\": tool,\n",
        "                    \"result\": f\"Event scheduled: {params.get('title')}\"\n",
        "                })\n",
        "            else:\n",
        "                results.append({\"error\": f\"Unknown tool: {tool}\"})\n",
        "        \n",
        "        return {\"status\": \"completed\", \"results\": results}\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    PlanningSystem = TaskPlanningSystem()\n",
        "    \n",
        "    # Example user request\n",
        "    user_request = \"Please schedule next week's product launch and create related preparation tasks\"\n",
        "    \n",
        "    # Planning generation phase\n",
        "    plan_stream = PlanningSystem.generate_plan(user_request)\n",
        "    generated_plan = []\n",
        "    for chunk in plan_stream:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "        generated_plan.append(chunk)\n",
        "    \n",
        "    full_plan = \"\".join(generated_plan)\n",
        "    \n",
        "    print(\"\\n\\n=== Plan Execution Phase ===\")\n",
        "    result = PlanningSystem.execute_plan(full_plan)\n",
        "    print(f\"\\nCall tool execution, execution result:\")\n",
        "    print(json.dumps(result, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf7690c",
      "metadata": {},
      "source": [
        "The above code implements the following core interaction process. The code mainly uses the qwq-32b model for streaming planning generation, then uses qwen-plus-0919 for instruction parsing, and finally decouples tool invocation through the _run_instructions method, making it easier to extend in the future:\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i4/O1CN01qJVT7z1raq4Pw20R7_!!6000000005648-2-tps-2716-1132.png\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01qJVT7z1raq4Pw20R7_!!6000000005648-2-tps-2716-1132.png\" width=\"1000\">\n",
        "</a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30451423",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "\n",
        "Through  this section, you have gained a solid understanding of prompting techniques for both general LLMs and reasoning LLMs. The flexible application of these techniques ensures a baseline level of quality in AI-generated responses. However, as AI technology continues to evolve, these methods may also change‚Äîsuch as when reasoning models no longer require chain-of-thought prompting to achieve optimal performance.What‚Äôs most important to understand is that the core purpose of prompt engineering remains the same: to clearly express your core needs and provide rich, relevant background information (context). This is essential for effectively leveraging LLMs and ensuring they deliver accurate, meaningful results.\n",
        "\n",
        "In real-world LLM applications, domain experts often play a key role in designing prompts. Therefore, hardcoding prompts directly into your code should be avoided. Instead, consider making prompts configurable or even building an entirely configurable application workflow. This allows domain experts‚Äîespecially those without technical backgrounds‚Äîto actively participate in refining prompts and shaping the overall process.\n",
        "\n",
        "Alibaba Cloud's Model Studio offers a powerful¬†application building capability, enabling users to write prompts and visually design complex application flows directly on the platform. This makes it an ideal tool for LLM projects that involve collaboration with non-technical domain experts.\n",
        "\n",
        "In the next section, you will learn about automated evaluation methods to assess the performance of Q&A bots. You‚Äôll explore how to use quantitative metrics to measure the effectiveness of your prompts and continuously improve your AI-driven applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3166b70c",
      "metadata": {},
      "source": [
        "## üî• Quiz\n",
        "\n",
        "### üîç Multiple choice question\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>Which of the following prompt elements is used to explicitly define the task that the LLM needs to complete‚ùì (Select 1.)</b>\n",
        "\n",
        "- A. Role\n",
        "- B. Audience\n",
        "- C. Objective\n",
        "- D. Context\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: C**</br>\n",
        "üìù **Explanation**:</br>\n",
        "- The Objective explicitly defines the operation or outcome that the large language model (LLM) is required to perform. Other options do not directly define the task itself.\n",
        "- Role defines the identity that the LLMs should assume, Audience specifies the target group, and Context provides background information.\n",
        "\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>You have inherited a complex, undocumented prompt designed for code generation. Before directly modifying this \"black-box\" prompt, what preparatory steps would you take to ensure your optimization efforts are effective? (Select all that apply.)</b>\n",
        "\n",
        "- A. Collect concrete examples of poor model output (\"bad cases\").\n",
        "- B. Immediately start drafting a new, simpler version from scratch.\n",
        "- C. Clearly define the desired output criteria and format in writing.\n",
        "- D. Read through the entire prompt and try to identify logical flaws based on intuition.\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: A, C**</br>\n",
        "üìù **Explanation**:</br>\n",
        "This question tests what core inputs are necessary before starting a structured optimization process. Effective optimization requires a clear definition of the \"problem\" and the \"goal.\"\n",
        "- Option A (bad cases) and Option C (desired output) are the key steps for defining the \"problem\" and the \"goal,\" respectively. They serve as the core inputs for a structured optimization approach like meta-prompting.\n",
        "- Options B (starting from scratch) and D (relying on intuition) are high-risk, inefficient methods because they do not rely on a structured, evidence-based approach.\n",
        "\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>When using a reasoning LLM (such as `qwen3-235b-a22b-thinking-2507`) for a complex reasoning task, which of the following prompting techniques are recommended‚ùì (Select all that apply.)</b>\n",
        "\n",
        "- A. Provide simple and clear task instructions.\n",
        "- B. Supplement with sufficient background information.\n",
        "- C. If there are specific user groups or task requirements, define a role or audience.\n",
        "- D. You must include phrases like \"think step by step\" or \"explain your reasoning\" in the prompt to activate the model's deep thinking mode.\n",
        "- E. When a description is too abstract or cannot be accurately described, clarify it by adding examples.\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: A, B, C, E**</br>\n",
        "üìù **Explanation**:</br>\n",
        "- Reasoning LLMs (like `qwen3-235b-a22b-thinking-2507`) are designed to engage in deep, chain-of-thought style reasoning by default, without needing an explicit instruction to do so.\n",
        "\n",
        "</div>\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b046be71",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
