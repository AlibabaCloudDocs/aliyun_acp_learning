{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92010cd5",
      "metadata": {},
      "source": [
        "# 2.3 Optimizing prompts to improve the quality of responses from Q&A bots\n",
        "\n",
        "## üöÑ Preface  \n",
        "\n",
        "The Q&A bot from the previous section can already answer questions about company rules and regulations, but this is only the first step in building an excellent user experience. Users often expect more personalized and precise interactions. This section introduces prompt engineering techniques, teaching you how to improve or control the output of the Q&A bot by optimizing prompts‚Äîfor example, adjusting tone, standardizing formats, or even enabling it to handle tasks like text summarization, inference, and transformation.\n",
        "\n",
        "## üçÅ Goals\n",
        "\n",
        "After completing this section , you will know about:\n",
        "\n",
        "* How prompt frameworks and templates work\n",
        "* Prompt techniques and their best practices\n",
        "* How to apply LLMs in engineering to handle various tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0e80fa",
      "metadata": {},
      "source": [
        "## 1. Previous content recap\n",
        "\n",
        "In the previous section, through the RAG method, the LLM has already obtained the company's private knowledge. For ease of invocation, this was encapsulated into several functions and saved in chatbot/rag.py. You can now quickly invoke it using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa6756d-47a4-4b11-8074-d992d6ce327b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:03.577636Z",
          "start_time": "2025-01-15T06:49:03.573248Z"
        }
      },
      "outputs": [],
      "source": [
        "from chatbot import rag\n",
        "import os\n",
        "from config.load_key import load_key, display_config_summary\n",
        "load_key()\n",
        "display_config_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91138451-fbc1-49ff-a268-2ece341b10bd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:11.770743Z",
          "start_time": "2025-01-15T06:49:11.611835Z"
        }
      },
      "outputs": [],
      "source": [
        "# Load index\n",
        "# The previous section has already built the index, so the index can be loaded directly here. If you need to rebuild the index, you can add a line of code: rag.indexing()\n",
        "index = rag.load_index(persist_path=\"knowledge_base/test\")\n",
        "query_engine = rag.create_query_engine(index=index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2d8d11-5a07-441c-9cd6-6ba10bf80603",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:16.961802Z",
          "start_time": "2025-01-15T06:49:16.959373Z"
        }
      },
      "outputs": [],
      "source": [
        "# Define the question-answering function\n",
        "def ask_llm(question, query_engine):\n",
        "  streaming_response = query_engine.query(question)\n",
        "  streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1715f36",
      "metadata": {},
      "source": [
        "## 2. Optimizing Prompts to Improve Response Quality \n",
        "\n",
        "In the previous section, you obtained the management tools used internally by the company through the RAG method. However, one of your colleagues wants that the Q&A bot to provide not only the tool name but also a link to the tool. You could ask all your colleagues to include a note like \"provide the download address\" in their questions, but this approach is inefficient. You decide to enhance the program by automatically supplementing user questions with additional requirements for the response content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12a310a-660c-4cd3-9d23-8819bdefb8bb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:34.992776Z",
          "start_time": "2025-01-15T06:49:29.417135Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"What tools should our company use for project management?\"\n",
        "instruction = \"If it's a tool consultation question, be sure to provide the download link.\"\n",
        "new_question = question + instruction\n",
        "ask_llm(new_question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806fadd6-673a-4542-b3e2-fe36310b30ad",
      "metadata": {},
      "source": [
        "After adjusting how prompts are provided, the Q&A bot's responses better align with your colleagues' requirements. Therefore, the choice of prompt significantly determines the quality of the LLM's responses. Next, you can refer to some prompt frameworks to construct your prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c006a5ea-5693-4805-9d2c-ccc0d2214939",
      "metadata": {},
      "source": [
        "## 3. Prompt framework\n",
        "\n",
        "### 3.1 Basic elements\n",
        "\n",
        "When communicating with an LLM, you can think of it as an entity that has been trained to understand human communication patterns. The way you interact should mirror how humans exchange information. Your requirements must be clear and unambiguous. The clearer and more precise your question (or prompt), the better the large language model will understand the core of the query, and the more likely its response will meet your expectations. Generally speaking, the following elements should be clarified in a prompt, and form a prompt framework that can help you construct a complete and effective prompt:\n",
        "\n",
        "|Element|Meaning|\n",
        "|----|----|\n",
        "|Objective|Clearly specify what task the LLM is required to complete, allowing it to focus on specific goals.|\n",
        "|Context|Background information about the task, such as operational processes or task scenarios, clarifying the scope of the discussion for the LLM.|\n",
        "|Role|The role the LLM should assume (e.g., \"You are a helpful assistant,\" \"You are a senior developer\"). This defines the model's persona, voice, and writing style.|\n",
        "|Audience|Specify the target audience for the response (e.g., \"Explain this to a 5-year-old,\"). This helps tailor the complexity and tone of the output.|\n",
        "|Sample|Provide specific input-output examples for the LLM to follow. This is also known as \"few-shot prompting\" and helps the model understand the desired pattern and format.|\n",
        "|Output Format|Specify the format of the output, output type, and range of enumerated values. You can also state content and information that should not be included in the output, which can be further clarified with examples.|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc35341a-3d7c-4881-81e3-935d42a1dcb2",
      "metadata": {},
      "source": [
        "Of course, in addition to the prompt framework discussed above, many problem analysis paradigms can be used to help you articulate clear and specific requirements. For example, SWOT analysis and 5W2H analysis offer structured approaches to thinking through your needs and expressing them more effectively. Additionally, you can also write a meta-prompt‚Äîa prompt designed to optimize or enhance another prompt‚Äîto guide the LLM in refining your initial input.\n",
        "\n",
        "By using such a meta-prompt, you can ask the model to:\n",
        "* Clarify ambiguous statements\n",
        "* Expand vague instructions into detailed guidelines\n",
        "* Add context, tone, format, and examples where necessary\n",
        "* Ensure the final prompt aligns with best practices for interacting with AI models\n",
        "\n",
        "This self-reflection approach improves the quality of your prompts, while deepening your understanding of how to communicate effectively with AI systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c618996",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_expand = \"\"\"\n",
        "    You are a Prompt Optimization and Expansion Assistant. Your primary role is to receive an arbitrary user input prompt ‚Äî which may be vague, incomplete, or lacking in detail ‚Äî and **refine and expand** it into a more structured, precise, and actionable instruction that can be clearly understood and executed by large language models.\n",
        "\n",
        "    Given the input prompt, you should generate an optimized version according to the following instructions:\n",
        "\n",
        "    Your task is to enhance the clarity of the original prompt while preserving its core intent. The resulting optimized prompt should:\n",
        "    - Be semantically richer and more descriptive.\n",
        "    - Include relevant context, constraints, or formatting instructions as needed.\n",
        "    - Specify desired tone, style, or audience (if applicable).\n",
        "    - Suggest example inputs and outputs where appropriate to guide model behavior.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Output Format:\n",
        "\n",
        "    Please respond with the following structure:\n",
        "\n",
        "    #### 1. Optimized Prompt:\n",
        "    [Here goes the enhanced and expanded version of the original prompt.]\n",
        "\n",
        "    #### 2. Example Input (if applicable):\n",
        "    [Provide one or more example(s) of what a user might input based on the prompt.]\n",
        "\n",
        "    #### 3. Example Output (based on the example input):\n",
        "    [Show what kind of output would be expected from the model when using this prompt with the given example input.]\n",
        "\n",
        "    #### 4. Optimization Notes:\n",
        "    [Briefly explain how you improved the original prompt ‚Äî e.g., added clarity, specified format, included context, etc.]\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Example:\n",
        "\n",
        "    #### Original Prompt:\n",
        "    \"Write a story about animals.\"\n",
        "\n",
        "    #### Optimized Prompt:\n",
        "    \"Compose a short narrative (approximately 300‚Äì500 words) about a group of forest animals who must work together to solve a problem. The story should include at least three distinct animal characters, a central conflict or challenge, and a resolution that highlights themes such as cooperation, courage, or environmental awareness. Use descriptive language to set the scene and convey character emotions. Aim for a tone suitable for children aged 6‚Äì10.\"\n",
        "\n",
        "    #### Example Input:\n",
        "    None required ‚Äî the model generates content autonomously.\n",
        "\n",
        "    #### Example Output:\n",
        "    Once upon a time in Greenleaf Forest, a wise old owl named Oliver noticed that the river had stopped flowing. Concerned, he called a meeting with his neighbors: Mina the mouse, Benny the bear, and Tilly the turtle. Together, they discovered a fallen tree blocking the stream. With teamwork and determination, they cleared the path and restored the water supply. From then on, the animals made sure to keep their home safe and healthy...\n",
        "\n",
        "    #### Optimization Notes:\n",
        "    - Added specific narrative elements (characters, setting, conflict, theme).  \n",
        "    - Defined word count and target audience.  \n",
        "    - Clarified tone and style expectations.  \n",
        "    - Introduced emotional and moral dimensions to enrich storytelling.\n",
        "\n",
        "    ---\n",
        "\n",
        "    Please apply this process to any user-submitted prompt and return your response using the above template.\n",
        "\n",
        "    The original prompt is:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def input_enhanced_llm_output(questions):\n",
        "    expanded_input = prompt_expand + questions\n",
        "    return ask_llm(expanded_input, query_engine)\n",
        "\n",
        "question = \"Please help me write a promotional copy for our company's new book. The general content of the book is xxx(will be given later)\"\n",
        "input_enhanced_llm_output(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1fa6d9",
      "metadata": {},
      "source": [
        "### 3.2 Prompt template\n",
        "\n",
        "In application development, it is impractical to expect end-users to write their own effective prompts from scratch. Instead, developers use the principles of prompt frameworks to create structured prompt templates. This allows developers to simply insert dynamic user inputs into predefined placeholders, ensuring that every final prompt is consistent. This consistency leads to more reliable and standardized LLM responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84e29a3-a017-47c4-907e-2ff897333d1d",
      "metadata": {},
      "source": [
        "In the RAG chatbot created using LlamaIndex, there is a default prompt template that looks like this:\n",
        "\n",
        "- The default template can be viewed using code. You can refer to [the code on the LlamaIndex official website.](https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/) The original LlamaIndex prompt template is:  \n",
        "\n",
        "\n",
        "\n",
        "```text\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "```\n",
        "\n",
        "Both¬†`context_str`¬†and¬†`query_str`¬† are placeholders that get replaced during the retrieval and querying process. The¬†`context_str`¬†is filled with relevant information retrieved from the vector database, while¬†`query_str`¬†is replaced with the user's actual question.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ff8788-4d67-47b8-a4a2-3f3a85e6b0c5",
      "metadata": {},
      "source": [
        "Since the default template is general-purpose , it may not be suitable for all scenarios. For example, you might want to adjust the chatbot's tone, add specific constraints, or define a precise output format. This customization ensures that the model‚Äôs output aligns more closely with your requirements.\n",
        " \n",
        "You can use the following sample code, where¬†`prompt_template_string`¬†represents the new prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05aea996-fdc8-4a35-922f-288313881fa3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:22.442975Z",
          "start_time": "2025-01-15T07:52:22.435217Z"
        }
      },
      "outputs": [],
      "source": [
        "# Build prompt template\n",
        "prompt_template_string = (\n",
        "    \"You are the company's customer service assistant, you need to answer users' questions concisely.\"\n",
        "    \"\\n„ÄêNotes„ÄëÔºö\\n\"\n",
        "    \"1. Answer user questions based on contextual information.\\n\"\n",
        "    \"2. Only answer the user's question, do not output other information\\n\"\n",
        "    \"The following is reference information.\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Question: {query_str}\\n.\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "# Update prompt template\n",
        "rag.update_prompt_template(query_engine, prompt_template_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa99f324",
      "metadata": {},
      "source": [
        "## 4. Techniques for writing effective prompts\n",
        "\n",
        "In Section 3.1, some key elements of prompt design were introduced. This section will explain specific prompt techniques in detail, focusing on practical scenarios and how to apply the elements of a prompt effectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b3eb55-646d-41dd-b801-152d76205bbe",
      "metadata": {},
      "source": [
        "### 4.1 Clearly express requirements and use delimiters\n",
        "\n",
        "Clearly stating your requirements ensures that the content generated by the LLM is highly relevant to the task at hand. These requirements typically include the¬†**task objective**,¬†**context**, and¬†**background information**.\n",
        "\n",
        "Using¬†**delimiters**¬†helps LLMs focus on specific goals, avoid ambiguous interpretations, and minimize unnecessary processing. Delimiters also help structure your prompt in a way that improves readability and model understanding.\n",
        "\n",
        "Here are some commonly used delimiters in English prompt engineering:\n",
        "\n",
        "| Delimiter Type      | Example                         | Use Case / Description |\n",
        "|---------------------|----------------------------------|--------------------------|\n",
        "| Triple Quotes         | `\"\"\"...\"\"\"`                      | For multi-line strings or input blocks |\n",
        "| Triple Hashes         | `### Section ###`                | To mark headings or instruction sections |\n",
        "| Double Angle Brackets | `<<example>>...<</example>>`     | To define custom tags for variables or examples |\n",
        "| XML-like Tags         | `<input>...</input>`             | For structured formatting and content separation |\n",
        "| Dashes                | `--- Section ---`                | To separate paragraphs or logical blocks |\n",
        "| Equal Signs           | `=== Output Format ===`          | Similar to headings; often used for formatting clarity |\n",
        "\n",
        "> ‚ö†Ô∏è Note: Avoid using symbols that appear frequently in your content as delimiters to prevent confusion. For example, if square brackets¬†`[]`¬†are commonly used  in your input text, they may not be suitable as delimiters.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example Usage in an English Prompt\n",
        "\n",
        "```text\n",
        "### Instruction ###\n",
        "You are a helpful assistant. Please summarize the following article.\n",
        "\n",
        "=== Article Content ===\n",
        "\"\"\" \n",
        "Artificial intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence.\n",
        "\"\"\"\n",
        "\n",
        "### Output Format ###\n",
        "Please provide a summary in no more than 50 words.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8918731d-49b6-4132-9ba5-2dce3eee360c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:52:32.192934Z",
          "start_time": "2025-01-15T07:52:24.619420Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "### Instruction ###\n",
        "Expand and polish the text enclosed in XML tags below to make the copy vivid, creative, and appealing to new employees. \n",
        "The tone should be professional yet welcoming, suitable for onboarding materials.\n",
        "\n",
        "=== Original Text ===\n",
        "<text>\n",
        "New Employee Training Camp Activity\n",
        "</text>\n",
        "\n",
        "=== Output Requirements ===\n",
        "- Enhance the content with engaging language.\n",
        "- Add context about the purpose and benefits of the training camp.\n",
        "- Include key elements such as team-building, mentorship, and orientation.\n",
        "- Keep the final output concise and motivational.\n",
        "\"\"\"\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ae4c28-a2e6-4ad3-995b-7af2d78b19d1",
      "metadata": {},
      "source": [
        "The execution results show that the prompt effectively communicates the task requirements: to expand and enhance the provided text.\n",
        "\n",
        "The core theme of the task is \"**New Employee Training Camp Activities,**\" and the intended audience is \"**new company employees.**\" In the prompt, the delimiter¬†<text>...</text>¬†is used to clearly encapsulate and isolate the original content for processing.\n",
        "\n",
        "By structuring the prompt with clear instructions, appropriate context, and standardized delimiters, the model is better guided to generate output that not only preserves the original meaning but also becomes more vivid, engaging, and tailored to the target audience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e4407d-3cff-4f00-a39b-b8ddfd7cce1d",
      "metadata": {},
      "source": [
        "### 4.2 Define Role and Audience\n",
        "\n",
        "The¬†**role**¬†is the identity the LLM should assume in a specific scenario, such as an expert, advisor, or assistant. The¬†**audience**¬†is the target user group for which the model's output is intended, such as general consumers or students. Defining these elements helps developers tailor the tone, depth, and structure of the content accordingly.\n",
        "The example below demonstrates how different system roles affect the style and content of the generated output. \n",
        "\n",
        "> Pass the prompt template into¬†`prompt_template_string`, and the user question into¬†`question`, then call the¬†`ask_llm`¬†function to execute it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e5f128-8b02-40ca-b789-d577b6e38a3a",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th width = '80px'>Role</th>\n",
        "      <th width = '500px'>LLM Algorithm Engineer</th>\n",
        "      <th width = '500px'>Elementary School Teacher</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody valign = 'top'>\n",
        "    <tr>\n",
        "      <td>Prompt Template</td>\n",
        "      <td>You are a senior large language model algorithm engineer. Please answer the user's question based on the provided information<br>---------------------<br>{context_str}<br>---------------------<br>Question: {query_str}.<br>Answer: \n",
        "</td>\n",
        "      <td>You are an elementary school teacher. Please answer the user's question based on the provided information<br>---------------------<br>{context_str}<br>---------------------<br>Question: {query_str}.<br>Answer: </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>User Question</td>\n",
        "      <td>What is Qwen-VL</td>\n",
        "      <td>What is Qwen-VL</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Output Answer</td>\n",
        "      <td>Qwen-VL is a multimodal pre-trained model introduced by Alibaba Cloud. It combines text and image processing capabilities, enabling it to understand and generate text descriptions related to images. Simply put, Qwen-VL can be regarded as an artificial intelligence assistant that can \"describe images in words.\" It not only understands the content of images but also generates corresponding textual descriptions or answers questions related to the images. This technology has broad application prospects in various fields, such as automatic image tagging, visual question-answering systems, and content-based image retrieval. For new media operations, tools like Qwen-VL can help improve content creation efficiency, enhance user experience, and provide users with richer and more diverse ways of presenting information.</td>\n",
        "      <td>Kids, today we‚Äôre going to meet a new friend named Qwen-VL!\\nQwen-VL is like a very smart robot that has \"eyes\" üëÄ and a \"brain\" üß†. It can \"see\" pictures and videos and understand their meaning, just like us!\\nFor example, if you show it a picture of a kitten playing with a ball of yarn, it can not only recognize that it‚Äôs a kitten üê± but also tell you that it‚Äôs playing with a ball of yarn üß∂. Even better, you can ask it, \"What color is the kitten?\", and it will tell you the color of the kitten.\\nIt can also understand the content in videos. For instance, if you show it a video of kids playing soccer ‚öΩÔ∏è, it can tell you who is kicking the ball, where the ball goes, and even guess what might happen next!\\nQwen-VL is like a child who is still learning. It keeps getting smarter and improving, and in the future, it will become even more intelligent, capable of doing more things and helping us better understand the world!</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f06bea-a34b-4e7a-b2b5-918af4c1ebdb",
      "metadata": {},
      "source": [
        "### 4.3 Specifying output formats\n",
        "\n",
        "Sometimes developers need LLMs to produce structured data (such as JSON or XML) that downstream systems can programmatically parse and use in the application workflow. While LLMs excel at generating free-form natural language, they can also reliably produce structured outputs such as JSON, XML, or CSV formats‚Äîas long as the desired format and requirements are clearly specified in the prompt.\n",
        "In the following example, based on the prompt template from Section 3.2, we will act as a Q&A bot to assist in reviewing the quality of company-developed documentation, and return the results in¬†**JSON format**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6692496-b6e4-4d75-aaa2-ef38b0c24f0c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:49:49.007218Z",
          "start_time": "2025-01-15T06:49:46.080617Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "[Task Requirements]\n",
        "You will be given a sentence or paragraph. Your task is to review it for any spelling or grammatical errors. \n",
        "If there are errors, you should identify them, explain the issue, and provide a corrected version. \n",
        "\n",
        "---\n",
        "[Output Requirements]\n",
        "Please output only in JSON format. Do not include markdown or code blocks.\n",
        "The JSON must contain:\n",
        "- \"label\": 0 if an error is found, 1 if no error.\n",
        "- \"reason\": A brief explanation of the error (if any).\n",
        "- \"correct\": The corrected version of the text (or the original if no error).\n",
        "\n",
        "---\n",
        "[User Input]\n",
        "The following is the user input. Please review it:\n",
        "\"\"\"\n",
        "\n",
        "question_doc = \"Delimiters are special symbols that help large langauge models (LLMs) identify which parts of the prompt should be considered a complete unit of meaning.\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe59cd3-a241-4b28-902e-7ca35966e6a7",
      "metadata": {},
      "source": [
        "As shown in the example results, the prompt¬†`question_task` specified that the output format should be JSON format, and clearly defined the content requirements. The model  generated responses in the required format. This consistent and structured output makes it easier to integrate the model into existing systems.\n",
        "\n",
        "On news websites, blog platforms, or internal knowledge-sharing platforms within enterprises, user-edited or published articles may contain typos, grammatical errors, or even sensitive information. However, traditional manual review methods are prone to oversight. In this case, LLMs can be integrated to automatically review content. If an article is flagged for serious grammatical errors or high-risk sensitive words, its priority for revision will be set to \"high.\" For articles with minor issues, the priority for revision can be set to \"low.\" This approach reduces labor costs and enhances the efficiency and accuracy of the system.\n",
        "\n",
        "Of course, there are many other applications similar to this scenario. Developers can analyze system bottlenecks or focus on data-intensive tasks to explore more use cases for LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f32b7a-9b47-40f1-82cb-e889a3b48048",
      "metadata": {},
      "source": [
        "### 4.4 Providing few-shot examples\n",
        "\n",
        "In the example from Section 4.3, the prompt specified the output format, and the LLM  generated formatted content. However, if we want the model's output  to not only be correctly formatted but also maintain consistency in style and structure, providing a few examples is a highly effective technique. This technique, known as `few-shot prompting`, is like giving the model a \"reference book\" to follow.\n",
        "\n",
        "In the following code example, let‚Äôs first observe the LLM's output without any examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcca16d368baf38",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T07:20:42.883097Z",
          "start_time": "2025-01-15T07:20:18.423060Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "### üìù Task Requirements\n",
        "You are to generate content based on the user's topic, following the **style and structure** of the example provided below. Do not add any extra text or explanations ‚Äî return only the final Markdown-formatted output.\n",
        "\n",
        "---\n",
        "### üß© Output Format Requirements\n",
        "Please ensure your response strictly follows this format:\n",
        "- Use Markdown syntax (headings, lists, bold, links, image placeholders, etc.)\n",
        "- Keep the tone informative and easy to follow\n",
        "- Structure content in a step-by-step or tutorial format\n",
        "- Avoid markdown code blocks around the final output itself\n",
        "\n",
        "---\n",
        "### üë§ User Input Topic\n",
        "The following is the user's requested topic:\n",
        "\"\"\" \n",
        "\n",
        "question_doc = \"Handmade Keychain Making Tutorial\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f61d316e005c4f",
      "metadata": {},
      "source": [
        "The model generated a tutorial for making handmade keychains, but the content was not concise enough. If you want the model to output content in a specific style and structure‚Äîsuch as focusing only on the theme, list of materials, and step-by-step instructions‚Äîyou can provide a few examples to guide it. This helps the model \"imitate\" the desired format and produce more consistent and structured results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e0f67f703e7ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:39.297041Z",
          "start_time": "2025-01-15T06:50:24.071562Z"
        }
      },
      "outputs": [],
      "source": [
        "question_task = \"\"\"\n",
        "### üìù Task Requirements\n",
        "You are to generate content based on the user's topic, following the **style and structure** of the example provided below. Do not add any extra text or explanations ‚Äî return only the final Markdown-formatted output.\n",
        "\n",
        "---\n",
        "### üß© Output Format Requirements\n",
        "Please ensure your response strictly follows this format:\n",
        "- Use Markdown syntax (headings, lists, bold, links, image placeholders, etc.)\n",
        "- Keep the tone informative and easy to follow\n",
        "- Structure content in a step-by-step or tutorial format\n",
        "- Avoid markdown code blocks around the final output itself\n",
        "\n",
        "---\n",
        "### üìö Example Content\n",
        "<example>\n",
        "### Example 1: Making a Simple Bookmark\n",
        "# Simple Bookmark Making Tutorial\n",
        "\n",
        "## Steps\n",
        "1. Choose a piece of colored cardstock.\n",
        "2. Use a pencil to draw a rectangle on the cardstock, with dimensions approximately 2 inches x 6 inches.\n",
        "3. Cut out the rectangle along the pencil lines.\n",
        "4. Personalize the bookmark by decorating it with stickers.\n",
        "5. Done! You now have a unique bookmark.\n",
        "</example>\n",
        "\n",
        "---\n",
        "### üë§ User Input Topic\n",
        "The following is the topic requested by the user:\n",
        "\"\"\"\n",
        "\n",
        "question_doc = \"Making Handmade Greeting Cards\"\n",
        "\n",
        "question = question_task + question_doc\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95447ee9772539d",
      "metadata": {},
      "source": [
        "From the above example results, it's clear that the LLM has fully adhered to the provided example and generated content with a consistent structure and style. When specifying the output format in a prompt, it is recommended to provide a few examples for the LLM to reference. This practice helps ensure that the model's output is more reliable, consistent, and aligned with your expectations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53569b47-1619-4527-a9c8-b53f5e5bdf8d",
      "metadata": {},
      "source": [
        "### 4.5 Giving the model \"thinking\" time\n",
        "\n",
        "For some complex tasks, even well-structured prompts may be insufficient for the LLM to complete the task effectively. In such cases, you can guide the model by allowing it to \"think\" step by step, encouraging it to provide intermediate reasoning before arriving at a final answer. This approach, known as the chain-of-thought (CoT) method, improves performance on tasks that require logical or mathematical reasoning.\n",
        "\n",
        "For example, consider a scenario where you ask the LLM to solve a math problem. The correct answer is $10,500. Initially, you might use a simple prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3569f469-c601-406a-a26d-fb0a74157c68",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:42.416991Z",
          "start_time": "2025-01-15T06:50:41.378053Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "[Background Information]\n",
        "An educational training institution (hereinafter referred to as the \"company\") incurred the following main expenses in the 2023 fiscal year:\n",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times during the year, each trip lasting one week. The specific expenses are as follows:\n",
        "   - Transportation and accommodation fees: average 1600 usd/trip\n",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 usd, which is expected to last for 4 years.\n",
        "\n",
        "[Problem Description]\n",
        "Based on the above background information, complete the following tasks:\n",
        "Calculate the total travel expenses for the year due to teacher business trips, including the amortized cost of teaching materials.\n",
        "\n",
        "[Output Requirements]\n",
        "Provide only the total travel expenses directly, without any other information. \"\"\"\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf9078a-f716-4537-b003-a90d70f988b5",
      "metadata": {},
      "source": [
        "As shown, the initial response is incorrect. To improve accuracy, you can use the CoT method by prompting the LLM to explain its reasoning step by step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cb1715-305c-43ee-bd5e-274d2556ad56",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:50:55.556383Z",
          "start_time": "2025-01-15T06:50:43.720893Z"
        }
      },
      "outputs": [],
      "source": [
        "question = \"\"\"An educational training institution (hereinafter referred to as the \"company\") incurred the following major expenses in the 2023 fiscal year:\n",
        "To provide classes for students in different cities, the company's teachers traveled on business trips 5 times throughout the year, with each trip lasting one week. The specific expenses are as follows:\n",
        "   - Transportation and accommodation fees: an average of 1600 usd per trip\n",
        "   - Teaching materials procurement costs: At the beginning of the year, the company purchased a batch of teaching materials for a total price of 10,000 usd, which is expected to last for 4 years.\n",
        "   \n",
        "### Problem Description\n",
        "Based on the above background information, complete the following tasks:\n",
        "Calculate the total travel expenses for the year caused by teacher business trips, including the amortized cost of teaching materials.\n",
        "\n",
        "### Output Requirements\n",
        "Please derive step by step to calculate the total travel expenses.\"\"\"\n",
        "\n",
        "ask_llm(question, query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ec9cdf-d6e1-469f-9e4c-f91fdf7f1c3e",
      "metadata": {},
      "source": [
        "After optimizing the prompt, the LLM was able to generate an accurate and detailed response. This demonstrates that prompting the model to generate a chain of thought can significantly improve the quality of reasoning-based outputs.\n",
        "\n",
        "While CoT is a powerful technique, there are other methods to enhance the model‚Äôs thinking process, such as tree of thoughts (ToT) and graph of thoughts (GOT). However, current LLMs still rely heavily on CoT for structured reasoning. As AI technology advances, models are gradually moving toward more sophisticated multi-agent systems (MAS), which can handle complex tasks more efficiently. You can explore this further in <2_6_Extend_FAQ_Bot_Capabilities_with_Plugins>."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6214d05f-8154-44f7-943b-7f04196a3275",
      "metadata": {},
      "source": [
        "## 5. Using LLMs for intent recognition\n",
        "\n",
        "After learning about prompt engineering techniques, your colleagues have realized that the Q&A bot can not only be used to retrieve information, but also assist  in reviewing documents and translating content. While the Q&A bot is generally effective at identifying issues, it still has some limitations, as shown in the following example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21013ec-b0e4-4428-9bb1-e2f309df563a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:02.890690Z",
          "start_time": "2025-01-15T06:51:00.165152Z"
        }
      },
      "outputs": [],
      "source": [
        "rag.ask('Please help me review this sentence: Does a technical content engineer need to design and develop high-quality educational materials and courses?', query_engine=query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25dd1116-048c-4599-ba9d-98750978504a",
      "metadata": {},
      "source": [
        "From the experimental results above, it's clear that the Q&A robot responded directly to the user's statement without recognizing the intended task of reviewing a sentence for errors. To understand why, let's look at what content the Q&A bot retrieved from the knowledge base before generating its response.\n",
        "\n",
        "The following excerpt  shows part of the retrieved content:\n",
        "\n",
        "```\n",
        "Core Responsibilities\n",
        "Integrating¬†educational¬†theory¬†with¬†technical¬†practice,¬†support¬†learners'¬†growth¬†and¬†development¬†through¬†high-quality¬†content¬†creation.\n",
        "```\n",
        "\n",
        "You can refer to <2_4_Automate_Evaluation_of_FAQ_Bot_Performance> to learn how to view the specific fragments retrieved  from the knowledge base.\n",
        "\n",
        "As seen from the retrieved content, most of the information relates to the responsibilities of a technical content engineer. This context likely influenced the Q&A bot's response, causing it to focus on content creation or editing rather than performing the requested document review.\n",
        "\n",
        "Since contextual information greatly impacts the responses generated by LLMs, it is not always necessary to use the RAG method for every query. You can leverage LLMs to first perform¬†intent recognitionclassifying the type of user question.\n",
        "\n",
        "For example:\n",
        "\n",
        "- If the task involves¬†document review,¬†grammar checking, or¬†content translation, the input should bypass the RAG pipeline and be handled directly by the LLM.\n",
        "- Only for¬†internal knowledge queries‚Äîsuch as \"What are the guidelines for writing API documentation?\"‚Äîshould the RAG pipeline be used to generate answers based on the knowledge base."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df6389a-cfa2-450e-b7fe-72e24dd79838",
      "metadata": {},
      "source": [
        "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01nWw7IO2AGCuuujq8G_!!6000000008175-2-tps-3329-1398.png\" width=\"1000\">  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fe0489-872c-466b-855a-3768a5127f9e",
      "metadata": {},
      "source": [
        "There are two main methods for using LLMs to perform intent recognition:\n",
        "\n",
        "- Using prompts: By designing specific prompts, you can guide the LLM to generate responses that meet your expectations. This method does not require modifying the model‚Äôs parameters; instead it relies on well-constructed inputs to activate knowledge already embedded within the model.\n",
        "- Fine-tuning the model: Based on a pre-trained foundational model, you can further train the model using specific annotated data to improve its ability to classify intents. This process involves adjusting some or all of the model‚Äôs parameters to better suit your use case.\n",
        "\n",
        "In this section, we will focus on helping the LLM perform intent recognition by crafting effective prompts. You can explore the method of fine-tuning in the subsection <2_7_Improve_Model_Accuracy_and_Efficiency_via_Fine_Tuning>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe46f635-ac13-488d-9ab7-89b49ae8878d",
      "metadata": {},
      "source": [
        "### 5.1 Intent recognition\n",
        "\n",
        "Next, we will construct prompts to enable LLMs to classify questions. Since many applications require structured output after intent recognition‚Äîsuch as for document review or integration with RAG pipelines‚Äîthe following prompting techniques will be used to ensure accurate classification of user queries:\n",
        "\n",
        "* Specify the output format: Clearly define the output format to make classification results standardized and easy to parse.\n",
        "* Few-shot examples: Include examples to help the LLM understand the characteristics and classification rules of each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ff599e-a7fa-41d8-b734-d949e9f93900",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:10.939753Z",
          "start_time": "2025-01-15T06:51:09.207243Z"
        }
      },
      "outputs": [],
      "source": [
        "from chatbot import llm\n",
        "\n",
        "# Build the prompt\n",
        "prompt = '''\n",
        "[Role Background]\n",
        "You are a question classification router, responsible for determining the type of user questions and categorizing them into one of the following three categories:\n",
        "1. Internal Company Document Query\n",
        "2. Content Translation\n",
        "3. Document Review\n",
        "\n",
        "[Task Requirements]\n",
        "Your task is to judge the intent based on the user's input content and select only the most appropriate category. Output only the category name without additional explanation. The judgment criteria are as follows:\n",
        "\n",
        "- If the question involves company policies, processes, internal tools, or job descriptions and responsibilities, choose \"Internal Company Document Query\".\n",
        "- If the input contains any foreign language or words like \"translation\", choose \"Content Translation\".\n",
        "- If the question involves checking or summarizing external documents or link content, choose \"Document Review\".\n",
        "- The user's previous inputs have no relation to the question classification; consider each dialogue independently for classification.\n",
        "\n",
        "[Few-shot Examples]\n",
        "Example 1: User input: \"What are the commonly used project management tools within our company?\"\n",
        "Category: Internal Company Document Query\n",
        "\n",
        "Example 2: User input: \"Please translate the following sentence into French: How can we finish the assignment on time?\"\n",
        "Category: Content Translation\n",
        "\n",
        "Example 3: User input: \"Please review the document at this link: https://www.alibabacloud.com/help/en/model-studio/models#daf364417cjit\"\n",
        "Category: Document Review\n",
        "\n",
        "Example 4: User input: \"Please review the following content: Does a technical content engineer need to design and develop high-quality educational materials and courses?\"\n",
        "Category: Document Review\n",
        "\n",
        "Example 5: User input: \"What are the core responsibilities of a technical content engineer?\"\n",
        "Category: Internal Company Document Query\n",
        "\n",
        "[User Input]\n",
        "The following is the user's input, please determine the classification:\n",
        "'''\n",
        "\n",
        "# Get the type of question\n",
        "def get_question_type(question):\n",
        "    return llm.invoke(prompt + question)\n",
        "\n",
        "print(get_question_type('https://www.promptingguide.ai/techniques/fewshot'), '\\n')\n",
        "print(get_question_type('Translate the following sentence into German: That is a big one I dont know why'), '\\n')\n",
        "print(get_question_type('As a technical content engineer, what should I pay attention to?'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f74506a-504d-47a2-bb9d-e17198294445",
      "metadata": {},
      "source": [
        "By providing clear output formats and few-shot examples, the Q&A bot can more accurately identify question types and produce responses in the expected format. This optimization makes classification tasks more standardized and lays the foundation for incorporating intent recognition into the Q&A bot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb001699-4451-4e1f-808d-1b38a80c9431",
      "metadata": {},
      "source": [
        "### 5.2 Applying intent recognition to Q&A bots\n",
        "\n",
        "Once the Q&A bot can recognize the intent of a user's question, it can first classify the type of query, then use tailored prompts and workflows to generate the most appropriate response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d8de14-53ee-4d5f-8f5a-0dfd9bf4e0ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:13.272321Z",
          "start_time": "2025-01-15T06:51:13.111875Z"
        }
      },
      "outputs": [],
      "source": [
        "def ask_llm_route(question):\n",
        "    question_type = get_question_type(question)\n",
        "    print(f'Question: {question}\\nType: {question_type}')\n",
        "  \n",
        "    reviewer_prompt = \"\"\"\n",
        "    [Role Background]\n",
        "    You are a document error-checking expert responsible for identifying obvious errors in documents or web content.\n",
        "    [Task Requirements]\n",
        "    - Your response should be concise.\n",
        "    - If there are no obvious issues, reply directly with 'No issues'.\\n\n",
        "    [Input as follows]\\n\"\"\"\n",
        "  \n",
        "    translator_prompt = \"\"\"\n",
        "   [Task Requirements]\n",
        "    You are a translation expert who identifies text in different languages and translates it into the target language. If not specified, the target language is French.\n",
        "    [Input as follows]\\n\"\"\"\n",
        "\n",
        "    if question_type == 'Document Review':\n",
        "        return llm.invoke(reviewer_prompt + question)\n",
        "    elif question_type == 'Internal Company Document Query':\n",
        "        return rag.ask(question, query_engine=query_engine)\n",
        "    elif question_type == 'Content Translation':\n",
        "        return llm.invoke(translator_prompt + question)\n",
        "    else:\n",
        "        return \"Unable to recognize the question type. Please re-enter.\"\n",
        "\n",
        "query_engine =rag.create_query_engine(index=rag.load_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d6ec83-8a99-45a5-a305-5088adc89be0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T06:51:22.925058Z",
          "start_time": "2025-01-15T06:51:14.191174Z"
        }
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "print(ask_llm_route('https://www.promptingguide.ai/techniques/fewshot'), '\\n')\n",
        "\n",
        "# Question 2\n",
        "print(ask_llm_route('Please help me check this document: Do technicel content engineers need to optimize and update content and collaborate across departments?'), '\\n')\n",
        "\n",
        "# Question 3\n",
        "print(ask_llm_route('Do technical content engineers need to optimize and update content and collaborate across departments?'), '\\n')\n",
        "\n",
        "# Question 4:\n",
        "print(ask_llm_route('Translate the following sentence into French: A true master always carries the heart of a student.'), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a95edd-ca52-434f-b6b5-6b0537fdfec6",
      "metadata": {},
      "source": [
        "As shown in the above experiments, the optimized Q&A bot does not always go through the RAG pipeline for every question. This approach not only saves computational resources but also prevents the knowledge base from interfering with the model's reasoning process, which could otherwise lead to lower-quality or irrelevant responses.\n",
        "\n",
        "- Resource savings: For tasks such as checking document errors, the LLM can respond without needing to retrieve reference materials. This highlights that previous implementations may have unnecessarily consumed resources by always using the RAG pipeline.\n",
        "- Avoiding misunderstandings: In earlier versions, the system retrieved reference materials for every query. These recalled text segments could sometimes confuse the LLM, affecting its ability to accurately understand the question and leading to irrelevant or off-target responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7c107a1",
      "metadata": {},
      "source": [
        "## 6. Reasoning LLMs\n",
        "\n",
        "The prompt techniques and prompt frameworks discussed earlier are widely applicable to general-purpose LLMs, such as Qwen2.5-max, GPT-4, and DeepSeek-V3. These models are designed for a broad range of tasks, including general conversation, knowledge-based Q&A, and text generation. \n",
        "\n",
        "In addition to general-purpose LLMs, there is another category of models specifically designed for reasoning‚ÄîReasoning LLMs. These models excel in tasks that require logical thinking, multi-step problem-solving, and mathematical computation.\n",
        "\n",
        "### 6.1 What Are reasoning LLMs?\n",
        "\n",
        "You may have already learned about Alibaba Cloud's reasoning LLM [QwQ](https://www.alibabacloud.com/help/en/model-studio/models#fd7d313ca5khs), through the extended reading in Section 2.1. To make it easier to use in this section, the code from Section 2.1 has been updated. Try running the following example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500b6e30",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "def reasoning_model_response(user_prompt, system_prompt=\"You are a programming assistant.\", model=\"qwq-plus\"):\n",
        "    \"\"\"\n",
        "    prompt: The prompt input by the user\n",
        "    model: Here we use qwq-32b as an example. You can replace it with other inference model names as needed, such as: deepseek-r1\n",
        "    \"\"\"\n",
        "    # Initialize client\n",
        "    client = OpenAI(\n",
        "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "        base_url=os.getenv(\"DASHSCOPE_API_BASE\")\n",
        "    )\n",
        "\n",
        "    # Initialize status variables\n",
        "    is_answering = False\n",
        "\n",
        "    # Initiate streaming request\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        # messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    # Print thinking process title\n",
        "    print(\"\\n\" + \"=\" * 20 + \"Thinking Process\" + \"=\" * 20 + \"\\n\")\n",
        "\n",
        "    # Handle streaming response\n",
        "    for chunk in completion:\n",
        "        if chunk.choices:\n",
        "            delta = chunk.choices[0].delta\n",
        "            if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:\n",
        "                # Process thinking process content\n",
        "                print(delta.reasoning_content, end='', flush=True)\n",
        "            else:\n",
        "                # Switch to answer output mode\n",
        "                if delta.content != \"\" and not is_answering:\n",
        "                    print(\"\\n\" + \"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")\n",
        "                    is_answering = True\n",
        "                # Process answer content\n",
        "                if delta.content:\n",
        "                    print(delta.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e65df7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "reasoning_model_response(user_prompt=\"Who are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fea9f93",
      "metadata": {},
      "source": [
        "As shown in the example, reasoning LLMs include an additional \"thinking process\" step, which mimics how humans approach complex problems. This allows the model to break down the problem logically before providing a final answer. This structured approach helps reduce errors and improves transparency in the model‚Äôs decision-making process.\n",
        "\n",
        "During the thinking process, if the model detects a contradiction, it can backtrack and revise its logic, making it easier for users to understand and verify the reasoning path. This feature is especially useful in scenarios requiring rigorous logic, such as mathematical problem-solving, legal analysis, or scientific reasoning.\n",
        "\n",
        "Compared with general LLMs, reasoning models are usually more reliable when solving complex problems, such as mathematical problem-solving, code writing, legal case analysis, and other scenarios that require rigorous reasoning. This does not mean that reasoning models are necessarily better; both types of models have their own application scenarios. The following table compares these two types of models across several key dimensions:\n",
        "\n",
        "| Dimension            | Reasoning Model  | General Model  |\n",
        "|-------------------|------------------|------------|\n",
        "| Design Goal     | Focuses on tasks requiring deep analysis such as **logical reasoning, multi-step problem solving, and mathematical calculations** | Targets broad scenarios such as **general conversation, knowledge-based Q&A, text generation |\n",
        "| Training Data Emphasis  | Enhanced reasoning capabilities through large datasets of **math problem solutions, code logic, scientific reasoning**, etc. | Covers massive data across multiple domains such as **encyclopedias, literature, conversations**, etc. |\n",
        "| Typical Output Characteristics | Outputs include **complete derivations**, focusing on the integrity of logical chains | Outputs are **concise and direct**, emphasizing natural language expression of results |\n",
        "| Response Speed | Complex reasoning tasks result in **slower responses** (requires multi-step calculations) | Routine tasks have **faster responses** (mainly single-step generation) |\n",
        "\n",
        "This comparison highlights that reasoning models excel in structured, analytical tasks, while general LLMs are more versatile for everyday use cases. Choosing the right model depends on the specific requirements of the task at hand. \n",
        "\n",
        "Here are some tips:\n",
        "* **Clearly defined general tasks**: For problems that are well-defined and straightforward,¬†general models¬†typically perform well.\n",
        "* **Complex tasks**: For highly complex tasks that require ¬†**more precise and reliable**¬†answers, ¬†**reasoning models** are recommended. These tasks may include:\n",
        "    * Ambiguous tasks: When there is very little task-related information available, and clear guidance cannot be provided to the model.\n",
        "    * Finding a needle in a haystack: When dealing with large amounts of unstructured data, extracting the most relevant information, or identifying connections and differences.\n",
        "    * Debugging and improving code: When the task involves reviewing and refining large volumes of code.\n",
        "* **Speed and cost**: Reasoning models generally take longer to process due to their detailed reasoning steps. If time and cost are critical factors and the task complexity is low,¬†**general models**¬†may be a more efficient choice.\n",
        "\n",
        "In addition, you can combine both types of models in your application: use the reasoning model for planning and decision-making, and the general model for executing specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580c0b84",
      "metadata": {},
      "source": [
        "### 6.2 Prompt techniques suitable for reasoning LLMs\n",
        "\n",
        "Reasoning models can provide detailed and well-structured responses even when faced with relatively ambiguous tasks. You can still ensure the baseline quality of reasoning in large language models through **prompting techniques**:<br>\n",
        "\n",
        "#### Technique One: Keep task prompts concise and clear, providing sufficient background information\n",
        "\n",
        "The¬†**clear expression of requirements**¬†introduced in Section 4.1 also applies to reasoning models. Although these models are highly capable, they cannot \"read minds.\" Therefore, it's important to keep your prompts¬†**concise and clear**, allowing the reasoning model to focus on the core task. Providing enough context ensures that the model understands the problem and can generate accurate, logical responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5cc8ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def example(a):\n",
        "  b = []\n",
        "  for i in range(len(a)):\n",
        "    b.append(a[i]*2)\n",
        "  return sum(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e6f145",
      "metadata": {},
      "source": [
        "Through the above example, you can see that even if you only provide a large inference model with a piece of code, it can still generate detailed and thoughtful responses through a series of reasoning steps. However, the generated reasoning may contain a lot of information that is not directly relevant to your needs. To get more focused and targeted results, it's important to clearly define the **task objective** and guide the model toward the specific outcome you want. Here's an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c90bd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_A = \"\"\"\n",
        "What's wrong with the following Python code? How to optimize it?\n",
        "def example(a):\n",
        "    b = []\n",
        "    for i in range(len(a)):\n",
        "        b.append(a[i]*2)\n",
        "    return sum(b)\n",
        "\"\"\"\n",
        "\n",
        "reasoning_model_response(user_prompt=prompt_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5500ab0",
      "metadata": {},
      "source": [
        "Similarly, you can further refine the scope by combining techniques from this section, such as¬†**4.2 Define roles and audience**¬†and¬†**4.3 Specifying output formats**, to ensure the results align with your expectations.Additionally, when the prompt is relatively complex, using delimiters can help the model better understand your intent and focus on the key elements of the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d81507",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_B = \"\"\"\n",
        "<audience>Beginner Python developers</audience>\n",
        "\n",
        "<task>Function performance optimization, optimize the code in the code.</task>\n",
        "\n",
        "<format>\n",
        "If there are multiple optimization solutions, please output them in the following format:\n",
        "[Optimization Solution X]\n",
        "Problem Description: [Description]\n",
        "Optimization Solution: [Description]\n",
        "Example Code: [Code Block]\n",
        "</format>\n",
        "\n",
        "<code>\n",
        "def example(a):\n",
        "  b = []\n",
        "  for i in range(len(a)):\n",
        "    b.append(a[i]*2)\n",
        "  return sum(b)\n",
        "</code>\n",
        "\"\"\"\n",
        "\n",
        "reasoning_model_response(user_prompt=prompt_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c163ae4c",
      "metadata": {},
      "source": [
        "#### Technique two: Avoid Explicit CoT prompts\n",
        "In Section 4.5, you learned how to improve response quality by using the chain-of-thought (CoT)technique with LLMs. However, for reasoning models, it is generally unnecessary to prompt them with phrases like \"think step by step\" or \"explain your reasoning.\" These models are designed to perform deep thinking naturally. Adding such instructions may actually limit their performance unless you specifically need the model to follow a strict reasoning path, which is uncommon.\n",
        "\n",
        "\n",
        "#### Technique three: Adjust prompts based on model responses\n",
        "Reasoning models are well-suited for analyzing their own thought processes, because their responses often include¬†**reasoning steps**. This makes it easier for you to refine prompts based on the model's output. You don‚Äôt need to worry about whether your initial prompt is perfect‚Äîsimply continue engaging with the model, providing additional information, and refining your prompts during the conversation.\n",
        "\n",
        "For example, if your descriptions are¬†too abstract¬†or¬†not accurately described, you can use the technique of¬†adding examples, as¬†discussed in Section 4.4, to clarify the task. These examples can often be drawn from previous interactions with the model.This iterative process can be repeated multiple times, continuously adjusting prompts and allowing the model to refine its reasoning until it meets your expectations.\n",
        "\n",
        "#### Technique four: Collaborative task completion between reasoning models and general models\n",
        "Reasoning models act like \"super-smart colleagues,\" effectively assisting you with tasks like reasoning and planning. However, for execution tasks involving repetitive actions, reasoning models may overthink, which could be inefficient.\n",
        "\n",
        "A better approach is to let reasoning models and general models work together, leveraging their respective strengths:\n",
        "* Reasoning models handle \"slow thinking\" tasks such as planning, analysis, and problem-solving.\n",
        "* General models focus on \"fast thinking\" tasks, such as executing specific actions or generating content quickly.\n",
        "\n",
        "Try running and understanding the following project, which demonstrates how these models can collaborate:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01807cca",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "from typing import Generator\n",
        "\n",
        "class TaskPlanningSystem:\n",
        "    def __init__(self):\n",
        "        # Initialize client connection\n",
        "        self.client = OpenAI(\n",
        "            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "            base_url=os.getenv(\"DASHSCOPE_API_BASE\")\n",
        "        )\n",
        "        \n",
        "        # System toolset (can be extended as needed)\n",
        "        self.tools = {\n",
        "            \"create_task\": {\n",
        "                \"desc\": \"Create a new task item\",\n",
        "                \"params\": {\"description\": \"str\", \"priority\": \"int\"}\n",
        "            },\n",
        "            \"schedule_event\": {\n",
        "                \"desc\": \"Schedule an event\", \n",
        "                \"params\": {\"title\": \"str\", \"time\": \"datetime\"}\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def generate_plan(self, user_request: str) -> Generator[str, None, None]:\n",
        "        \"\"\"Stream-generated task planning\"\"\"\n",
        "        # Build planning prompt\n",
        "        system_prompt = f\"\"\"You are a senior task planner, please convert the user's requirements into executable steps:\n",
        "        \n",
        "        Available tools (format: [[tool name]]):\n",
        "        {json.dumps(self.tools, indent=2)}\n",
        "        \n",
        "        Output requirements:\n",
        "        1. Use Markdown list format\n",
        "        2. Each step should indicate the tool name\n",
        "        3. Include necessary parameter examples\n",
        "        \"\"\"\n",
        "        model=\"qwq-plus\"\n",
        "        # Initiate streaming request\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_request}\n",
        "            ],\n",
        "            stream=True,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Print thinking process title\n",
        "        print(f\"\\n\\n=== [Planning Thinking Phase] Using reasoning model: {model}===\")\n",
        "        is_answering = False\n",
        "        # Process streaming response\n",
        "        for chunk in completion:\n",
        "            if chunk.choices:\n",
        "                delta = chunk.choices[0].delta\n",
        "                if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:\n",
        "                    # Process thinking process content\n",
        "                    reasoning_content = delta.reasoning_content\n",
        "                    yield reasoning_content\n",
        "                else:\n",
        "                    # Switch to answer output mode\n",
        "                    if delta.content != \"\" and not is_answering:\n",
        "                        print(f\"\\n\\n=== [Planning Generation Phase] Using reasoning model: {model}===\")\n",
        "                        is_answering = True\n",
        "                    # Process answer content\n",
        "                    if delta.content:\n",
        "                        content = delta.content\n",
        "                        yield content\n",
        "\n",
        "    def execute_plan(self, plan: str) -> dict:\n",
        "        \"\"\"Execute the generated task plan\"\"\"\n",
        "        # Call general model to parse the plan\n",
        "        analysis_prompt = f\"\"\"Please parse the following task plan and generate executable instructions:\n",
        "        \n",
        "        Plan content:\n",
        "        {plan}\n",
        "        \n",
        "        Output requirements:\n",
        "        - The returned content must be in JSON format only, do not include other information, do not output code blocks.\n",
        "        - Include tool names and parameters\n",
        "        - Example:\n",
        "            {{\"steps\": [\n",
        "                {{\"tool\": \"create_task\", \"params\": {{\"description\": \"...\"}}}}\n",
        "            ]}}\n",
        "        \"\"\"\n",
        "        model=\"qwen-plus\"\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        print(f\"\\n\\nUsing general model {model} to format output:\\n{response.choices[0].message.content}\")\n",
        "        # Parse execution instructions\n",
        "        try:\n",
        "            instructions = json.loads(response.choices[0].message.content)\n",
        "            return self._run_instructions(instructions)\n",
        "        except json.JSONDecodeError:\n",
        "            return {\"error\": \"Instruction parsing failed\"}\n",
        "\n",
        "    def _run_instructions(self, instructions: dict) -> dict:\n",
        "        \"\"\"Actually execute tool calls\"\"\"\n",
        "        results = []\n",
        "        for step in instructions.get(\"steps\", []):\n",
        "            tool = step.get(\"tool\")\n",
        "            params = step.get(\"params\", {})\n",
        "            \n",
        "            # Execute tool call (this is an example implementation)\n",
        "            if tool == \"create_task\":\n",
        "                results.append({\n",
        "                    \"tool\": tool,\n",
        "                    \"result\": f\"Task created: {params.get('description')}\"\n",
        "                })\n",
        "            elif tool == \"schedule_event\":\n",
        "                results.append({\n",
        "                    \"tool\": tool,\n",
        "                    \"result\": f\"Event scheduled: {params.get('title')}\"\n",
        "                })\n",
        "            else:\n",
        "                results.append({\"error\": f\"Unknown tool: {tool}\"})\n",
        "        \n",
        "        return {\"status\": \"completed\", \"results\": results}\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    PlanningSystem = TaskPlanningSystem()\n",
        "    \n",
        "    # Example user request\n",
        "    user_request = \"Please schedule next week's product launch and create related preparation tasks\"\n",
        "    \n",
        "    # Planning generation phase\n",
        "    plan_stream = PlanningSystem.generate_plan(user_request)\n",
        "    generated_plan = []\n",
        "    for chunk in plan_stream:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "        generated_plan.append(chunk)\n",
        "    \n",
        "    full_plan = \"\".join(generated_plan)\n",
        "    \n",
        "    print(\"\\n\\n=== Plan Execution Phase ===\")\n",
        "    result = PlanningSystem.execute_plan(full_plan)\n",
        "    print(f\"\\nCall tool execution, execution result:\")\n",
        "    print(json.dumps(result, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf7690c",
      "metadata": {},
      "source": [
        "The above code implements the following core interaction process. The code mainly uses the qwq-32b model for streaming planning generation, then uses qwen-plus-0919 for instruction parsing, and finally decouples tool invocation through the _run_instructions method, making it easier to extend in the future:\n",
        "\n",
        "<a href=\"https://img.alicdn.com/imgextra/i4/O1CN01qJVT7z1raq4Pw20R7_!!6000000005648-2-tps-2716-1132.png\" target=\"_blank\">\n",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01qJVT7z1raq4Pw20R7_!!6000000005648-2-tps-2716-1132.png\" width=\"1000\">\n",
        "</a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30451423",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "\n",
        "Through  this section, you have gained a solid understanding of prompting techniques for both general LLMs and reasoning LLMs. The flexible application of these techniques ensures a baseline level of quality in AI-generated responses. However, as AI technology continues to evolve, these methods may also change‚Äîsuch as when reasoning models no longer require chain-of-thought prompting to achieve optimal performance.What‚Äôs most important to understand is that the core purpose of prompt engineering remains the same: to clearly express your core needs and provide rich, relevant background information (context). This is essential for effectively leveraging LLMs and ensuring they deliver accurate, meaningful results.\n",
        "\n",
        "In real-world LLM applications, domain experts often play a key role in designing prompts. Therefore, hardcoding prompts directly into your code should be avoided. Instead, consider making prompts configurable or even building an entirely configurable application workflow. This allows domain experts‚Äîespecially those without technical backgrounds‚Äîto actively participate in refining prompts and shaping the overall process.\n",
        "\n",
        "Alibaba Cloud's Model Studio offers a powerful¬†application building capability, enabling users to write prompts and visually design complex application flows directly on the platform. This makes it an ideal tool for LLM projects that involve collaboration with non-technical domain experts.\n",
        "\n",
        "In the next section, you will learn about automated evaluation methods to assess the performance of Q&A bots. You‚Äôll explore how to use quantitative metrics to measure the effectiveness of your prompts and continuously improve your AI-driven applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3166b70c",
      "metadata": {},
      "source": [
        "## üî• Quiz\n",
        "\n",
        "### üîç Multiple choice question\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>Which of the following prompt elements is used to explicitly define the task that the LLM needs to complete‚ùì (Select 1.)</b>\n",
        "\n",
        "- A. Role\n",
        "- B. Audience\n",
        "- C. Objective\n",
        "- D. Context\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: C**</br>\n",
        "üìù **Explanation**:</br>\n",
        "- The Objective explicitly defines the operation or outcome that the large language model (LLM) is required to perform. Other options do not directly define the task itself.\n",
        "- Role defines the identity that the LLMs should assume, Audience specifies the target group, and Context provides background information.\n",
        "\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>You have inherited a complex, undocumented prompt designed for code generation. Before directly modifying this \"black-box\" prompt, what preparatory steps would you take to ensure your optimization efforts are effective? (Select all that apply.)</b>\n",
        "\n",
        "- A. Collect concrete examples of poor model output (\"bad cases\").\n",
        "- B. Immediately start drafting a new, simpler version from scratch.\n",
        "- C. Clearly define the desired output criteria and format in writing.\n",
        "- D. Read through the entire prompt and try to identify logical flaws based on intuition.\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: A, C**</br>\n",
        "üìù **Explanation**:</br>\n",
        "This question tests what core inputs are necessary before starting a structured optimization process. Effective optimization requires a clear definition of the \"problem\" and the \"goal.\"\n",
        "- Option A (bad cases) and Option C (desired output) are the key steps for defining the \"problem\" and the \"goal,\" respectively. They serve as the core inputs for a structured optimization approach like meta-prompting.\n",
        "- Options B (starting from scratch) and D (relying on intuition) are high-risk, inefficient methods because they do not rely on a structured, evidence-based approach.\n",
        "\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>When using a reasoning LLM (such as `qwen3-235b-a22b-thinking-2507`) for a complex reasoning task, which of the following prompting techniques are recommended‚ùì (Select all that apply.)</b>\n",
        "\n",
        "- A. Provide simple and clear task instructions.\n",
        "- B. Supplement with sufficient background information.\n",
        "- C. If there are specific user groups or task requirements, define a role or audience.\n",
        "- D. You must include phrases like \"think step by step\" or \"explain your reasoning\" in the prompt to activate the model's deep thinking mode.\n",
        "- E. When a description is too abstract or cannot be accurately described, clarify it by adding examples.\n",
        "\n",
        "**[Click to view answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: A, B, C, E**</br>\n",
        "üìù **Explanation**:</br>\n",
        "- Reasoning LLMs (like `qwen3-235b-a22b-thinking-2507`) are designed to engage in deep, chain-of-thought style reasoning by default, without needing an explicit instruction to do so.\n",
        "\n",
        "</div>\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b046be71",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
