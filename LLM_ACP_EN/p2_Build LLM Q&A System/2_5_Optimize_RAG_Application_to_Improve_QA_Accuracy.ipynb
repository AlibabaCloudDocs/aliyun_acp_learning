{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92010cd5",
      "metadata": {},
      "source": [
        "# 2.5 Optimizing RAG applications to improve question-answer accuracy\n",
        "\n",
        "## üöÑ Preface  \n",
        "\n",
        "In the previous lessons, you have already identified some issues with the RAG chatbot through automated evaluations. However, optimizing prompts alone cannot fix problems caused by **inaccurate retrieval**, just as it would be difficult to provide the correct answer during an open-book exam if you were using the wrong reference material.\n",
        "\n",
        "In this section, you will gain a deeper understanding of the **RAG workflow** and work on improving the **accuracy of your RAG application‚Äôs question-answering**. This involves refining both the **retrieval** and **generation** phases to ensure that the model not only finds relevant information but also uses it effectively to produce accurate and reliable responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a39a58",
      "metadata": {},
      "source": [
        "## üçÅ Goals\n",
        "\n",
        "After completing this course, you will be able to:\n",
        "\n",
        "* Gain a deeper understanding of the implementation principles and technical details of RAG\n",
        "* Understand common issues with RAG applications and recommended solutions\n",
        "* Improve the performance of RAG applications through hands-on case studies\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0e80fa",
      "metadata": {},
      "source": [
        "## 1. Previous content recap\n",
        "\n",
        "In the previous chapter, you discovered that the Q&A bot was unable to adequately answer the question: \"Which department is Michael Johnson from?\" You can reproduce the issue using the following code:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f319db8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the required dependency packages\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
        "from config.load_key import load_key\n",
        "import logging\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex, PromptTemplate\n",
        "from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.core.node_parser import (\n",
        "    SentenceSplitter,\n",
        "    SemanticSplitterNodeParser,\n",
        "    SentenceWindowNodeParser,\n",
        "    MarkdownNodeParser,\n",
        "    TokenTextSplitter\n",
        ")\n",
        "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
        "from langchain_community.llms.tongyi import Tongyi\n",
        "from langchain_community.embeddings import DashScopeEmbeddings\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import context_recall, context_precision, answer_correctness\n",
        "from chatbot import rag\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6525c224",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set log level\n",
        "logging.basicConfig(level=logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1410463",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load API key\n",
        "load_key()\n",
        "# Do not print the API Key to logs in production environment to avoid leakage\n",
        "print(f'Your configured API Key is: {os.environ[\"DASHSCOPE_API_KEY\"][:5]+\"*\"*5}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8651814",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the Qwen LLM and text vector model\n",
        "Settings.llm = OpenAILike(\n",
        "    model=\"qwen-plus\",\n",
        "    api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "    is_chat_model=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9d7c39",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Configure text vector model, set batch size and maximum input length\n",
        "Settings.embed_model = DashScopeEmbedding(\n",
        "    model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,\n",
        "    embed_batch_size=6,\n",
        "    embed_input_length=8192\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b970212a",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Define the question-answering function\n",
        "def ask(question, query_engine):\n",
        "    # Update the prompt template\n",
        "    rag.update_prompt_template(query_engine=query_engine)\n",
        "\n",
        "    # Output the question\n",
        "    print('=' * 50)  # Generate a dividing line using multiplication\n",
        "    print(f'ü§î Question: {question}')\n",
        "    print('=' * 50 + '\\n')  # Generate a dividing line using multiplication\n",
        "\n",
        "    # Get the answer\n",
        "    response = query_engine.query(question)\n",
        "\n",
        "    # Output the answer\n",
        "    print('ü§ñ Answer:')\n",
        "    if hasattr(response, 'print_response_stream') and callable(response.print_response_stream):\n",
        "        response.print_response_stream()\n",
        "    else:\n",
        "        print(str(response))\n",
        "\n",
        "    # Output reference documents\n",
        "    print('\\n' + '-' * 50)  # Generate a dividing line using multiplication\n",
        "    print('üìö Reference Documents:\\n')\n",
        "    for i, source_node in enumerate(response.source_nodes, start=1):\n",
        "        print(f'Document {i}:')\n",
        "        print(source_node)\n",
        "        print()\n",
        "\n",
        "    print('-' * 50)  # Generate a dividing line using multiplication\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e130e9dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = rag.create_query_engine(rag.load_index())\n",
        "response = ask('Which department is Michael Johnson in?', query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073b9153",
      "metadata": {},
      "source": [
        "You will find that the reason for this issue is that the correct reference information (document chunks) was not recalled during the retrieval phase. To improve this issue, you can apply a few simple strategies to preliminarily optimize the retrieval effect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1715f36",
      "metadata": {},
      "source": [
        "## 2. Initial optimization\n",
        "\n",
        "As mentioned in the introduction, you need to ensure that the LLM has access to the correct \"reference materials\" to provide accurate \"answers.\" Therefore, you can try increasing the number of \"reference materials\" retrieved or organizing the \"knowledge points\" in the reference materials into structured tables. You can start with the former:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3fd908d",
      "metadata": {},
      "source": [
        "### 2.1 Allowing LLMs to access more reference information\n",
        "\n",
        "Since the knowledge base contains information about Michael Johnson's employment history, you can expand the search scope and increase the probability of finding relevant information by recalling more document chunks. In the previous code, only 2 document chunks were retrieved. Now, you can increase the number of recalled chunks to 5 and observe whether the retrieval performance improves."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1cb6fc5",
      "metadata": {},
      "source": [
        "#### 2.1.1 Adjusting the code\n",
        "\n",
        "You can configure the following settings to allow the retrieval engine to recall the top 5 most relevant document chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba86a10",
      "metadata": {},
      "outputs": [],
      "source": [
        "index = rag.load_index()\n",
        "query_engine = index.as_query_engine(\n",
        "    streaming=True,\n",
        "    # Retrieve 5 document chunks at once, default is 2\n",
        "    similarity_top_k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624c4b9a",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "response = ask('Which department is Michael Johnson in?', query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1fa6d9",
      "metadata": {},
      "source": [
        "As you can see, after adjusting the number of recalls, your Q&A bot is now able to answer the question \"*Which department is Michael Johnson in?*\" This is because the recalled document chunks already contain information about Michael Johnson and his department.\n",
        "\n",
        "However, simply increasing the number of recalled chunks is not a good solution. Think about it‚Äîif this method could solve the problem, why not recall the entire knowledge base? That way, no information would be missed. But this would not only exceed the LLM's input length limit, but also reduce the efficiency and accuracy of the model's responses due to excessive irrelevant information.\n",
        "\n",
        "In fact, there may be many colleagues named Michael Johnson in your company, which leads to another issue: when a user asks \"**Which department is Michael Johnson in?**\" , the system cannot determine which Michael Johnson the user is referring to. Simply increasing the number of recalls might retrieve information about multiple Michael Johnsons, but the system would still be unable to accurately decide which one's information to return. Therefore, we need to use other methods to further improve the RAG chatbot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d852191e",
      "metadata": {},
      "source": [
        "#### 2.1.2 Evaluate improvement effectiveness\n",
        "\n",
        "To quantify the effectiveness of improvements in subsequent enhancements, you can continue to use Ragas from the previous chapter for evaluation. Suppose your company has three colleagues named Michael Johnson, who respectively work in the Teaching and Research, Course Development, and IT Department."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e36894d",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Define evaluation function\n",
        "def evaluate_result(question, response, ground_truth):\n",
        "    # Get the response content\n",
        "    if hasattr(response, 'response_txt'):\n",
        "        answer = response.response_txt\n",
        "    else:\n",
        "        answer = str(response)\n",
        "    # Get the retrieved context\n",
        "    context = [source_node.get_content() for source_node in response.source_nodes]\n",
        "\n",
        "    # Construct evaluation dataset\n",
        "    data_samples = {\n",
        "        'question': [question],\n",
        "        'answer': [answer],\n",
        "        'ground_truth': [ground_truth],\n",
        "        'contexts': [context],\n",
        "    }\n",
        "    dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "    # Evaluate using Ragas\n",
        "    score = evaluate(\n",
        "        dataset=dataset,\n",
        "        metrics=[answer_correctness, context_recall, context_precision],\n",
        "        llm=Tongyi(model_name=\"qwen-plus-0919\"),\n",
        "        embeddings=DashScopeEmbeddings(model=\"text-embedding-v3\")\n",
        "    )\n",
        "    return score.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87734c7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'Which department is Michael Johnson in?'\n",
        "ground_truth = '''There are three employees named Michael Johnson in the company:\n",
        "- Michael Johnson in the Teaching and Research Department: Position is Teaching and Research Specialist, email zhangwei@educompany.com.\n",
        "- Michael Johnson in the Course Development Department: Position is Course Development Specialist, email zhangwei01@educompany.com.\n",
        "- Michael Johnson in the IT Department: Position is IT Specialist, email zhangwei036@educompany.com.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "182d8bea",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_result(question=question, response=response, ground_truth=ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa99f324",
      "metadata": {},
      "source": [
        "As you can see, the current RAG system is still unable to operate efficiently. The retrieved document chunks contain irrelevant information, and the relevant information has not been fully recalled, resulting in an incorrect final answer. You must consider other improvement strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8e8a0f",
      "metadata": {},
      "source": [
        "### 2.2 Provide more structured reference information\n",
        "\n",
        "In practical applications, the organizational structure of a document significantly impacts retrieval performance. Imagine this: the same information is placed either in a well-structured table or scattered throughout a block of plain text. Which one would be easier to locate and understand? Clearly, the former.\n",
        "\n",
        "The same applies to LLMs. When information originally presented in a table is converted into plain text, although no information is lost, its structure is diminished. This is akin to turning an organized drawer into a pile of scattered items‚Äîwhile everything is still there, it becomes less convenient to find things."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4019ffe",
      "metadata": {},
      "source": [
        "#### 2.2.1 Rebuild the Index\n",
        "\n",
        "Markdown format is a great choice because it:\n",
        "* Has a clear structure and well-defined hierarchy\n",
        "* Simple syntax, making it easy to read and maintain\n",
        "* Is particularly suitable for organizing documents in RAG chatbot scenarios\n",
        "\n",
        "To validate the effectiveness of structured documents,  an optimized Markdown format file has been prepared for you. Next, you will:\n",
        "\n",
        "1. Add this Markdown file to the docs directory\n",
        "2. Rebuild the index\n",
        "3. Test the improvement in retrieval performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f6e94c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy the markdown formatted employee information document to the ./docs directory\n",
        "! mkdir -p ./docs/2_5\n",
        "! cp ./resources/2_4/Employee\\ Key\\ Contact\\ Information.md ./docs/2_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee5bbce",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('=' * 50)\n",
        "print('üìÇ Loading documents...')\n",
        "print('=' * 50 + '\\n')\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader('./docs/2_5').load_data()\n",
        "print(f'‚úÖ Document loading completed.\\n')\n",
        "\n",
        "print('=' * 50)\n",
        "print('üõ†Ô∏è Rebuilding index...')\n",
        "print('=' * 50 + '\\n')\n",
        "\n",
        "# Rebuild index\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents\n",
        ")\n",
        "print('‚úÖ Index rebuilding completed!')\n",
        "\n",
        "print('=' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613d3642",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    streaming=True,\n",
        "    similarity_top_k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47495a4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = ask('Which department is Michael Johnson in?', query_engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72f807bc",
      "metadata": {},
      "source": [
        "#### 2.2.2 Evaluate improvement effect\n",
        "\n",
        "You can see that your Q&A bot can accurately answer this question. You can run the Ragas evaluation again, and the evaluation data will also show that the answer accuracy has improved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64aa328",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_result(question=question, response=response, ground_truth=ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "247c76f9",
      "metadata": {},
      "source": [
        "## 3. Familiarize yourself with the RAG workflow\n",
        "\n",
        "So far, you have made some improvements to increase the accuracy of the Q&A for the RAG chatbot. However, in a real production environment, the problems you may encounter go far beyond this. Previously, you have already learned about some of the RAG workflow. Here, you can review the important steps to help you identify new areas for improvement:\n",
        "\n",
        "RAG is a technology that combines information retrieval and generative models, allowing it to leverage relevant information from an external knowledge base when generating answers. Its workflow can be divided into several key steps: parsing and chunking, vector storage, retrieval recall, and answer generation. You can refer back to the section \"Expanding the Knowledge Scope of the RAG chatbot\" for specific concepts.\n",
        "\n",
        "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01zk9HW723iQ11MXgEJ_!!6000000007289-2-tps-5205-2710.png\" alt=\"RAG Working Principle\" width=\"1000px\">\n",
        "\n",
        "Next, we will focus on each step of RAG, and optimize its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d2f244",
      "metadata": {},
      "source": [
        "## 4. Stages of RAG chatbot and improvement strategies\n",
        "\n",
        "### 4.1 Document preparation stage\n",
        "\n",
        "In traditional customer service systems, customer service personnel accumulate a knowledge base based on the questions raised by users, and share it with other staff for reference. This process is equally essential when building a RAG chatbot.\n",
        "\n",
        "* **Intent space**: We can map the needs behind user questions as points, which together form a user intent space.\n",
        "* **Knowledge space**: The knowledge points accumulated in the knowledge base documents constitute a knowledge space. These knowledge points can be a paragraph or a chapter.\n",
        "\n",
        "When we project the intent space and knowledge space together, we find that there are overlaps and differences between the two spaces. These areas correspond to our three subsequent optimization strategies:\n",
        "\n",
        "1. **Overlapping area**:\n",
        "    * This refers to parts where user questions can be answered based on the content of the knowledge base, forming the foundation of ensuring the effectiveness of the RAG chatbot.\n",
        "    * For these user intents, you can continuously improve the quality of responses through¬†optimizing content quality, engineering, and algorithms.\n",
        "2. **Uncovered intent space**:\n",
        "    * Due to the lack of supporting content in the knowledge base, LLMs tend to generate hallucinations. For example, if the company has added a new \"Data Analysis Department,\" but there are no related documents in the knowledge base, no matter how much you improve the engineering algorithms, the RAG chatbot will not be able to accurately answer this question.\n",
        "    * What you need to do is proactively¬†supplement the missing knowledge,¬†and continually track changes in the user intent space.\n",
        "3. **Unused knowledge space**:\n",
        "    * Recalling irrelevant information may interfere with the LLM's responses.\n",
        "    * Therefore, you need to¬†optimize the recall algorithm¬†to avoid recalling unrelated content. Additionally, you should periodically check the knowledge base and¬†remove irrelevant content.\n",
        "\n",
        "<img src=\"https://img.alicdn.com/imgextra/i2/O1CN01Icn1Bt1tDbCqYa94M_!!6000000005868-2-tps-2122-1176.png\" alt=\"RAG Intent Space to Knowledge Space\" width=\"1000px\">\n",
        "\n",
        "Before attempting to optimize engineering or algorithms, you should prioritize building a mechanism that continuously collects user intents. By systematically gathering real user needs to enrich the knowledge base content and involving domain experts with deep understanding of user intents in the evaluation process, a closed-loop optimization process of \"data collection - knowledge update - expert validation\" is formed to ensure the effectiveness of the RAG chatbot.\n",
        "\n",
        "Once you have prepared these, you can further optimize various stages of the RAG chatbot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d1a481",
      "metadata": {},
      "source": [
        "### 4.2 Document parsing and chunking phase\n",
        "\n",
        "The RAG application first parses the content of your document,  then divides the document content into chunks.\n",
        "\n",
        "If the document chunks that the LLM receives when answering questions lack key information, the response may be inaccurate. Similarly, if the  chunks contain too much irrelevant information (noise), it will also affect the quality of the response. In other words, either too little or too much information can impact the model's ability to generate effective responses.\n",
        "\n",
        "Therefore, during the document parsing and chunking phase, it is essential to ensure that the final chunks contain complete and relevant information without including excessive interfering content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "186f3fb2",
      "metadata": {},
      "source": [
        "#### 4.2.1 Problem classification and improvement strategies\n",
        "\n",
        "During the document parsing and chunking phase, you may encounter the following issues:\n",
        "\n",
        "<table border=\"1\">\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Category</th>\n",
        "      <th>Subtype</th>\n",
        "      <th>Improvement Strategy</th>\n",
        "      <th>Scenario Example</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td rowspan=\"3\">Document Parsing</td>\n",
        "      <td>Non-uniform document types. Some formats are not supported for parsing;¬†for example, SimpleDirectoryLoader used earlier does not support Keynote files.</em></td>\n",
        "      <td>Develop a parser for the corresponding format or convert the document format.</td>\n",
        "      <td>A company uses a large number of Keynote files to store employee information, but the existing parser does not support the Keynote format. A Keynote parser can be developed, or the files can be converted into a supported format (such as  PDF).</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Within the already supported document formats, there is some special content,¬†such as embedded tables, images, and videos.</em></td>\n",
        "      <td>Improve the document parser.</td>\n",
        "      <td>A document contains many tables and images, and the current parser cannot correctly extract information from the tables. The parser can be improved to handle tables and images.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td rowspan=\"4\">Document Chunking</td>\n",
        "      <td>The document contains much content with similar themes.¬†For example, in a work manual, each stage, including requirements analysis, development, and release, has precautions and operational guidance.</em></td>\n",
        "      <td>Expand document titles and subtitles:¬†\"Precautions\" => \"Requirements Analysis > Precautions\"; create document metadata (tagging).</td>\n",
        "      <td>A document contains precautions for multiple stages. When a user asks, \"What are the precautions for requirements analysis?\" the system returns precautions for all stages. Titles can be expanded and tagging can be used to distinguish content across different stages.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document chunks are too long, introducing excessive noise.</td>\n",
        "      <td>Reduce chunk length, or develop more suitable chunking strategies based on specific business needs.</td>\n",
        "      <td>A document's chunks are too long and contain multiple unrelated topics, resulting in irrelevant information being returned during retrieval. Chunk length can be reduced to ensure that each chunk contains only one topic.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document chunks are too short, truncating useful information.</td>\n",
        "      <td>Increase chunk length, or develop more suitable chunking strategies based on specific business needs.</td>\n",
        "      <td>Each chunk in a document contains only one sentence, making it impossible to retrieve complete context during search. Chunk length can be increased to ensure that each chunk contains complete context.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34175364",
      "metadata": {},
      "source": [
        "#### 4.2.2 Parsing PDF files using Model Studio\n",
        "\n",
        "In the previous sections of this course, we provided a Markdown document converted from a PDF so that you could quickly see the effects of format conversion. However, in real-world work scenarios, writing code to properly convert PDFs into Markdown is not an easy task.\n",
        "\n",
        "In practical work, you can also use DashScopeParse provided by Model Studio to parse files in formats such as PDF and Word. Behind DashScopeParse lies Alibaba Cloud's¬†[Document Intelligence](https://www.aliyun.com/product/ai/docmind) service, which uses image recognition  to recognize images within documents, and optical character recognition (OCR) to extract structured text information from files in formats like PDF and Word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d53df69",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.readers.dashscope.utils import ResultType\n",
        "from llama_index.readers.dashscope.base import DashScopeParse\n",
        "import os\n",
        "import json\n",
        "import nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bce4c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "nest_asyncio.apply()\n",
        "# Use environment variables\n",
        "os.environ['DASHSCOPE_API_KEY'] = os.getenv('DASHSCOPE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69e566a",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Create a silent logger to replace the original logger\n",
        "silent_logger = logging.getLogger(__name__)\n",
        "# Set the log level to ERROR to avoid outputting irrelevant information. If you need to view more detailed log information, set it to INFO\n",
        "silent_logger.setLevel(logging.ERROR)\n",
        "\n",
        "class SilentDashScopeParse(DashScopeParse):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Replace the logger in all related modules\n",
        "        import llama_index.readers.dashscope.base as base_module\n",
        "        import llama_index.readers.dashscope.domain.lease_domains as lease_domains_module\n",
        "        import llama_index.readers.dashscope.utils as utils_module\n",
        "\n",
        "        base_module.logger = silent_logger\n",
        "        lease_domains_module.logger = silent_logger\n",
        "        utils_module.logger = silent_logger\n",
        "\n",
        "        # Call the parent class initialization\n",
        "        super().__init__(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779386c7",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# The file is parsed into markdown text that is easy for programs and large models to process via the DashScopeParse interface.\n",
        "def file_to_md(file, category_id):\n",
        "    parse = SilentDashScopeParse(\n",
        "        result_type=ResultType.DASHSCOPE_DOCMIND,\n",
        "        category_id=category_id\n",
        "    )\n",
        "    documents = parse.load_data(file_path=file)\n",
        "    # Initialize an empty string to store Markdown content\n",
        "    markdown_content = \"\"\n",
        "    for doc in documents:\n",
        "        doc_json = json.loads(json.loads(doc.text))\n",
        "        for item in doc_json[\"layouts\"]:\n",
        "            if item[\"text\"] in item[\"markdownContent\"]:\n",
        "                markdown_content += item[\"markdownContent\"]\n",
        "            else:\n",
        "                # When DashScopeParse processes, it will also parse the text information inside document images into the initial markdown text (similar to OCR). This is sufficient for command-line screenshots and text screenshots in the example files of this article. No deep parsing of images is required in this example.\n",
        "                # For actual knowledge base documents, if they involve irregular, complex information in images and require a deeper understanding of the image content, you can call a vision model to further understand the meaning of the image.\n",
        "                # (In the data structure returned by DashScopeParse, for image data, the markdownContent field is the image URL, and the text field is the parsed text.)\n",
        "                # if \".jpg\" in item[\"markdownContent\"] or \".jpeg\" in item[\"markdownContent\"] or \".png\" in item[\"markdownContent\"]:\n",
        "                #     image_url = re.findall(r'\\!\\[.*?\\]\\((https?://.*?)\\)', item[\"markdownContent\"])[0]\n",
        "                #     print(image_url)\n",
        "                #     markdown_content = markdown_content + parse_image_to_text(image_url)+\"\\n\"\n",
        "                # else:\n",
        "                #     markdown_content = markdown_content + item[\"text\"]+\"\\n\"\n",
        "                markdown_content = markdown_content + item[\"text\"]+\"\\n\"\n",
        "    return markdown_content\n",
        "\n",
        "### Example usage\n",
        "\n",
        "# 1. Optional configuration.\n",
        "# On the Bailian platform, different business spaces can be configured for different projects. By default, the default business space is used.\n",
        "# If you need to use a non-default space, go to [Bailian Console - Business Space Management](https://bailian.console.aliyun.com/?admin=1#/efm/business_management), configure the business space, and obtain the Workspace ID.\n",
        "# After completion, uncomment and modify this code to the actual value:\n",
        "# os.environ['DASHSCOPE_WORKSPACE_ID'] = \"<Your Workspace id, Default workspace is empty.>\"\n",
        "\n",
        "# 2. Optional configuration.\n",
        "# When files are parsed through DashScopeParse, the uploaded data directory ID needs to be configured. Go to [Bailian Console - Data Management](https://bailian.console.aliyun.com/#/data-center), configure categories, and obtain the ID.\n",
        "category_id=\"default\" # It is recommended to modify this to a custom category ID for better file classification management.\n",
        "\n",
        "md_content = file_to_md(['./docs/Employee Key Contact Information.pdf'], category_id)\n",
        "print(\"Parsed Markdown text:\")\n",
        "print(\"-\"*100)\n",
        "print(md_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6da7bb",
      "metadata": {},
      "source": [
        "Due to the diversity of sources for various file formats such as PDF and docx, there may be some minor formatting issues during the process of parsing files into markdown. For example, table rows spanning pages in a PDF might be parsed into multiple lines.\n",
        "\n",
        "LLMs can be used to refine the generated markdown text, correcting issues such as table of contents levels and missing information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "553aa8a8",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "from dashscope import Generation\n",
        "\n",
        "def md_polisher(data):\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': \n",
        "            '''The following text is converted from PDF to markdown, and there may be some issues with the format and content. I need you to optimize it:\n",
        "                1. Directory levels: If the directory level order is incorrect, please complete or modify it in markdown format;\n",
        "                2. Content errors: If there are inconsistencies in the context, please correct them;\n",
        "                3. Tables: Pay attention to inconsistencies between rows;\n",
        "                4. The overall output should not differ significantly from the input; do not create content on your own‚ÄîI need to polish the original text;\n",
        "                5. Output format requirement: Markdown text, all your responses should be placed inside a markdown file.\n",
        "                Special Note: Only output the converted markdown content itself, without any other information.\n",
        "                The content to be processed is: \n",
        "            ''' + data\n",
        "        }]\n",
        "    response = Generation.call(\n",
        "        model=\"qwen-plus-0919\",\n",
        "        messages=messages,\n",
        "        result_format='message',\n",
        "        stream=True,\n",
        "        incremental_output=True\n",
        "    )\n",
        "    result = \"\"\n",
        "    print(\"Polished Markdown Text:\")\n",
        "    print(\"-\"*100)\n",
        "    for chunk in response:\n",
        "        print(chunk.output.choices[0].message.content, end='')\n",
        "        result += chunk.output.choices[0].message.content\n",
        "\n",
        "    return(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98a199d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "md_polisher(md_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab3e6628",
      "metadata": {},
      "source": [
        "Through the above steps, you have successfully converted the PDF into markdown and made some formatting corrections. If there are images in the document, the information in the images can also be extracted to build a knowledge base that is more conducive to search performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "750af73a",
      "metadata": {},
      "source": [
        "#### 4.2.3 Using multiple document chunking methods\n",
        "\n",
        "During the document chunking process, the chunking method can affect the effectiveness of retrieval recall. Let's understand the characteristics of different chunking methods through specific examples. First, create a general evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a58c12b",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def evaluate_splitter(splitter, documents, question, ground_truth, splitter_name):\n",
        "    \"\"\"Evaluate the effectiveness of different document splitting methods\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üîç Testing with {splitter_name} method...\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    # Build index\n",
        "    print(\"üìë Processing documents...\")\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "    index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)\n",
        "\n",
        "    # Create query engine\n",
        "    query_engine = index.as_query_engine(\n",
        "        similarity_top_k=5,\n",
        "        streaming=True\n",
        "    )\n",
        "\n",
        "    # Execute query\n",
        "    print(f\"\\n‚ùì Test question: {question}\")\n",
        "    print(\"\\nü§ñ Model response:\")\n",
        "    response = query_engine.query(question)\n",
        "    response.print_response_stream()\n",
        "\n",
        "    # Output reference snippets\n",
        "    print(f\"\\nüìö Reference snippets recalled by {splitter_name}:\")\n",
        "    for i, node in enumerate(response.source_nodes, 1):\n",
        "        print(f\"\\nDocument snippet {i}:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(node)\n",
        "\n",
        "    # Evaluate results\n",
        "    print(f\"\\nüìä Evaluation results for {splitter_name}:\")\n",
        "    print(\"-\" * 40)\n",
        "    display(evaluate_result(question, response, ground_truth))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07b76ca6",
      "metadata": {},
      "source": [
        "Next, let's look at the characteristics and examples of various Chunking methods:\n",
        "\n",
        "#### 4.2.3.1 Token Chunking\n",
        "\n",
        "Suitable for scenarios with strict requirements on the number of tokens, such as when using models with smaller context lengths.\n",
        "\n",
        "Example text: \"LlamaIndex is a powerful RAG framework. It provides various document processing methods. Users can choose the appropriate method based on their needs.\"\n",
        "\n",
        "Possible results after applying token chunking (chunk_size=10):\n",
        "\n",
        "* Chunk 1: [\"LlamaIndex\", \"is\", \"a\", \"powerful\", \"RAG\", \"framework.\", \"It\", \"provides\", \"various\", \"document\"]\n",
        "* Chunk 2: [\"processing\", \"methods.\", \"Users\", \"can\", \"choose\", \"the\", \"appropriate\", \"method\", \"based\", \"on\"]\n",
        "* Chunk 3: [\"their\", \"needs.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f4183a",
      "metadata": {},
      "outputs": [],
      "source": [
        "token_splitter = TokenTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "evaluate_splitter(token_splitter, documents, question, ground_truth, \"Token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1955ebb7",
      "metadata": {},
      "source": [
        "#### 4.2.3.2 Sentence Chunking\n",
        "\n",
        "This is the default chunking strategy, which preserves the integrity of sentences.\n",
        "\n",
        "The same text after sentence chunking:\n",
        "\n",
        "* Chunk 1: \"LlamaIndex is a powerful RAG framework.\"\n",
        "* Chunk 2: \"It provides various document processing methods.\"\n",
        "* Chunk 3: \"Users can choose the appropriate method based on their needs.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167f07d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_splitter = SentenceSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "evaluate_splitter(sentence_splitter, documents, question, ground_truth, \"Sentence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb4052e",
      "metadata": {},
      "source": [
        "#### 4.2.3.3 Sentence window Retrieval\n",
        "\n",
        "Each chunk includes surrounding sentences as the context window.\n",
        "\n",
        "Example text after using sentence window retrieval (window_size=1):\n",
        "\n",
        "* Chunk 1: \"LlamaIndex is a powerful RAG framework.\" Context: \"It provides various document processing methods.\"\n",
        "* Chunk 2: \"It provides various document processing methods.\" Context: \"LlamaIndex is a powerful RAG framework. Users can choose the appropriate method based on their needs.\"\n",
        "* Chunk 3: \"Users can choose the appropriate method based on their needs.\" Context: \"It provides various document processing methods.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0394bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_window_splitter = SentenceWindowNodeParser.from_defaults(\n",
        "    window_size=3,\n",
        "    window_metadata_key=\"window\",\n",
        "    original_text_metadata_key=\"original_text\"\n",
        ")\n",
        "# Note: Sentence window retrieval requires a special post-processor\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=5,\n",
        "    streaming=True,\n",
        "    node_postprocessors=[MetadataReplacementPostProcessor(target_metadata_key=\"window\")]\n",
        ")\n",
        "evaluate_splitter(sentence_window_splitter, documents, question, ground_truth, \"Sentence Window\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e6c776",
      "metadata": {},
      "source": [
        "#### 4.2.3.4 Semantic chunking\n",
        "\n",
        "Adaptively select chunk points based on semantic relevance.\n",
        "\n",
        "Example text: \"LlamaIndex is a powerful RAG framework. It provides various document processing methods. Users can choose the appropriate method according to their needs. Additionally, it supports vector-based retrieval. This retrieval method is highly efficient.\"\n",
        "\n",
        "Possible results of semantic chunking:\n",
        "\n",
        "* Chunk 1: \"LlamaIndex is a powerful RAG framework. It provides various document processing methods. Users can choose the appropriate method according to their needs.\"\n",
        "* Chunk 2: \"Additionally, it supports vector-based retrieval. This retrieval method is highly efficient.\" (Note that this is grouped by semantic relevance.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e1bb7a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_splitter = SemanticSplitterNodeParser(\n",
        "    buffer_size=1,\n",
        "    breakpoint_percentile_threshold=95,\n",
        "    embed_model=Settings.embed_model\n",
        ")\n",
        "evaluate_splitter(semantic_splitter, documents, question, ground_truth, \"Semantic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67722c68",
      "metadata": {},
      "source": [
        "#### 4.2.3.5 Markdown Chunking\n",
        "\n",
        "A chunking method specifically optimized for Markdown documents.\n",
        "\n",
        "Example Markdown text:\n",
        "\n",
        "\n",
        "\n",
        "```markdown\n",
        "# RAG Framework\n",
        "LlamaIndex is a powerful RAG framework.\n",
        "\n",
        "## Features\n",
        "- Provides multiple document processing methods\n",
        "- Supports vector retrieval\n",
        "- Easy and convenient to use\n",
        "\n",
        "### Detailed Description\n",
        "Users can choose the appropriate method based on their needs.\n",
        "```\n",
        "\n",
        "Markdown chunks will be intelligently divided based on heading levels:\n",
        "\n",
        "* Chunk 1: \"# RAG Framework\n",
        "LlamaIndex is a powerful RAG framework.\"\n",
        "* Chunk 2: \"## Features\n",
        "- Provides various document processing methods\n",
        "- Supports vector retrieval\n",
        "- Simple and convenient to use\"\n",
        "* Chunk 3: \"### Detailed Description\n",
        "Users can choose the appropriate method according to their needs.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e28ee065",
      "metadata": {},
      "outputs": [],
      "source": [
        "markdown_splitter = MarkdownNodeParser()\n",
        "evaluate_splitter(markdown_splitter, documents, question, ground_truth, \"Markdown\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0927b881",
      "metadata": {},
      "source": [
        "In practical applications, there's no need to overthink when choosing a chunking method. You can consider it this way:\n",
        "\n",
        "* If you are new to RAG, it is recommended to start with the default sentence chunking method, which provides good results in most scenarios.\n",
        "* When you find that the retrieval results are not ideal, you can try the following:\n",
        "    * Are you handling long documents, and need to maintain context? Try sentence window retrieval.\n",
        "    * Is the document logical and highly specialized? Semantic chunking may be helpful.\n",
        "    * Is the model always reporting token limits exceeded? Token chunking can help you control precisely.\n",
        "    * Processing Markdown documents? Don‚Äôt forget there‚Äôs dedicated Markdown chunking.\n",
        "\n",
        "There is no single best chunking method‚Äîonly the one that is most suitable for your specific scenario. Experiment with different chunking methods, observe Ragas evaluation results, and find the solution that best fits your needs. The learning process is all about constant trial and adjustment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e35625a",
      "metadata": {},
      "source": [
        "### 4.3 Vectorization and storage phase for chunks\n",
        "\n",
        "After document chunking, you also need to index the chunks for later retrieval. A common approach is to use a text embedding model to vectorize the chunks and store them in a vector database.\n",
        "\n",
        "In this phase, selecting an appropriate word embedding model and vector database is crucial for improving retrieval performance.\n",
        "\n",
        "#### 4.3.1 Understanding word embedding and vectorization\n",
        "\n",
        "The text embedding model can convert text into high-dimensional vectors to represent textual semantics. Similar texts are mapped to  vectors that are close to each other, allowing documents with high similarity to be identified based on the vector representation of a query.\n",
        "\n",
        "_A directed line segment in a plane coordinate system is a 2-dimensional vector. For example, the directed line segment from the origin (0, 0) to point A (xa, ya) can be called vector A. The smaller the angle between vector A and vector B, the higher their similarity._\n",
        "\n",
        "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01wKAL7C1bhDgbxr2Aa_!!6000000003496-0-tps-1556-1382.jpg\" width=\"400\" ></td>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c566ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Cosine similarity\"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Example vectors\n",
        "a = np.array([0.2, 0.8])\n",
        "b = np.array([0.3, 0.7])\n",
        "c = np.array([0.8, 0.2])\n",
        "\n",
        "print(f\"Cosine similarity between A and B: {cosine_similarity(a, b)}\")\n",
        "print(f\"Cosine similarity between B and C: {cosine_similarity(b, c)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39489df7",
      "metadata": {},
      "source": [
        "#### 4.3.2 Selecting the appropriate embedding model\n",
        "\n",
        "Different embedding models may produce completely different vectors when calculating the same set of text. Generally, newer embedding models perform better. For example, in the previous section, we used the text-embedding-v2 provided by Alibaba Cloud's Bailian platform. If you switch to a newer version,¬†[text-embedding-v3](https://help.aliyun.com/zh/model-studio/user-guide/embedding), you will notice that even without performing the earlier optimizations,  retrieval performance will still improve to some extent.\n",
        "\n",
        "For example, running the following code shows that different versions of the embedding model yield varying similarity scores for the question \"Which department is Michael Johnson from?\" and different document chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc66320",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_embeddings(query, chunks, embedding_models):\n",
        "    \"\"\"Compare text similarity across different embedding models\n",
        "\n",
        "    Args:\n",
        "        query: Query text\n",
        "        chunks: List of text chunks to compare\n",
        "        embedding_models: Dictionary of embedding models, format {model_name: model_instance}\n",
        "    \"\"\"\n",
        "    # Print input texts\n",
        "    print(f\"Query: {query}\")\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        print(f\"Text {i}: {chunk}\")\n",
        "\n",
        "    # Calculate and display similarity results for each model\n",
        "    for model_name, model in embedding_models.items():\n",
        "        print(f\"\\n{'='*20} {model_name} {'='*20}\")\n",
        "        query_embedding = (model.get_query_embedding(query) if hasattr(model, 'get_query_embedding')\n",
        "                         else model.get_text_embedding(query))\n",
        "\n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            chunk_embedding = model.get_text_embedding(chunk)\n",
        "            similarity = cosine_similarity(query_embedding, chunk_embedding)\n",
        "            print(f\"Similarity between query and text {i}: {similarity:.4f}\")\n",
        "\n",
        "# Prepare test data\n",
        "query = \"Which department is Michael Johnson in?\"\n",
        "chunks = [\n",
        "    # Chunk 1: QA Specialist\n",
        "    \"Course Development Department. Michael Johnson, EID-205, works as a QA Specialist at office location 456 Tech Hub #205. His responsibilities include system validation testing.\",\n",
        "    \n",
        "    # Chunk 2: Technical Writer\n",
        "    \"Course Development Department. Michael Johnson, EID-209, serves as a Technical Writer at office location 456 Tech Hub #209. He is responsible for documentation creation.\"\n",
        "]\n",
        "\n",
        "# Define embedding models to be tested\n",
        "embedding_models = {\n",
        "    \"text-embedding-v2\": DashScopeEmbedding(model_name=\"text-embedding-v2\"),\n",
        "    \"text-embedding-v3\": DashScopeEmbedding(model_name=\"text-embedding-v3\")\n",
        "}\n",
        "\n",
        "# Perform comparison\n",
        "compare_embeddings(query, chunks, embedding_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4b3f761",
      "metadata": {},
      "source": [
        "In addition to evaluating the performance of different embedding models through similarity comparisons, you can also assess them from a practical application perspective. Below, you will use the Ragas evaluation tool to compare the actual performance of the text-embedding-v2 and text-embedding-v3 models within a RAG chatbot.\n",
        "\n",
        "By running the following code, you can clearly see that, under the same RAG chatbot strategy, the overall performance of the text-embedding-v3 model is better than that of text-embedding-v2. Let's take a look at the specific evaluation process and results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c4dcef",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_embedding_models(documents, question, ground_truth, sentence_splitter):\n",
        "    \"\"\"Compare the performance of different embedding models in RAG\n",
        "\n",
        "    Args:\n",
        "        documents: List of documents\n",
        "        question: Query question\n",
        "        ground_truth: Standard answer\n",
        "        sentence_splitter: Text splitter\n",
        "    \"\"\"\n",
        "    # Document splitting\n",
        "    print(\"üìë Processing documents...\")\n",
        "    nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    # Define the embedding model configurations to be tested\n",
        "    embedding_models = {\n",
        "        \"text-embedding-v2\": DashScopeEmbedding(\n",
        "            model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2\n",
        "        ),\n",
        "        \"text-embedding-v3\": DashScopeEmbedding(\n",
        "            model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,\n",
        "            embed_batch_size=6,\n",
        "            embed_input_length=8192\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Test each model\n",
        "    for model_name, embed_model in embedding_models.items():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"üîç Testing {model_name}...\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Build index and query engine\n",
        "        index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
        "        query_engine = index.as_query_engine(streaming=True, similarity_top_k=5)\n",
        "\n",
        "        # Execute query\n",
        "        print(f\"\\n‚ùì Test question: {question}\")\n",
        "        print(\"\\nü§ñ Model response:\")\n",
        "        response = query_engine.query(question)\n",
        "        response.print_response_stream()\n",
        "\n",
        "        # Display recalled document fragments\n",
        "        print(f\"\\nüìö Recalled reference fragments:\")\n",
        "        for i, node in enumerate(response.source_nodes, 1):\n",
        "            print(f\"\\nDocument fragment {i}:\")\n",
        "            print(\"-\" * 40)\n",
        "            print(node)\n",
        "\n",
        "        # Evaluate results\n",
        "        print(f\"\\nüìä Evaluation results for {model_name}:\")\n",
        "        print(\"-\" * 40)\n",
        "        evaluation_score = evaluate_result(question, response, ground_truth)\n",
        "        display(evaluation_score)\n",
        "\n",
        "# Prepare test data\n",
        "documents = SimpleDirectoryReader('./docs/2_5').load_data()\n",
        "sentence_splitter = SentenceSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "# Perform comparison\n",
        "compare_embedding_models(\n",
        "    documents=documents,\n",
        "    question=question,\n",
        "    ground_truth=ground_truth,\n",
        "    sentence_splitter=sentence_splitter\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2a90e3",
      "metadata": {},
      "source": [
        "You can see that:\n",
        "* Newer versions of embedding models generally yield better results (text-embedding-v3 performs better than v2).\n",
        "* In practice, simply upgrading the embedding model can significantly improve retrieval quality.\n",
        "* We recommend you first try the latest text-embedding-v3 model, which delivers good performance across most tasks. Meanwhile, you can keep an eye on updates to DashScope embedding models, and choose to upgrade to a higher-performing version if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135529fc",
      "metadata": {},
      "source": [
        "#### 4.3.3 Choosing the right vector database\n",
        "\n",
        "When building a RAG chatbot, you have multiple vector storage options to choose from, ranging from simple to complex:\n",
        "\n",
        "##### 4.3.3.1 In-memory vector storage\n",
        "\n",
        "The simplest approach is to use the vector database built into LlamaIndex. Simply install the llama-index package, and with no additional configuration, you can quickly develop and test your RAG chatbot:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce24dd06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "# Create in-memory vector index\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "208c0bd2",
      "metadata": {},
      "source": [
        "The advantage is that it is quick to get started, making it suitable for development and testing; the disadvantages are that data cannot be persisted, and it is limited by memory size.\n",
        "\n",
        "##### 4.3.3.2 Local vector database\n",
        "\n",
        "When the data volume increases, open-source vector databases such as Milvus and Qdrant can be used. These databases provide data persistence and efficient retrieval capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf210cc",
      "metadata": {},
      "source": [
        "The advantage is that the functionality is complete and highly controllable; the disadvantage is that it requires self-hosting and maintenance.\n",
        "\n",
        "##### 4.3.3.3 Cloud service vector storage\n",
        "\n",
        "For production environments, it is recommended to use vector storage capabilities provided by cloud services. Alibaba Cloud offers multiple options:\n",
        "\n",
        "* **Vector Retrieval Service (DashVector)**: Pay-as-you-go, automatic scaling, suitable for quickly starting projects. For detailed functionalities, please refer to¬†[Vector Retrieval Service (DashVector)](https://www.aliyun.com/product/ai/dashvector).\n",
        "* **Vector Retrieval Service Milvus Edition**: Compatible with open-source Milvus, making it convenient to migrate existing applications. For detailed functionalities, please refer to¬†[Vector Retrieval Service Milvus Edition](https://www.aliyun.com/product/milvus).\n",
        "* **Vector Capabilities of Existing Databases**: If you are already using Alibaba Cloud databases (such as RDS and PolarDB), you can  utilize their vector functionalities.\n",
        "\n",
        "The advantages of cloud services include:\n",
        "\n",
        "* With automatic scaling, there's no need to worry about operations and maintenance.\n",
        "* Comprehensive monitoring and management tools are provided.\n",
        "* Pay-as-you-go, with better cost control.\n",
        "* Support for hybrid retrieval of vectors + scalars, improving retrieval accuracy.\n",
        "\n",
        "Recommendations:\n",
        "1. Use in-memory vector storage during development and testing.\n",
        "2. For small-scale applications, you can use local vector databases.\n",
        "3. For production environments, it is recommended to use cloud services, and choose the appropriate service type based on your needs.\n",
        "\n",
        "```python\n",
        "import dashvector\n",
        "\n",
        "# Create Client and get collection\n",
        "dashvector_client = dashvector.Client(api_key='YOUR_API_KEY', endpoint='YOUR_CLUSTER_ENDPOINT')\n",
        "collection = dashvector_client.get('quickstart')\n",
        "\n",
        "# Similar vector query\n",
        "collection.query(\n",
        "    vector=[0.1, 0.2, 0.3, 0.4]\n",
        ")\n",
        "\n",
        "# Query using filter conditions\n",
        "collection.query(\n",
        "    vector=[0.1, 0.2, 0.3, 0.4],\n",
        "    topk=100,\n",
        "    filter='age>18',  # Filter condition, only perform similarity search on Docs with age > 18\n",
        "    output_fields=['name', 'age'],  # Only return the name and age fields\n",
        "    include_vector=True\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a6b314e",
      "metadata": {},
      "source": [
        "### 4.4 Retrieval recall phase\n",
        "\n",
        "The main issue encountered during the retrieval phase is the difficulty of identifying, from a large number of chunks, the fragment that is most relevant to the user's question and contains the correct answer.\n",
        "\n",
        "From the perspective of intervention timing, solutions can be divided into two main categories:\n",
        "\n",
        "1. Before executing the retrieval, many user queries are incomplete or even ambiguous. It is necessary to find ways to reconstruct the user's true intent to improve retrieval effectiveness.\n",
        "2. After executing the retrieval, you may discover some irrelevant information that needs to be filtered out to avoid interfering with the subsequent answer generation.\n",
        "\n",
        "<table border=\"1\">\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Timing</th>\n",
        "      <th>Improvement Strategy</th>\n",
        "      <th>Example</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td rowspan=\"7\">Before Retrieval</td>\n",
        "      <td>Question Rewriting</td>\n",
        "      <td>\"Are there any good restaurants nearby?\" => \"Please recommend a few highly-rated restaurants near me.\"</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Question Expansion <em>Adding more information to make the search results more comprehensive</em></td>\n",
        "      <td>\"Which department does Michael Johnson belong to?\" => \"Which department does Michael Johnson belong to? What are his contact details, responsibilities, and work objectives?\"</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Context Expansion Based on User Profile <em>Expanding the question based on user information and behavior data</em></td>\n",
        "      <td>Content Engineer asks \"Work Precautions\" => \"What precautions should a content engineer take at work?\" Project Manager asks \"Work Precautions\" => \"What precautions should a project manager take at work?\"</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Tag Extraction <em>Tag Extraction:¬†Extract tags for subsequent tag filtering + vector similarity search.</em></td>\n",
        "      <td>\"What precautions should a content engineer take at work?\" => <ul><li>Tag Filtering: {\"Position\": \"Content Engineer\"}</li><li>Vector Search: \"What precautions should a content engineer take at work?\"</li></ul></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Ask the User</td>\n",
        "      <td>\"What are the job responsibilities?\" => LLM asks back: \"May I ask which position‚Äôs job responsibilities you want to know about?\" <em>Prompt examples for asking back can be found here:</em><a href=\"https://help.aliyun.com/zh/model-studio/use-cases/create-an-ai-shopping-assistant\">Build an AI Shopping Assistant in 10 Minutes</a></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Think and Plan Multiple Searches</td>\n",
        "      <td>\"Michael Johnson is not available, who can I contact?\" => LLM thinks and plans: => task_1: What are Michael Johnson's responsibilities, task_2: Who else has ${task_1_result} responsibilities => Execute multiple searches in sequence.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td rowspan=\"3\">After Retrieval</td>\n",
        "      <td>reranking + Filtering <em>Most vector databases consider efficiency and sacrifice some accuracy; the retrieved chunks may contain items with low relevance.</em></td>\n",
        "      <td>chunk1, chunk2..., chunk10 => chunk 2, chunk4, chunk5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Sliding Window Retrieval <em>After retrieving a chunk, supplement it with several adjacent chunks before and after. This is because adjacent chunks often have semantic connections, and looking at a single chunk might lose important information.</em> <em>Sliding window retrieval ensures that semantic connections between texts are not lost due to excessive segmentation.</em></td>\n",
        "      <td>A common implementation is sentence sliding windows. You can understand it using the simplified form below: Assume the original text is ABCDEFG (each letter represents a sentence). When the retrieved chunk is D, after supplementing adjacent chunks, it becomes BCDEF (taking 2 chunks before and after). Here, BC and EF are the context of D. For example:<ul><li>BC may contain background information explaining D</li><li>EF may contain subsequent developments or results of D</li><li>These contextual pieces of information help you understand the full meaning of D more accurately</li></ul>By recalling these related context chunks, you can improve the accuracy and completeness of the retrieval results.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ee79c9",
      "metadata": {},
      "source": [
        "#### 4.4.1 Question rewriting\n",
        "\n",
        "ü§î **Why is question rewriting necessary?**\n",
        "\n",
        "Imagine you are searching for keywords like \"Find Michael Johnson\" or \"Michael Johnson Department.\" It seems simple, but for a RAG system, such scattered search terms can be challenging to process. This is because, in real-world scenarios, there may be multiple employees named Michael Johnson, and the keywords entered by users are often too simplistic, lacking necessary contextual information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71bed53",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"Find Michael Johnson\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6914f3f9",
      "metadata": {},
      "source": [
        "‚ú® **What can problem rewriting bring?**\n",
        "\n",
        "Problem rewriting is like helping the system better understand user intent. For example, when you ask \"Find Michael Johnson,\" the system can rewrite the question into a more complete form, such as \"Please tell me all employees named Michael Johnson in the company and their departments.\" Such rewriting improves the accuracy of retrieval, and also makes the answers more comprehensive.\n",
        "\n",
        "Next, you can try different problem rewriting strategies through practical examples. In this case, you will use the following configuration:\n",
        "\n",
        "* Document: Markdown format\n",
        "* Chunking: Default sentence chunking strategy\n",
        "* Model: text-embedding-v3\n",
        "* Storage: Default vector storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720fc61c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure embedding model\n",
        "Settings.embed_model = DashScopeEmbedding(\n",
        "    model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,\n",
        "    embed_batch_size=6,\n",
        "    embed_input_length=8192\n",
        ")\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader('./docs/2_5').load_data()\n",
        "\n",
        "# Configure document splitter\n",
        "sentence_splitter = SentenceSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "# Document splitting\n",
        "sentence_nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "# Build index\n",
        "sentence_index = VectorStoreIndex(sentence_nodes, embed_model=Settings.embed_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0809604",
      "metadata": {},
      "source": [
        "**Conventional Method: Direct Retrieval without Rewriting the Question**\n",
        "\n",
        "Before  attempting to rewrite the question, take a look at the results of using the original question for retrieval. This comparison will give you a more intuitive sense of the improvements that question rewriting can bring:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448c3cc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create query engine\n",
        "query_engine = sentence_index.as_query_engine(\n",
        "    streaming=True,\n",
        "    similarity_top_k=5\n",
        ")\n",
        "\n",
        "# Execute query\n",
        "print(f\"‚ùì User question: {question}\\n\")\n",
        "streaming_response = query_engine.query(question)\n",
        "\n",
        "print(\"\\nüí≠ AI Response:\")\n",
        "print(\"-\" * 40)\n",
        "streaming_response.print_response_stream()\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display reference documents\n",
        "print(\"\\nüìö Reference Sources:\")\n",
        "print(\"-\" * 40)\n",
        "for i, node in enumerate(streaming_response.source_nodes, 1):\n",
        "    print(f\"\\nDocument snippet {i}:\")\n",
        "    print(f\"Relevance score: {node.score:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(node.text)\n",
        "\n",
        "# Evaluate results\n",
        "print(\"\\nüìä Response Quality Evaluation:\")\n",
        "print(\"-\" * 40)\n",
        "evaluation_score = evaluate_result(question, streaming_response, ground_truth)\n",
        "display(evaluation_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc446e7",
      "metadata": {},
      "source": [
        "After running this code, you may find the results less than ideal. Although the system retrieved five relevant snippets, it did not find all the information about \"Michael Johnson.\" Why is that?\n",
        "\n",
        "The issue lies in the way the question was asked. When a user asks, \"Find Michael Johnson,\" this question is easy for a person to understand but lacks important context for an LLM‚Äîthere are multiple Michael Johnsons in the company! This is similar to walking into a company with several employees named John Smith and asking, \"Where is John Smith's desk?\" Someone is likely to respond, \"Which John Smith do you mean?\"\n",
        "\n",
        "So, what if we made the question more complete? For example, by clearly stating that you want to know the department information of \"all employees named Michael Johnson in the company.\" Next, you can try using an LLM to rephrase the question and see if the results improve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612e77b9",
      "metadata": {},
      "source": [
        "**Method 1: Using LLMs to Expand User Questions**\n",
        "\n",
        "You can ask the LLM to act as a question-rewriting assistant. It will help you rewrite simple questions to make them more complete and clear. For example, it will not only consider the possibility of multiple individuals named Michael Johnson, but also supplement all related contextual information. Here are the details of how to do it :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63552231",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_gen_str = \"\"\"\n",
        "System role setting:\n",
        "You are a professional question rewriting assistant. Your task is to expand the user's original question into a more complete and comprehensive question.\n",
        "\n",
        "Rules:\n",
        "1. Integrate possible ambiguities, related concepts, and contextual information into a complete question\n",
        "2. Use parentheses to supplement explanations for ambiguous concepts\n",
        "3. Add key qualifiers and modifiers\n",
        "4. Ensure that the rewritten question is clear and semantically complete\n",
        "5. For vague concepts, list the main possibilities in parentheses\n",
        "6. About 15 words or less in length\n",
        "\n",
        "Original question:\n",
        "{query}\n",
        "\n",
        "Please generate a comprehensive rewritten question, ensuring:\n",
        "- Contains the core intent of the original question\n",
        "- Covers possible interpretations of ambiguities\n",
        "- Uses clear logical connectives to link different aspects\n",
        "- When necessary, use parentheses to provide supplementary explanations\n",
        "\n",
        "Output format:\n",
        "[Comprehensive rewrite] - The rewritten question\n",
        "\"\"\"\n",
        "query_gen_prompt = PromptTemplate(query_gen_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470dd06f",
      "metadata": {
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def generate_queries(query: str):\n",
        "    response = Settings.llm.predict(\n",
        "        query_gen_prompt, query=query\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2a0a37",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate extended queries\n",
        "print(\"\\nüîç Original question:\")\n",
        "print(f\"   {question}\")\n",
        "query = generate_queries(question)\n",
        "print(\"\\nüìù Extended queries:\")\n",
        "print(f\"   {query}\\n\")\n",
        "\n",
        "# Create query engine\n",
        "query_engine = sentence_index.as_query_engine(\n",
        "    streaming=True,\n",
        "    similarity_top_k=10\n",
        ")\n",
        "# Execute query\n",
        "response = query_engine.query(query)\n",
        "\n",
        "print(\"üí≠ AI Response:\")\n",
        "print(\"-\" * 40)\n",
        "response.print_response_stream()\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display reference documents\n",
        "print(\"\\nüìö Reference sources:\")\n",
        "print(\"-\" * 40)\n",
        "for i, node in enumerate(response.source_nodes, 1):\n",
        "    print(f\"\\nDocument snippet {i}:\")\n",
        "    print(f\"Relevance score: {node.score:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(node.text)\n",
        "\n",
        "# Evaluate results\n",
        "print(\"\\nüìä Response quality evaluation:\")\n",
        "print(\"-\" * 40)\n",
        "evaluation_score = evaluate_result(query, response, ground_truth)\n",
        "display(evaluation_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffca5379",
      "metadata": {},
      "source": [
        "After running the code above, you will find that questions rewritten by LLMs can achieve better retrieval results. However, for some complex questions, rewriting alone may not be sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc61fb48",
      "metadata": {},
      "source": [
        "**Method 2: Rewriting a single query into multi-Step queries**\n",
        "\n",
        "In addition to rewriting a query for clarity, you can also break down complex questions into simpler, sequential steps. This approach is particularly useful when dealing with ambiguous or multi-faceted queries‚Äîsuch as identifying information about individuals with common names like \"Michael Johnson.\" By breaking down the original question, you enable the system to retrieve more comprehensive and accurate results.\n",
        "\n",
        "LlamaIndex offers two powerful tools that support this approach:\n",
        "\n",
        "1. `StepDecomposeQueryTransform`: This tool helps break down a complex question into multiple sub-questions. It leverages an LLM to analyze the intent behind the query, and generate a series of logical follow-up questions that lead to the final answer. For the question *\"Which department does Michael Johnson belong to?\"*, the tool might break it down into:\n",
        "    - **Step 1:** *\"How many employees named Michael Johnson are there in the company?\"*\n",
        "    - **Step 2:** *\"Which departments do these Michael Johnsons belong to?\"*\n",
        "\n",
        "By doing so, the system avoids missing relevant information due to ambiguity and ensures all possible matches are considered.\n",
        "\n",
        "2. `MultiStepQueryEngine`: This query engine processes each of the sub-questions generated by `StepDecomposeQueryTransform` in sequence. After retrieving answers to each step, it combines the results into a single, coherent response.\n",
        "    - First, it retrieves all employees named Michael Johnson.\n",
        "    - Then, it queries the department information for each one.\n",
        "    - Finally, it compiles a response such as (just for illustrative purposes):\n",
        "        > *\"There are three Michael Johnsons in the company, working in the Teaching and Research Department, Course Development Department, and IT Department, respectively.\"*\n",
        "\n",
        "This step-by-step processing ensures that no piece of relevant information is overlooked and allows the system to handle complex queries more effectively.\n",
        "\n",
        "**Benefits of multi-step querying**\n",
        "\n",
        "- **Improved Accuracy:** Breaking down queries leads to more thorough retrieval and reduces the risk of missing key details.\n",
        "- **Better Handling of Ambiguity:** Especially useful for queries involving common names or unclear references.\n",
        "- **Logical Flow:** Mimics how humans solve complex problems ‚Äî by dividing them into smaller, manageable parts.\n",
        "\n",
        "**Considerations**\n",
        "\n",
        "- **Increased LLM Usage:** Since each step may involve calling an LLM, this method consumes more tokens than a single-query approach.\n",
        "- **Longer Processing Time:** Due to multiple sequential calls, the overall response time may be longer.\n",
        "\n",
        "Nonetheless, for complex or ambiguous queries, the trade-off is often worth it in terms of result quality and completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3168f666",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    StepDecomposeQueryTransform,\n",
        ")\n",
        "step_decompose_transform = StepDecomposeQueryTransform(verbose=True)\n",
        "# set Logging to DEBUG for more detailed outputs\n",
        "from llama_index.core.query_engine import MultiStepQueryEngine\n",
        "query_engine = sentence_index.as_query_engine(streaming=True,similarity_top_k=5)\n",
        "query_engine = MultiStepQueryEngine(\n",
        "    query_engine=query_engine,\n",
        "    query_transform=step_decompose_transform,\n",
        "    index_summary=\"Employee Key Contact Information\"\n",
        ")\n",
        "print(f\"‚ùì User question: {question}\\n\")\n",
        "print(\"ü§ñ AI is performing multi-step query...\")\n",
        "response = query_engine.query(question)\n",
        "print(\"\\nüìö Reference basis:\")\n",
        "print(\"-\" * 40)\n",
        "for i, node in enumerate(response.source_nodes, 1):\n",
        "    print(f\"\\nDocument fragment {i}:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(node.text)\n",
        "\n",
        "# Evaluation results\n",
        "print(\"\\nüìä Multi-step query evaluation results:\")\n",
        "print(\"-\" * 40)\n",
        "evaluation_score = evaluate_result(question, response, ground_truth)\n",
        "display(evaluation_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04def672",
      "metadata": {},
      "source": [
        "In this way, the system begins by understanding the overall intent behind the query, then breaks it down into a series of smaller, sequential steps to resolve ambiguity and gather information more accurately. For example, when the user asks the LLM to \"Find Michael Johnson,\" it might first attempt to clarify the request by asking:\n",
        "\n",
        "```\n",
        "\"Who is Michael Johnson according to the employee key contact information?\"\n",
        "```\n",
        "\n",
        "Based on the retrieved information, the system may then follow up with a more specific question such as:\n",
        "\n",
        "```\n",
        "\"Which Michael Johnson is being referred to in the request ‚Äî the QA Specialist or the Technical Writer?\"\n",
        "```\n",
        "\n",
        "This step-by-step approach ensures that all possible matches are considered and helps the system avoid making assumptions. Once the correct individual is identified, the system can then proceed to retrieve detailed information, such as their department, contact details, or supervisor.\n",
        "\n",
        "By processing queries in this structured manner, the system significantly improves the accuracy and relevance of its responses, especially when dealing with ambiguous or incomplete questions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0d5034",
      "metadata": {},
      "source": [
        "**Method 3: Enhance retrieval with HyDE**\n",
        "\n",
        "The previous methods have all been about adjusting the question itself. Now, let's try a different approach: what if we first assume a possible answer? This is the unique aspect of the Hypothetical Document Embeddings  (HyDE) method.\n",
        "\n",
        "Here is its working mechanism:\n",
        "\n",
        "1. First, have the LLM generate a \"hypothetical answer document\" based on the question.\n",
        "2. Use this hypothetical document to retrieve real documents.\n",
        "3. Finally, use the retrieved real documents to generate an actual answer.\n",
        "\n",
        "This is analogous to when you're looking for a book and already have a rough outline of its content in mind, then use that outline to match similar books in the library. Let's see how this can be implemented specifically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f49b65",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    HyDEQueryTransform,\n",
        ")\n",
        "from llama_index.core.query_engine import TransformQueryEngine\n",
        "# run query with HyDE query transform\n",
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "query_engine = sentence_index.as_query_engine(streaming=True,similarity_top_k=5)\n",
        "query_engine = TransformQueryEngine(query_engine, query_transform=hyde)\n",
        "\n",
        "print(f\"‚ùì User question: {question}\\n\")\n",
        "print(\"ü§ñ AI is analyzing using HyDE...\")\n",
        "streaming_response = query_engine.query(question)\n",
        "\n",
        "print(\"\\nüí≠ AI response:\")\n",
        "print(\"-\" * 40)\n",
        "streaming_response.print_response_stream()\n",
        "\n",
        "# Display reference documents\n",
        "print(\"\\nüìö Reference sources:\")\n",
        "print(\"-\" * 40)\n",
        "for i, node in enumerate(streaming_response.source_nodes, 1):\n",
        "    print(f\"\\nDocument snippet {i}:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(node.text)\n",
        "\n",
        "# Evaluate results\n",
        "print(\"\\nüìä HyDE Query Evaluation Results:\")\n",
        "print(\"-\" * 40)\n",
        "evaluation_score = evaluate_result(question, streaming_response, ground_truth)\n",
        "display(evaluation_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d22721",
      "metadata": {},
      "source": [
        "As you can see from the evaluation results, this method has indeed brought some improvements. You may be wondering: How does the system generate this \"hypothetical document\"? Let‚Äôs take a look at what content the LLM  generated during this process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76a564d",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_bundle = hyde(question + \"from the employee key contact information table\")\n",
        "hyde_doc = query_bundle.embedding_strs[0]\n",
        "print(f\"ü§ñ AI-generated hypothetical document:\\n{hyde_doc}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe30925",
      "metadata": {},
      "source": [
        "Although this \"hypothetical document\" is entirely fabricated by AI, its structure and style are very similar to real company employee information. LlamaIndex provides flexible control mechanisms to optimize this process:\n",
        "The HyDEQueryTransform class lets us precisely control the generation of hypothetical documents in the following ways:\n",
        "\n",
        "* Custom LLM: By passing different configurations of LLMs through the llm parameter, you can choose a more suitable language model for generating hypothetical documents.\n",
        "* Prompt template: Customize the prompt template via the hyde_prompt parameter to precisely control the format and content of the output.\n",
        "* Query strategy: Use the include_original parameter to decide whether to combine the original query with the hypothetical document.\n",
        "\n",
        "TransformQueryEngine acts as a wrapper for the query engine, which will:\n",
        "\n",
        "1. First call HyDEQueryTransform to generate the hypothetical document.\n",
        "2. Use the hypothetical document for vector retrieval.\n",
        "3. Return the query results.\n",
        "\n",
        "This architecture lets us optimize the retrieval effect by adjusting the parameters of HyDEQueryTransform without modifying the underlying query engine. Even though the specific content of the hypothetical document may not be entirely accurate, a well-designed configuration can help the system retrieve relevant information more accurately."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d9325c8",
      "metadata": {},
      "source": [
        "#### 4.4.2 Extracting tags to enhance retrieval\n",
        "\n",
        "On the basis of vector retrieval, we can also add tag filtering to improve retrieval accuracy. This method is similar to a library having both title search and a classification numbering system, which allows for more precise retrieval.\n",
        "\n",
        "There are two key scenarios for tag extraction:\n",
        "\n",
        "1. When building an index, extract structured tags from document chunks.\n",
        "2. During retrieval, extract corresponding tags from user queries for filtering.\n",
        "\n",
        "Let's look at two examples to understand how to extract tags from different types of text:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fe9eca",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=os.getenv(\"DASHSCOPE_API_KEY\"), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n",
        "system_message = \"\"\"You are a tag extraction expert. Please extract structured information from the text and output tags as required.\n",
        "---\n",
        "[Supported Tag Types]\n",
        "- Person Name\n",
        "- Department Name\n",
        "- Job Title\n",
        "- Technical Field\n",
        "- Product Name\n",
        "---\n",
        "[Output Requirements]\n",
        "1. Please output in JSON format, such as: [{\"key\": \"Department Name\", \"value\": \"Teaching and Research Department\"}]\n",
        "2. If a certain type of tag is not identified, do not output that type\n",
        "---\n",
        "The text to be analyzed is as follows:\n",
        "\"\"\"\n",
        "def extract_tags(text):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"qwen-turbo\",\n",
        "        messages=[\n",
        "            {'role': 'system', 'content': system_message},\n",
        "            {'role': 'user', 'content': text}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90889dbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: HR Document\n",
        "hr_text = \"\"\"David Miller is the Technical Director of our AI Research and Development department. He led the team in building the next-generation intelligent conversational platform, AstraChat, and has extensive experience in the field of natural language processing. For more information about the project, feel free to contact him directly.\"\"\"\n",
        "print(\"HR Document Tag Extraction Results:\")\n",
        "print(extract_tags(hr_text))\n",
        "\n",
        "# Example 2: Technical Document\n",
        "tech_text = \"\"\"This paper introduces a deep learning-based image recognition algorithm that has achieved significant breakthroughs in medical imaging analysis. The algorithm has been deployed in the CT diagnostic system at Johns Hopkins Hospital.\"\"\"\n",
        "print(\"\\nTechnical Document Tag Extraction Results:\")\n",
        "print(extract_tags(tech_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241b1c05",
      "metadata": {},
      "source": [
        "When we build the index, we can store these tags alongside document chunks. This way, during retrieval, for example, when a user asks \"Which department is Michael Johnson in?\" we can:\n",
        "\n",
        "1. Extract the name tag {\"key\": \"Name\", \"value\": \"Michael Johnson\"} from the question.\n",
        "2. Use the tag to filter out all document chunks containing \"Michael Johnson.\"\n",
        "3. Apply vector similarity search to find the most relevant content.\n",
        "\n",
        "This combination of \"tag filtering + vector retrieval\" significantly improves retrieval accuracy. It performs especially well when dealing with highly structured enterprise documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce4befa3",
      "metadata": {},
      "source": [
        "#### 4.4.3 Re-ranking\n",
        "\n",
        "You can delete the Markdown file created earlier to recreate the initial poor response state for the query \"Find Michael Johnson\" mentioned at the beginning of this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7125c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "![ -d ./docs/2_5/ ] && rm -r ./docs/2_5/ && echo \"Folder has been deleted.\" || echo \"Folder does not exist, no need to delete.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6af44f1",
      "metadata": {},
      "source": [
        "Then, execute the following code. As you can see, the code is set to retrieve three relevant document chunks from the vector database.\n",
        "\n",
        "However, the retrieval results are not sufficient‚Äîone entry for \"Michael Johnson\" is missing. Due to this incomplete recall, the Q&A bot is unable to correctly respond to the request \"Find Michael Johnson,\" resulting in an incorrect response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b39d4b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.dashscope import DashScope\n",
        "from chatbot import rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa75114",
      "metadata": {},
      "outputs": [],
      "source": [
        "index = rag.create_index('./docs')\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=3,\n",
        "    streaming=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c424b6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = ask(\"Find Michael Johnson\", query_engine=query_engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29964541",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(evaluate_result(question, response, ground_truth))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0956716b",
      "metadata": {},
      "source": [
        "You can adjust the code to first retrieve 20 document chunks from the vector database, then use the¬†[text rerank](https://help.aliyun.com/zh/model-studio/getting-started/models#eafbfdceb7n03) provided by Alibaba Cloud Model Studio to re-rank them, and filter out the three most relevant reference pieces of information.\n",
        "\n",
        "After running the code, you will notice that, with the same three reference pieces of information, the LLM is now able to answer the question accurately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b32f4f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.postprocessor.dashscope_rerank import DashScopeRerank\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c59623e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    # First, set a larger number of recall chunks\n",
        "    similarity_top_k=20,\n",
        "    streaming=True,\n",
        "    node_postprocessors=[\n",
        "        # In the rerank model, select the final number of chunks you want to recall. Use the gte-rerank model from Tongyi Lab for reranking.\n",
        "        DashScopeRerank(top_n=3, model=\"gte-rerank\"),\n",
        "        # Set a similarity threshold; chunks below this threshold will be filtered out\n",
        "        SimilarityPostprocessor(similarity_cutoff=0.2)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26bdb676",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = ask(\"Which department is Michael Johnson in\", query_engine=query_engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636e00f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(evaluate_result(question, response, ground_truth))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c282face",
      "metadata": {},
      "source": [
        "### 4.5 Answer generation phase\n",
        "\n",
        "Now, the LLM will generate the final answer based on your question and the retrieved content. However, this answer may still not meet your expectations. The issues you might encounter include:\n",
        "1. No relevant information was retrieved, causing the LLM to hallucinate an answer.\n",
        "2. Relevant information was retrieved, but the LLM did not generate the answer as required.\n",
        "3. Relevant information was retrieved, and the LLM provided an answer, but you expected the AI to give a more comprehensive response.\n",
        "To address these issues, you can analyze and resolve them from the following perspectives:\n",
        "* Choosing the right LLM:\n",
        "    * For simple information queries and summaries, a small-parameter model is sufficient, such as¬†[qwen-turbo](https://help.aliyun.com/zh/model-studio/models#ff492e2c10lub).\n",
        "    * If you want the Q&A bot to perform complex logical reasoning, it is recommended to choose a larger-parameter LLM with stronger reasoning capabilities, such as¬†[qwen-plus](https://help.aliyun.com/zh/model-studio/models#bb0ffee88bwnk) or even¬†[Qwen-Max](https://help.aliyun.com/zh/model-studio/models#cf6cc4aa2aokf).\n",
        "    * If your question requires reviewing a large number of document fragments, it is recommended to choose a model with a longer context length, such as¬†[qwen-long](https://help.aliyun.com/zh/model-studio/models#27b2b3a15d5c6),¬†[qwen-turbo](https://help.aliyun.com/zh/model-studio/models#ff492e2c10lub), or¬†[qwen-plus](https://help.aliyun.com/zh/model-studio/models#bb0ffee88bwnk).\n",
        "    * If the RAG chatbot you are building is for non-general domains such as the legal field, it is recommended to use a model trained specifically for that domain, such as¬†[Tongyi Farei](https://help.aliyun.com/zh/model-studio/models#f0436273ef1xm).\n",
        "* Fully optimize the prompt template. For example:\n",
        "    * Clearly request no fabrication of answers:\n",
        "        * LLMs may produce inaccurate content, a phenomenon commonly referred to as hallucination.\n",
        "        * You can reduce the likelihood of LLM hallucinations by requiring in the prompt: \"If the provided information is insufficient to answer the question, please explicitly state 'Based on the available information, I cannot answer this question.' Do not fabricate answers.\"\n",
        "    * Add content delimiters: \n",
        "        * If the retrieved document chunks are randomly mixed into the prompt, it will be difficult for humans to see the structure of the entire prompt, and the LLM will also be affected. \n",
        "        * It is recommended to clearly separate the prompt and the retrieved chunks so that the LLM can correctly understand your intent.\n",
        "    * Adjust the template according to the type of question: \n",
        "        * Different types of questions may require different response paradigms. You can use the LLM to identify the question type and then map different prompt templates accordingly. \n",
        "        * For example, for some questions, you may want the LLM to first output the overall framework and then the details; for other questions, you may prefer the LLM to provide concise conclusions.\n",
        "* Adjust the parameters of the LLM. For example:\n",
        "    * If you want the LLM to produce the same output for the same question, pass the same seed value each time the model is invoked.\n",
        "    * If you want to discourage the model from repeating tokens that have already appeared in the response, you can increase the presence_penalty value. \n",
        "    * If you are querying factual content, appropriately decrease the temperature or top_p values; conversely, when generating creative content,  increase their values.\n",
        "    * If you need to limit the word count (such as generating summaries or keywords), control costs, or reduce response time, appropriately lower the max_tokens value. However, if max_tokens is too low, it may lead to truncated output. Conversely, when generating long text, increase its value.\n",
        "    * Refer to the¬†[Qwen API Reference](https://help.aliyun.com/zh/model-studio/use-qwen-by-calling-api) to learn more about the usage instructions for various parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c18b00",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.core import Settings\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c98514a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Factual query scenario - Low temperature, high certainty\n",
        "factual_llm = OpenAILike(\n",
        "    model=\"qwen-plus-0919\",  # Use the Qwen-Plus model\n",
        "    api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "    is_chat_model=True,\n",
        "    temperature=0.1,      # Lower temperature for more deterministic output\n",
        "    max_tokens=512,       # Control output length; however, if max_tokens is too small, it may lead to truncated output\n",
        "    presence_penalty=0.0, # Default presence_penalty\n",
        "    seed=42              # Fixed seed for reproducible output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242a5ded",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creative scenario - High temperature, more diversity\n",
        "creative_llm = OpenAILike(\n",
        "    model=\"qwen-plus-0919\",\n",
        "    api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
        "    is_chat_model=True,\n",
        "    temperature=0.7,      # Increase temperature to make the output more creative\n",
        "    max_tokens=1024,      # Allow longer output\n",
        "    presence_penalty=0.6  # Increase presence_penalty to reduce repetition\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c085ee76",
      "metadata": {},
      "source": [
        "* LLM Fine-Tuning: If all the above methods have been thoroughly attempted but still fall short of your expectations, or if you hope to achieve further performance improvements, you can also try model fine-tuning tailored to your specific scenario. In later chapters, you will learn and practice this process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30451423",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "\n",
        "Through this section, you have gained an understanding the workflow of a simple RAG and common optimization techniques. You can also combine the knowledge you've acquired with your specific needs to route certain questions to different RAG chatbots, thereby building a more powerful modular RAG system. Additionally, from the previous lessons, you should also recognize that LLMs are not only useful for building question-answering systems: Leveraging LLMs to identify user intent and extract structured information‚Äîsuch as extracting tags from user questions as mentioned earlier‚Äîcan also be applied in many other application scenarios.\n",
        "\n",
        "Of course, the optimization methods for RAG go far beyond those introduced in this course. The industry continues to research and explore RAG, and there are still many advanced RAG topics worth  studying. As shown through the previous learning, building a well-rounded and high-performing RAG chatbot is no simple task. In real-world applications, you may need to act quickly and won‚Äôt always have time to dive into every detail. Below are some directions worth exploring:\n",
        "\n",
        "* GraphRAG ingeniously combines the strengths of RAG and query-focused summarization (QFS), providing a powerful solution for handling large-scale text data. It integrates the advantages of both technologies: RAG excels at finding precise detailed information, while QFS is better at understanding and summarizing the overall content of an article. This combination allows GraphRAG to accurately answer specific questions and handle complex queries that require deeper understanding, making it particularly suitable for building intelligent question-answering systems. If you want to learn how to  apply GraphRAG in practice, you can refer to the detailed tutorial provided by LlamaIndex:¬†[Building a GraphRAG Application with LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v2/).\n",
        "* With Model Studio, you can follow the document¬†[Build a Private Knowledge Question-Answering Application Without Coding](https://help.aliyun.com/zh/model-studio/getting-started/build-knowledge-base-qa-assistant-without-coding) to quickly build a fairly effective RAG chatbot.\n",
        "* If your business processes are more complex, you can also leverage¬†Visual Workflow, an agent orchestration application on Model Studio to build a more powerful application.\n",
        "* Model Studio also offers a range of¬†[LlamaIndex components](https://help.aliyun.com/zh/model-studio/developer-reference/llamaindex/), allowing you to fully leverage Model Studio's capabilities while continuing to use the familiar LlamaIndex API to build RAG chatbots."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f132f563",
      "metadata": {},
      "source": [
        "## üî• Quiz\n",
        "\n",
        "### üîç Single choice question\n",
        "\n",
        "<details>\n",
        "<summary style=\"cursor: pointer; padding: 12px; border: 1px solid #dee2e6; border-radius: 6px;\">\n",
        "<b>In RAG applications, the length and content of document chunks significantly impact retrieval performance. If the chunk size is too large, introducing excessive noise, how should it be addressed‚ùì(Select 1.)</b>\n",
        "\n",
        "- A. Increase the number of documents\n",
        "- B. Reduce the chunk size, or develop a more reasonable chunking strategy based on business characteristics\n",
        "- C. Use a more advanced retrieval algorithm\n",
        "- D. Improve the training level of the large model\n",
        "\n",
        "**[Click to view the answer]**\n",
        "</summary>\n",
        "\n",
        "<div style=\"margin-top: 10px; padding: 15px; border: 1px solid #dee2e6; border-radius: 0 0 6px 6px;\">\n",
        "\n",
        "‚úÖ **Reference Answer: B**  \n",
        "üìù **Explanation**: \n",
        "- Excessively long document chunks may include too much irrelevant information (noise), directly affecting retrieval accuracy.\n",
        "- For example, if a single chunk contains multiple topics, unrelated content may be retrieved during searches.\n",
        "- Optimizing the chunking strategy is the fundamental solution to address noise, as it controls input quality rather than relying on subsequent algorithmic or model compensation.\n",
        "\n",
        "</div>\n",
        "</details>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d2938e0",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "llm_learn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
