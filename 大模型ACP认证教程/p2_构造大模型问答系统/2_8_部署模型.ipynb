{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ce9447cd7acd65",
   "metadata": {},
   "source": [
    "# 2.8 部署模型\n",
    "## 🚄 前言\n",
    "在经过模型微调和评测后，答疑机器人的开发工作已经接近完成。本节课程将继续学习如何在计算资源上部署模型，使其成为一个可访问的应用服务。\n",
    "同时还会进一步介绍在云上部署模型的常见方法，并帮助你根据自身的需求选择最合适的方式部署模型。\n",
    "\n",
    "## 🍁 课程目标\n",
    "学完本节课程后，你将能够：\n",
    "* 了解如何手动部署一个模型\n",
    "* 了解在云上部署模型的常见方式\n",
    "* 基于自身的需求选择最合适的方式部署模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740aa389",
   "metadata": {},
   "source": [
    "## 1. 直接调用模型（无需部署）\n",
    "\n",
    "部署模型是将训练好的AI模型从开发环境转移到生产环境的过程，使其能够处理实时数据并为实际应用提供服务，从而服务于真实用户，创造价值。\n",
    "\n",
    "回顾 2.1 至 2.6 节的课程中，你已经多次调用过模型（如:qwq-32b，qwen-plus等），但并没有对模型进行部署。这是因为你所调用的模型都是阿里云的预置模型，它们已经部署在阿里云服务器上，可以直接通过 API 进行调用。\n",
    "\n",
    "这种直接调用服务提供商（如阿里云）完全托管的API服务有几点好处：\n",
    "\n",
    "- **直接调用**：无需部署模型，只需简单地调用API即可。\n",
    "- **按需计费**：按token量计费，无需担心模型部署的资源消耗。\n",
    "- **无需运维**：你无需担心模型的部署和运维，如自动扩缩容、模型版本升级等，这部分工作均由模型服务提供商完成。\n",
    "\n",
    "这种方式很适合**业务初期或中小规模场景**，可以有效降低业务初期投入成本，并避免闲置GPU资源的浪费。\n",
    "\n",
    "**需要注意的是**：直接调用模型一般会被“[限流](https://help.aliyun.com/zh/model-studio/user-guide/rate-limit)”，例如通过百炼API调用时，每分钟的调用次数（QPM）和消耗的Token数（TPM）均有限制。超出限制将导致请求失败，需等待限流条件解除后才能再次调用。”\n",
    "\n",
    "同时，如果模型经过微调或服务提供商尚未支持，直接调用将无法满足需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0e0bc",
   "metadata": {},
   "source": [
    "## 2. 在测试环境中部署模型\n",
    "\n",
    "在 2.7 节的课程中，你通过微调一个小参数模型（Qwen2.5-1.5B-Instruct），在加速推理的同时保持了较高的准确度。接下来，你需要部署这个微调后的模型以提供服务。\n",
    "\n",
    "部署模型通常包括下载模型、编写加载代码和发布为支持API访问的应用服务，这涉及较高的人工成本。vLLM是一个专为大模型推理设计的开源框架，可以简化这一流程。它通过简单的命令行参数快速部署模型，并通过内存优化和缓存策略提升推理速度和支持高并发请求。\n",
    "\n",
    "本节将使用vLLM加载模型并启动服务。该服务提供的HTTP接口兼容OpenAI API，可以通过调用/v1/chat/completions等接口，让你快速体验大模型的推理能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbeb4b",
   "metadata": {},
   "source": [
    "### 2.1 环境准备\n",
    "\n",
    "本课程实验环境需与第2.7微调章节保持一致，确保在GPU环境下执行模型部署操作。 \n",
    "\n",
    "如已按课程目录顺序学习，请继续使用第2.7章节启动的PAI-DSW实例；若单独学习本章节，则按第2.7章节环境准备进行配置即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87854404",
   "metadata": {},
   "source": [
    "请在指定目录下打开Terminal终端窗口。\n",
    "\n",
    "在本课程的目录下，**即/mnt/workspace/aliyun_acp_learning/大模型ACP认证教程/p2_构造大模型问答系统**，打开一个新的终端窗口。\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i2/O1CN01rZsIL71Zfj8TpX2Wq_!!6000000003222-2-tps-1392-1334.png\" width=\"400\">\n",
    "\n",
    "\n",
    "你可以在终端窗口中输入`pwd`命令查看终端窗口所在的目录位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc61f0",
   "metadata": {},
   "source": [
    "### 2.2 使用vLLM部署模型\n",
    "\n",
    "#### 2.2.1 部署开源模型\n",
    "\n",
    "推荐你在[ModelScope模型库](https://modelscope.cn/models)中下载**Qwen2.5-1.5B-Instruct**模型进行部署。（境外开发者也可以访问[HuggingFace模型库](https://huggingface.co/models)下载开源模型进行部署，暂不在此演示。）\n",
    "\n",
    "首先，下载模型文件到本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abcb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./model/qwen2_5-1_5b-instruct\n",
    "!modelscope download --model qwen/Qwen2.5-1.5B-Instruct --local_dir './model/qwen2_5-1_5b-instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f60239",
   "metadata": {},
   "source": [
    "下载成功后，模型文件会保存在./model/qwen2_5-1_5b-instruct文件夹下。\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN010iGHeL1REEQvGxUXD_!!6000000002079-0-tps-770-166.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bfb11",
   "metadata": {},
   "source": [
    "接着安装依赖项，在终端窗口执行以下指令安装vllm（如果你遇到版本冲突，你也可以尝试安装vllm==0.6.2版本）\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "      width: 80%;\n",
    "      margin: 20px; /* Center the table */\n",
    "      border-collapse: collapse; /* Collapse borders for a cleaner look */\n",
    "      font-family: sans-serif; \n",
    "    }\n",
    "\n",
    "    th, td {\n",
    "      padding: 10px;\n",
    "      text-align: left;\n",
    "      border: 1px solid #ddd; /* Light gray border */\n",
    "    }\n",
    "\n",
    "    th {\n",
    "      background-color: #f2f2f2; /* Light gray background for header */\n",
    "      font-weight: bold;\n",
    "    }\n",
    "\n",
    "    tr:nth-child(even) { /* Zebra striping */\n",
    "      background-color: #f9f9f9;\n",
    "    }\n",
    "\n",
    "    tr:hover { /* Highlight row on hover */\n",
    "      background-color: #e0f2ff; /* Light blue */\n",
    "    }\n",
    "</style>\n",
    "<table width=\"90%\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "```bash\n",
    "pip install vllm==0.6.0\n",
    "```\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "安装好vllm后，在终端窗口执行**vllm命令**启动一个模型服务。\n",
    "<table width=\"90%\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "```bash\n",
    "vllm serve \"./model/qwen2_5-1_5b-instruct\" --load-format \"safetensors\" --port 8000\n",
    "```\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "- vllm serve：表示启动模型服务。\n",
    "- \"./model/qwen2_5-1_5b-instruct\"：表示加载的模型路径，通常包含模型文件、版本信息等。\n",
    "- --load-format \"safetensors\"：指定加载模型时使用的格式。\n",
    "- --port 8000：指定端口号，如果端口被占用，请切换为其他端口，如8100等。\n",
    "\n",
    "服务启动成功后，终端窗口会打印 **“INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)”** 信息。\n",
    "\n",
    "请注意，关闭终端窗口将会立即终止服务。后续的测试和评估性能将依赖于该服务进行，因此请不要关闭此终端窗口。\n",
    "\n",
    "> 如果你希望在后台持续运行服务而不受终端窗口关闭的影响，可以使用这条命令。\n",
    "> ```bash\n",
    "> # 后台运行服务，且服务的运行日志存储到vllm.log\n",
    "> nohup vllm serve \"./model/qwen2_5-1_5b-instruct\" --load-format \"safetensors\" --port 8000 > vllm.log 2>&1 &\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b7a50",
   "metadata": {},
   "source": [
    "#### 2.2.2 部署微调模型（可选）\n",
    "\n",
    "第 2.7 节的微调模型默认放在**output**目录。下面示例选择微调后的merge模型进行部署。请打开一个新的终端窗口执行vllm命令。\n",
    "\n",
    "<style>\n",
    "    table {\n",
    "      width: 80%;\n",
    "      margin: 20px; /* Center the table */\n",
    "      border-collapse: collapse; /* Collapse borders for a cleaner look */\n",
    "      font-family: sans-serif; \n",
    "    }\n",
    "\n",
    "    th, td {\n",
    "      padding: 10px;\n",
    "      text-align: left;\n",
    "      border: 1px solid #ddd; /* Light gray border */\n",
    "    }\n",
    "\n",
    "    th {\n",
    "      background-color: #f2f2f2; /* Light gray background for header */\n",
    "      font-weight: bold;\n",
    "    }\n",
    "\n",
    "    tr:nth-child(even) { /* Zebra striping */\n",
    "      background-color: #f9f9f9;\n",
    "    }\n",
    "\n",
    "    tr:hover { /* Highlight row on hover */\n",
    "      background-color: #e0f2ff; /* Light blue */\n",
    "    }\n",
    "</style>\n",
    "<table width=\"90%\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "```bash\n",
    "vllm serve \"./output/qwen2_5-1_5b-instruct/v0-202xxxxx-xxxxxx/checkpoint-xxx-merged\" --load-format \"safetensors\" --port 8001\n",
    "```\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "- \"./output/qwen2_5-1_5b-instruct/v0-202xxxxx-xxxxxx/checkpoint-xxx-merged\"：替换为真实的微调模型路径。\n",
    "- --port 8001：设置与步骤2.1中不同的端口号，避免端口占用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8331432",
   "metadata": {},
   "source": [
    "### 2.3 测试服务运行状态\n",
    "\n",
    "vLLM 支持启动兼容OpenAI API的本地服务器，即按照OpenAI API标准返回结果。\n",
    "\n",
    "通过cURL发送HTTP请求，测试 **2.2.1** 中部署的**Qwen2.5-1.5B-Instruct**模型服务是否能够正常响应。如果使用微调模型服务，请确保将请求的URL端口号从8000修改为8001。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"./model/qwen2_5-1_5b-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"你是一个帮助助手。\"},\n",
    "            {\"role\": \"user\", \"content\": \"请告诉我2008年北京奥运会，中国队总共获得了多少枚金牌？\"}\n",
    "        ]\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9ce42",
   "metadata": {},
   "source": [
    "上述接口正常响应即表示服务正常运行。\n",
    "\n",
    "此外，还兼容了 /v1/models 接口，支持查看部署的模型列表。更多信息请查看[vLLM兼容的OpenAI API](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#api-reference)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6414050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X GET http://localhost:8000/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a3edd",
   "metadata": {},
   "source": [
    "### 2.4 评估服务性能\n",
    "\n",
    "为了评估部署后的模型服务性能，这里使用一个简单的HTTP性能测试工具**wrk**来快速模拟压测请求并生成报告。下面以压测 **POST /v1/chat/completions** 接口为例，展示服务的相关性能指标。\n",
    "\n",
    "首先，打开一个新的终端窗口，安装压测工具wrk的依赖包。注意：终端窗口是在步骤1中指定的目录下。\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install wrk\n",
    "```\n",
    "\n",
    "接着，准备POST请求所需要的Body数据。数据已放在./resources/2_9/post.lua文件，文件内容如下所示。\n",
    "\n",
    "```bash\n",
    "wrk.method = \"POST\"\n",
    "wrk.headers[\"Content-Type\"] = \"application/json\"\n",
    "wrk.body = [[\n",
    "    {\n",
    "       \"model\": \"./model/qwen2_5-1_5b-instruct\",\n",
    "       \"messages\": [\n",
    "           {\"role\": \"system\", \"content\": \"你是一个帮助助手。\"},\n",
    "           {\"role\": \"user\", \"content\": \"请告诉我2008年北京奥运会，中国队总共获得了多少枚金牌？\"}\n",
    "       ]\n",
    "   }\n",
    "]]\n",
    "```\n",
    "\n",
    "然后，在终端窗口执行wrk压测命令，分别设置chat接口的并发量(-c)为1和10，压测时间(-d)均为10s，观察两个实验的压测结果。\n",
    "\n",
    "```bash\n",
    "wrk -t1 -c1 -d10s -s ./resources/2_9/post.lua http://localhost:8000/v1/chat/completions\n",
    "\n",
    "wrk -t1 -c10 -d10s -s ./resources/2_9/post.lua http://localhost:8000/v1/chat/completions\n",
    "```\n",
    "\n",
    "wrk压测结果如下所示：\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01ybO7TU1X6LJ12FYdV_!!6000000002874-2-tps-1452-322.png\" width=\"500\" height=\"150\">\n",
    "<img src=\"https://img.alicdn.com/imgextra/i2/O1CN01bberC61txr86CpFjU_!!6000000005969-2-tps-1472-362.png\" width=\"500\" height=\"150\">\n",
    "\n",
    "\n",
    "根据压测结果可见，随着并发量增加（1 -> 10），QPS提升了约6倍（3.30 -> 20.08），平均延迟增加了约30%（324.61ms -> 426.84ms）。特别地是，第二个压测实验中出现了2个超时错误。这是因为在并发量较高情况下，服务器的负载超过了其处理能力，性能的不足导致了部分请求超时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0db6a",
   "metadata": {},
   "source": [
    "## ☁3. 在云上部署模型\n",
    "\n",
    "上述压测结果显示，由于部署模型的设备算力有限，模型服务无法满足**低延迟**和**高并发**的推理需求。\n",
    "\n",
    "传统的解决方式是通过购买更高性能的“服务器”，并重新将模型部署到服务器上。但是这种方式的问题在于：\n",
    "- **资源成本**：需要一次性购买大量高性能的“服务器”。\n",
    "\n",
    "- **运维成本**：日常维护服务器，包括监控、升级、故障排查等，需要较高的专业技能。\n",
    "\n",
    "- **可靠性**：服务的稳定可靠一方面依赖于维护人员的能力，另一方面依赖于项目成本，在有限的成本下，很难建立高可用、稳定可靠的模型服务。\n",
    "\n",
    "- **灵活性较低**：受限于固有的硬件资源，无法根据实际需要动态调整资源，从而导致模型服务性能不足或资源浪费。\n",
    "\n",
    "<br>\n",
    "\n",
    "相对于购买服务器部署模型，**使用云服务部署模型** 通常是一种更好的选择。云服务可以为你提供更多灵活的部署方式，你可以根据自身能力和需求，选择[**大模型服务平台百炼**](https://help.aliyun.com/zh/model-studio/getting-started/what-is-model-studio)、[**函数计算FC**](https://help.aliyun.com/zh/functioncompute/fc-3-0/product-overview/what-is-function-compute)、[**人工智能平台PAI-EAS**](https://help.aliyun.com/zh/pai/user-guide/overview-2)、[**GPU云服务器**](https://help.aliyun.com/zh/egs/what-is-elastic-gpu-service)、[**容器服务ACK**](https://help.aliyun.com/zh/ack/product-overview/product-introduction)、[**容器计算服务ACS**](https://help.aliyun.com/zh/cs/product-overview/product-introduction)等云服务，以获得可扩展、高并发、低延迟、灵活管理以及稳定的服务，快速适应业务变化。\n",
    "\n",
    "接下来将详细介绍这些云服务，帮助你快速识别自身需求，并选择适合的云服务，快速搭建模型服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51e026",
   "metadata": {},
   "source": [
    "### 3.1 使用大模型服务平台百炼部署模型\n",
    "\n",
    "你可以使用百炼的控制台页面完成模型的快速部署，这种方式简单、直接，你无需掌握复杂的部署模型方法，也可以轻松拥有独占的模型服务。同时也可以通过简单的[API部署模型](https://help.aliyun.com/zh/model-studio/developer-reference/model-deployment-quick-start)。\n",
    "\n",
    "部署过程如下所示：\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01KkOfZb1HAtQ9sTyop_!!6000000000718-0-tps-2630-278.jpg\" width=\"450\">\n",
    "\n",
    "- **选择模型**：选择预置模型或自定义模型。\n",
    "    - 预置模型：百炼支持的标准模型，根据需要选择合适的模型进行部署，可在[部署新模型](https://bailian.console.aliyun.com/?spm=a2c4g.11186623.0.0.63e56cfcXIU4Qj#/efm/model_deploy)时查看支持的模型。\n",
    "\n",
    "        <img src=\"https://help-static-aliyun-doc.aliyuncs.com/assets/img/zh-CN/9723494371/p892458.png\" width=\"300\">\n",
    "\n",
    "    - 自定义模型：百炼平台调优后的模型，可参考：[调优支持的模型](https://help.aliyun.com/zh/model-studio/model-training-on-console?spm=a2c4g.11186623.0.0.63e56cfcMC90g9#a6da1accf0dun)。\n",
    "- **一键部署模型**：控制台上支持一键部署模型，也可以通过API快速部署。\n",
    "- **基于百炼生态使用模型**：部署后的模型可无缝集成百炼生态，支持在百炼控制台直接使用，并可通过HTTP和DashScope调用复用百炼API。\n",
    "\n",
    "模型部署操作可参考[百炼部署模型](https://help.aliyun.com/zh/model-studio/user-guide/model-deployment)文档。\n",
    "\n",
    "虽然通过百炼平台部署模型可以大大降低模型部署和维护的难度，但因为百炼平台支持的模型种类有限，如果你的模型不在支持的范围内，可以通过接下来的几种方法进行部署。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93472d",
   "metadata": {},
   "source": [
    "### 3.2 使用函数计算FC部署模型\n",
    "\n",
    "你可以使用函数计算FC部署更多类型的模型，函数计算提供Serverless GPU服务，无需运维底层资源，同时通过按需付费，对于不频繁使用的模型可以节省大量的成本，尤其适合计算资源要求高的临时任务。\n",
    "\n",
    "通过函数计算部署模型的步骤如下图所示：\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN010io9jc1p0znYJipd3_!!6000000005299-0-tps-2670-502.jpg\" width=\"750\">\n",
    "\n",
    "- 创建项目：创建空白项目和基于模版创建两种方式\n",
    "- 新建模型服务：选择创建空白项目后，需要选择新建“模型”类型的服务。\n",
    "- 配置：包括基础配置和资源配置，其中基础配置包括地域、选择模型、名称配置等，资源配置包括了选择GPU类型、GPU规格、内存规格等。\n",
    "- 确认部署：确认配置后部署模型。\n",
    "\n",
    "你可以[一键部署QwQ-32B推理模型](https://help.aliyun.com/zh/functioncompute/fc-3-0/use-cases/two-ways-to-quickly-deploy-and-experience-qwq-32b-reasoning-model)体验函计算提供的部署能力，更多部署实践可参见[函数计算3.0-实践教程](https://help.aliyun.com/zh/functioncompute/fc-3-0/use-cases/?spm=a2c4g.11186623.help-menu-2508973.d_3.228e493fj6un1Y&scm=20140722.H_2509019._.OR_help-V_1)。\n",
    "\n",
    "通过函数计算来部署模型也不是没有缺点：\n",
    "- 冷启动延迟：如果一段时间内没有请求到达，则函数可能会进入“冷”状态，在接收到新的调用请求时需要重新启动实例，这可能导致首次响应时间较长。\n",
    "- 调试难度增加：基于函数的应用可能更难于调试和监控。在多步骤处理流程中定位问题较难。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcbf468",
   "metadata": {},
   "source": [
    "### 3.3 使用 PAI-EAS 部署模型\n",
    "\n",
    "你可以通过人工智能平台PAI的模型在线服务（EAS）将从开源社区下载的模型或自己训练获得的模型部署为在线服务。它提供的弹性扩缩容、蓝绿部署、资源组管理、版本控制以及资源监控等功能，帮助你更好地管理模型应用。同时为了解决模型初次请求耗时较长的问题，EAS提供了模型预热功能，使模型服务在上线之前得到预热，从而实现模型服务上线后即可进入正常服务状态。\n",
    "\n",
    "PAI-EAS支持镜像部署和Processor部署两种方式，镜像部署的主要步骤如下所示。\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01Z9r6gL1ZsYG718kRZ_!!6000000003250-0-tps-2648-230.jpg\" width=\"600\">\n",
    "\n",
    "- 上传模型及数据：支持上传开源社区或者自定义的模型，除此之外，还可以上传代码、模型预热数据等。\n",
    "- 配置环境：可以使用PAI预置的官方镜像，也可以使用自定义镜像，同时还可能需要安装第三方依赖。\n",
    "- 部署服务：在镜像中启动vLLM服务。\n",
    "- 基于EAS-API调用服务：服务启动成功后，EAS会自动生成公网域名访问URL，用于调用服务。\n",
    "\n",
    "你可以参考[5分钟操作EAS一键部署通义千问模型](https://help.aliyun.com/zh/pai/use-cases/use-pai-eas-to-quickly-deploy-tongyi-qianwen?spm=a2c4g.11186623.0.i0#ba6b53303bb66)来快速部署一个通用模型，立即体验PAI-EAS所提供的模型服务能力。\n",
    "\n",
    "若你希望部署自定义模型，建议参考[5分钟使用EAS一键部署LLM大语言模型应用](https://help.aliyun.com/zh/pai/use-cases/deploy-llm-in-eas#c1d769ba33kh5)。\n",
    "\n",
    "相比较于函数计算，PAI-EAS可能会有更高的固定成本，对于低频使用的场景，可能不如函数计算经济实惠，你可以尝试通过 Spot Instance 模式帮助节省成本，详见：[PAI-EAS Spot最佳实践](https://help.aliyun.com/zh/pai/use-cases/pai-eas-spot-best-practices)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519841b",
   "metadata": {},
   "source": [
    "### 3.4 使用云服务器ECS或容器服务部署模型\n",
    "\n",
    "通过云服务器ECS部署模型是比较通用的部署方式，可以完全控制服务器的配置、操作系统和环境设置，这对于需要高度定制化和特定依赖项的模型非常有用。\n",
    "\n",
    "同时ECS提供了稳定的计算资源，不会像函数计算那样有冷启动延迟的问题。ECS可以结合弹性伸缩服务实现实例的弹性扩容和缩容，结合负载均衡器（如SLB）来实现高可用性和负载均衡。还可以通过安全组、访问控制、数据加密等，确保数据和服务的安全性。\n",
    "\n",
    "但是这些功能的配置和管理需要一定的技能和经验，维护成本较高。\n",
    "- **适合场景**：需要高度定制化、稳定性能和长期运行的大型模型；对成本可预测性和资源控制有较高要求的企业。\n",
    "- **不适合场景**：需要快速部署和弹性伸缩的小型项目；对运维复杂性敏感且资源有限的团队。\n",
    "\n",
    "\n",
    "通过ECS服务器部署模型的基本步骤如下图所示。\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01AJfJ1u1XoJMO5QhmS_!!6000000002970-0-tps-2994-884.jpg\" width=\"900\">\n",
    "\n",
    "- 规划需求：明确业务目标，确定部署的模型类型和规模。\n",
    "- 选择ECS服务器配置：按需求选择 CPU 或 GPU 类型的服务器、选择磁盘类型与容量、配置VPC和带宽、选择镜像、设置安全组规则等。\n",
    "- 创建和启动ECS：完成ECS实例的创建并启动。\n",
    "- 部署模型：包括上传模型及数据、以及配置基础环境环境（如配置防火墙、安装Docker/kubernetes环境、第三方依赖等）。\n",
    "- 启动模型服务：可通过如本节中2.2的方法基于vLLM启动服务，该服务运行在云服务器的本地环境。\n",
    "- 验证服务访问：实验测试可用ECS IP访问；正式对外服务需申请公网域名并进行域名备案以获得可公开访问的URL。\n",
    "\n",
    "你可以参考[使用vLLM容器镜像快速构建大语言模型在GPU上的推理环境](https://help.aliyun.com/zh/egs/use-cases/use-a-vllm-container-image-to-run-inference-tasks-on-a-gpu-accelerated-instance)执行具体操作。\n",
    "\n",
    "如果你要部署的模型是Llama模型、ChatGLM模型、百川模型、通义千问模型及其微调模型，请[安装并使用DeepGPU-LLM进行模型的推理服务](https://help.aliyun.com/zh/egs/developer-reference/install-and-use-deepgpu-llm-for-model-inference?spm=a2c4g.11186623.0.i6)以加速模型推理能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c5f42",
   "metadata": {},
   "source": [
    "如果你的组织已经积累了基于容器的模型部署经验，也可以使用 ACK 结合 GPU 云服务器节点，无需学习过多新概念。\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i1/O1CN01QN7YTj21mQCPHDFDs_!!6000000007027-0-tps-2410-1414.jpg\" width=\"800\">\n",
    "\n",
    "另外，你也可以考虑使用 ACS，可以帮助你在熟悉的 Kubernetes 容器集群环境中，直接获取 GPU 算力容器，同时无需关注集群的运维。\n",
    "\n",
    "ACS部署模型的方法可以参考： [使用ACS GPU算力构建QwQ-32B模型推理服务](https://help.aliyun.com/zh/cs/user-guide/build-qwq-32b-model-inference-service-using-acs-gpu-computing-power)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f1e88d7d95ac9",
   "metadata": {},
   "source": [
    "## ✅本节小结\n",
    "\n",
    "本节课程详细介绍了模型部署的基本方法，你学习了：\n",
    "\n",
    "- 部署模型的实操步骤，即如何将模型部署为一个可访问的模型推理服务，部署模型可以是开源模型或微调后的模型。\n",
    "\n",
    "- 部署模型并不是必须的，你可以直接调用服务提供商（如阿里云）完全托管的API服务，从而有效降低业务初期投入成本，避免闲置GPU资源的浪费。\n",
    "\n",
    "- 根据自身的需求和能力选择不同的云服务（如百炼平台、函数计算FC、PAI-EAS、云服务器ECS、ACK/ACS等方式）部署模型，从而实现业务需求和资源利用的平衡。\n",
    "\n",
    "    <img src=\"https://img.alicdn.com/imgextra/i2/O1CN01eZXXeI1UpHLljjCJw_!!6000000002566-0-tps-2864-1470.jpg\" width=\"650\">\n",
    "\n",
    "通过本课程的学习，你已经掌握了模型部署的基本方法，为构建高性能、可扩展的大模型应用打下了坚实的基础。\n",
    "\n",
    "接下来，你将进一步学习在大模型应用的生产环节中如何更好地保障模型的可用性、安全性和性能。\n",
    "\n",
    ">⚠️ **注意**：完成本节学习后，请及时停止当前的 PAI-DSW GPU 实例，以避免产生额外费用。\n",
    "\n",
    "### 扩展阅读\n",
    "\n",
    "本课程介绍了云端部署，在实际部署中可分为公共云部署和专有云部署。\n",
    "\n",
    "- **公共云部署**：将模型封装为API供用户调用，类似SaaS模式。这种方式降低了使用门槛，便于集成，但需要确保API稳定和安全。\n",
    "\n",
    "- **专有云部署**：在企业内部搭建专有云平台，并将模型部署在专有云上。它能提供更高的数据安全性和控制权，支持定制化，但需投入一定的维护成本。\n",
    "\n",
    "此外，端云协同部署也是一种常见的部署方式。\n",
    "\n",
    "**端云协同部署**结合云端和边缘部署的优势，部分计算在边缘设备上进行，复杂计算任务则上传到云端处理，能够在保证用户体验的同时处理复杂的计算任务。\n",
    "\n",
    "端云协同部署适用于需要实时响应但计算资源有限的场景，如乐天派与通义大模型合作打造端侧陪伴智能语音机器人，见下图。这里的“端”指客户端部署的小模型，“云”指云端部署的大模型。这些小模型需要微调以适配不同客户端设备，并与客户端应用在同一个环境中，负责初步处理数据预处理等简单任务，然后将处理后的信息传输给云端，由云端大模型进行深入处理，以快速且高质量响应端侧用户需求。\n",
    "\n",
    "<img src=\"https://img.alicdn.com/imgextra/i2/O1CN015VBCB524nk9sYPd8s_!!6000000007436-0-tps-1662-474.jpg\" width=\"650\">\n",
    "\n",
    "在一些特定场景，**嵌入式系统部署**是更为合适的选择，如汽车、机器人和医疗设备等。这种方式将模型部署在硬件平台上，能够实现实时控制和决策，但对模型和硬件的优化要求较高。\n",
    "\n",
    "因此，在面对实际业务需求时，你需要考虑系统的性能要求、数据隐私和安全性，以及实施的复杂性，从而确保部署方案的高效与可持续性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d9718",
   "metadata": {},
   "source": [
    "## ✉️ 评价反馈\n",
    "感谢你学习阿里云大模型ACP认证课程，如果你觉得课程有哪里写得好、哪里写得不好，期待你[通过问卷提交评价和反馈](https://survey.aliyun.com/apps/zhiliao/Mo5O9vuie)。\n",
    "\n",
    "你的批评和鼓励都是我们前进的动力。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_learn)",
   "language": "python",
   "name": "llm_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
